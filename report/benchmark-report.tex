\documentclass[11pt,a4paper]{article}

\usepackage[margin=2.5cm]{geometry}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{microtype}
\usepackage{multirow}

\pgfplotsset{compat=1.18}

\sisetup{
  per-mode=symbol,
  round-mode=figures,
  round-precision=3,
}

\definecolor{hmatrixblue}{HTML}{2166AC}
\definecolor{massivred}{HTML}{B2182B}
\definecolor{lineargreen}{HTML}{1B7837}

\title{Benchmark Report: \texttt{linear-massiv} vs.\ \texttt{hmatrix} vs.\ \texttt{linear}\\[0.3em]
\large Performance Comparison of Haskell Linear Algebra Libraries}
\author{Nadia Chambers \and Claude Opus 4.6}
\date{February 2026}

\begin{document}
\maketitle

\begin{abstract}
We present a comprehensive performance comparison of three Haskell
numerical linear algebra libraries: \texttt{linear-massiv} (pure Haskell,
type-safe dimensions via massiv arrays), \texttt{hmatrix} (FFI bindings to
BLAS/LAPACK via OpenBLAS), and \texttt{linear} (pure Haskell, optimised for
small fixed-size vectors and matrices).  Benchmarks cover BLAS-level
operations, direct solvers, orthogonal factorisations, eigenvalue
problems, and singular value decomposition across matrix dimensions from
$4 \times 4$ to $500 \times 500$.  Additionally, we evaluate the parallel
scalability of \texttt{linear-massiv}'s massiv-backed computation
strategies on a 20-core workstation.

Initial baseline results showed \texttt{hmatrix} (OpenBLAS) dominating at
all sizes for $O(n^3)$ operations---\texttt{linear-massiv} was
$36\text{--}21{,}000\times$ slower depending on operation and size---while
\texttt{linear} excelled at $4 \times 4$ through unboxed product types.
Over nineteen rounds of optimisation---progressing from algorithmic
improvements (cache-blocked GEMM, in-place QR/eigenvalue via the ST monad),
through raw \texttt{ByteArray\#} with GHC~9.14's \texttt{DoubleX4\#} AVX2
SIMD primops, to register-blocked micro-kernels, panel-based blocked
factorisations, and eigenvalue convergence acceleration---\texttt{linear-massiv}
now \textbf{outperforms hmatrix (OpenBLAS/LAPACK) in all nine benchmarked
operation categories} at $500{\times}500$ and in eight of nine at
$200{\times}200$: GEMM is $10\times$ faster single-threaded
($0.096\times$ ratio) and $\mathbf{29\times}$ faster with parallel
scheduling; dot product is $4\times$ faster ($0.25\times$); matrix--vector
multiply is $2.3\times$ faster ($0.44\times$); LU solve is $1.5\times$
faster ($0.68\times$); Cholesky solve is $\mathbf{2.1\times}$ faster
($0.48\times$); QR factorisation is $\mathbf{46\times}$ faster
($0.021\times$); eigenvalue decomposition is at parity at $100{\times}100$
and $1.4\times$ faster at $200{\times}200$ ($0.69\times$);
and SVD is at parity at $200{\times}200$ ($0.98\times$) and
$1.2\times$ faster at $500{\times}500$ ($0.82\times$).
\texttt{linear-massiv} demonstrates that pure Haskell with GHC's native
SIMD primops, raw \texttt{ByteArray\#} primops, and lightweight
thread-level parallelism can comprehensively outperform FFI-based
BLAS/LAPACK across all benchmarked numerical linear algebra operations,
while providing compile-time dimensional safety, zero FFI dependencies,
and user-controllable parallelism.
\end{abstract}

\tableofcontents
\newpage

%% ####################################################################
\part{Context}
%% ####################################################################

%% ====================================================================
\section{Introduction}
\label{sec:intro}

The Haskell ecosystem offers several numerical linear algebra libraries,
each occupying a distinct niche:

\begin{description}
\item[\texttt{linear}] Edward Kmett's library provides small
  fixed-dimension types (\texttt{V2}, \texttt{V3}, \texttt{V4}) with
  unboxed product representations, making it extremely fast for
  graphics, game physics, and any application where dimensions are
  statically known and small. It does not support arbitrary-dimension
  matrices.

\item[\texttt{hmatrix}] Alberto Ruiz's library wraps BLAS and LAPACK
  via Haskell's FFI, delegating numerical computation to
  highly-optimised Fortran routines (on this system, OpenBLAS). It
  supports arbitrary dimensions but carries an FFI dependency and
  provides no compile-time dimension checking.

\item[\texttt{linear-massiv}] Our library implements algorithms from
  Golub \& Van Loan's \emph{Matrix Computations} (4th ed.)~\cite{gvl4}
  in pure Haskell, using massiv arrays~\cite{massiv} as the backing
  store. Matrix dimensions are tracked at the type level via GHC's
  \texttt{DataKinds} and \texttt{KnownNat}, providing compile-time
  rejection of dimensionally incorrect operations. Massiv's computation
  strategies (\texttt{Seq}, \texttt{Par}, \texttt{ParN~$n$}) offer
  user-controllable parallelism.
\end{description}

This report benchmarks all three libraries across the standard numerical
linear algebra operation suite (Table~\ref{tab:operations}) and evaluates
\texttt{linear-massiv}'s parallel scalability from 1 to 20 threads.

\begin{table}[h]
\centering
\caption{Operations benchmarked and library coverage.}
\label{tab:operations}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Operation} & \textbf{\texttt{linear}} & \textbf{\texttt{hmatrix}} & \textbf{\texttt{linear-massiv}} \\
\midrule
GEMM (matrix multiply)      & $4\times4$ only & all sizes & all sizes \\
Dot product                  & $n=4$ only     & all sizes & all sizes \\
Matrix--vector product       & $4\times4$ only & all sizes & all sizes \\
LU solve ($Ax = b$)          & ---            & all sizes & all sizes \\
Cholesky solve ($Ax = b$)    & ---            & all sizes & all sizes \\
QR factorisation             & ---            & all sizes & all sizes \\
Symmetric eigenvalue         & ---            & all sizes & all sizes \\
SVD                          & ---            & all sizes & all sizes \\
Parallel GEMM                & ---            & ---       & all sizes \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Hardware and Software Environment}

\begin{itemize}
\item \textbf{CPU:} 20-core x86\_64 processor (Linux 6.17, Fedora 43)
\item \textbf{Compiler:} GHC 9.12.2 with \texttt{-O2} (Rounds~1--2);
      GHC 9.14.1 with LLVM~17 backend (\texttt{-fllvm -mavx2 -mfma}) (Rounds~3--10)
\item \textbf{BLAS backend:} OpenBLAS (system-installed via FlexiBLAS)
\item \textbf{Benchmark framework:} Criterion~\cite{criterion} with 95\% confidence intervals
\item \textbf{Protocol:} Single-threaded (\texttt{+RTS -N1}) for cross-library comparisons;
      multi-threaded (\texttt{+RTS -N}) for parallel scaling
\end{itemize}

%% ====================================================================
\section{Methodology}
\label{sec:method}

All benchmarks use the Criterion framework~\cite{criterion}, which
employs kernel density estimation and robust regression to estimate
mean execution time with confidence intervals. Each benchmark evaluates
to normal form (\texttt{nf}) to ensure full evaluation of lazy results.

\paragraph{Matrix construction.}
Matrices are constructed from the same deterministic formula across all
three libraries:
\[
A_{ij} = \frac{7i + 3j + 1}{100}
\]
ensuring identical numerical content. For solver benchmarks, matrices are
made diagonally dominant ($A_{ii} \mathrel{+}= n$) or symmetric positive
definite ($A = B^T B + nI$) as appropriate.

\paragraph{Single-threaded protocol.}
Cross-library comparisons use \texttt{+RTS -N1} to restrict the GHC
runtime to a single OS thread, ensuring that neither hmatrix's OpenBLAS
nor massiv's parallel strategies introduce implicit multi-threading.

\paragraph{Parallel scaling protocol.}
Parallel benchmarks use \texttt{+RTS -N} (all 20 cores) and vary
massiv's computation strategy from \texttt{Seq} through
\texttt{ParN~1} to \texttt{ParN~20}.

%% ####################################################################
\part{Baseline Assessment}
%% ####################################################################

%% ====================================================================
\section{Baseline Performance}
\label{sec:baseline}

This section presents the initial (Round~1) benchmark results before any
optimisation work.  The performance gaps documented here---ranging from
$36\times$ to $21{,}000\times$---motivated the eighteen rounds of
optimisation described in Part~III.

\subsection{BLAS Operations}
\label{sec:blas}

\subsubsection{General Matrix Multiply (GEMM)}

Table~\ref{tab:gemm} presents GEMM timings across matrix dimensions.
At $4 \times 4$, the \texttt{linear} library's unboxed \texttt{V4 (V4 Double)}
representation achieves \SI{143}{\nano\second}, roughly $4.5\times$ faster than
\texttt{hmatrix}'s \SI{646}{\nano\second} and $240\times$ faster than
\texttt{linear-massiv}'s \SI{34.5}{\micro\second}. The advantage of
\texttt{linear} at this size is entirely due to GHC's ability to unbox the
product type into registers, avoiding all array indexing overhead.

As matrix dimension grows, \texttt{hmatrix} (OpenBLAS DGEMM) dominates
decisively. At $100 \times 100$, hmatrix takes \SI{1.53}{\milli\second}
versus \texttt{linear-massiv}'s \SI{505}{\milli\second}---a factor of
$330\times$. At $200 \times 200$, the ratio grows to $297\times$
(\SI{13.8}{\milli\second} vs.\ \SI{4.09}{\second}). This reflects the
massive constant-factor advantage of OpenBLAS's hand-tuned assembly
kernels with cache blocking, SIMD, and microarchitectural optimisation.

\begin{table}[h]
\centering
\caption{GEMM execution time (mean, single-threaded). Best per size in \textbf{bold}.}
\label{tab:gemm}
\begin{tabular}{@{}rrrr@{}}
\toprule
{Size} & {\texttt{linear}} & {\texttt{hmatrix}} & {\texttt{linear-massiv}} \\
\midrule
$4\times4$     & \textbf{\SI{143}{\nano\second}}  & \SI{646}{\nano\second}  & \SI{34.5}{\micro\second} \\
$10\times10$   & ---                               & \textbf{\SI{2.33}{\micro\second}} & \SI{678}{\micro\second}  \\
$50\times50$   & ---                               & \textbf{\SI{174}{\micro\second}}  & \SI{55.0}{\milli\second}  \\
$100\times100$ & ---                               & \textbf{\SI{1.53}{\milli\second}} & \SI{505}{\milli\second}  \\
$200\times200$ & ---                               & \textbf{\SI{13.8}{\milli\second}} & \SI{4.09}{\second}       \\
\bottomrule
\end{tabular}
\end{table}

Both \texttt{hmatrix} and \texttt{linear-massiv} exhibit $O(n^3)$ scaling,
as shown in Figure~\ref{fig:gemm}. The consistent vertical offset on the
log--log plot reflects the constant-factor difference between OpenBLAS
assembly and pure Haskell array operations.

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{loglogaxis}[
  xlabel={Matrix dimension $n$},
  ylabel={Time (seconds)},
  title={GEMM: Time vs.\ Matrix Dimension},
  legend pos=north west,
  legend style={font=\small},
  grid=major,
  width=0.85\textwidth,
  height=0.55\textwidth,
]
\addplot[color=hmatrixblue, mark=square*, thick] coordinates {
  (4, 6.46e-7) (10, 2.33e-6) (50, 1.74e-4) (100, 1.53e-3) (200, 1.38e-2)
};
\addlegendentry{hmatrix (OpenBLAS)}
\addplot[color=massivred, mark=triangle*, thick] coordinates {
  (4, 3.45e-5) (10, 6.78e-4) (50, 5.50e-2) (100, 5.05e-1) (200, 4.09)
};
\addlegendentry{linear-massiv}
\addplot[color=lineargreen, mark=o, thick] coordinates {
  (4, 1.43e-7)
};
\addlegendentry{linear (V4 only)}
\end{loglogaxis}
\end{tikzpicture}
\caption{GEMM scaling comparison (log--log). Both libraries exhibit $O(n^3)$
behaviour; the vertical offset reflects constant-factor differences between
OpenBLAS assembly and pure Haskell.}
\label{fig:gemm}
\end{figure}

\subsubsection{Dot Product}

\begin{table}[h]
\centering
\caption{Dot product execution time (mean, single-threaded).}
\label{tab:dot}
\begin{tabular}{@{}rrrr@{}}
\toprule
{$n$} & {\texttt{linear}} & {\texttt{hmatrix}} & {\texttt{linear-massiv}} \\
\midrule
4    & \textbf{\SI{13.1}{\nano\second}} & \SI{593}{\nano\second}  & \SI{1.67}{\micro\second} \\
100  & ---                               & \textbf{\SI{749}{\nano\second}} & \SI{34.1}{\micro\second} \\
1000 & ---                               & \textbf{\SI{2.81}{\micro\second}} & \SI{379}{\micro\second}  \\
\bottomrule
\end{tabular}
\end{table}

The dot product is an $O(n)$ operation, so the absolute times are small.
At $n = 4$, \texttt{linear}'s unboxed \texttt{V4} achieves \SI{13}{\nano\second}---essentially
four fused multiply-adds in registers. At $n = 1000$, \texttt{hmatrix}
achieves \SI{2.81}{\micro\second} (DDOT with SIMD), while \texttt{linear-massiv}'s
array-based loop takes \SI{379}{\micro\second}---a $135\times$ gap that
reflects the overhead of massiv's general-purpose array indexing versus
BLAS's contiguous-memory vectorised inner loop.

\subsubsection{Matrix--Vector Product}

\begin{table}[h]
\centering
\caption{Matrix--vector product execution time (mean, single-threaded).}
\label{tab:matvec}
\begin{tabular}{@{}rrrr@{}}
\toprule
{$n$} & {\texttt{linear}} & {\texttt{hmatrix}} & {\texttt{linear-massiv}} \\
\midrule
4   & \textbf{\SI{41.8}{\nano\second}} & \SI{815}{\nano\second}  & \SI{11.2}{\micro\second} \\
50  & ---                               & \textbf{\SI{3.76}{\micro\second}} & \SI{1.24}{\milli\second} \\
100 & ---                               & \textbf{\SI{14.1}{\micro\second}} & \SI{4.71}{\milli\second} \\
\bottomrule
\end{tabular}
\end{table}

Matrix--vector multiplication is $O(n^2)$. At $n = 100$, \texttt{hmatrix}
(DGEMV) achieves \SI{14.1}{\micro\second} while \texttt{linear-massiv}
takes \SI{4.71}{\milli\second}---a $334\times$ difference consistent with
the GEMM results, confirming that the performance gap is primarily due to
low-level memory access patterns and SIMD utilisation rather than
algorithmic differences.

\subsection{Linear System Solvers}
\label{sec:solve}

\subsubsection{LU Solve}

\begin{table}[h]
\centering
\caption{LU solve ($Ax = b$) execution time (mean, single-threaded). Includes factorisation + back-substitution.}
\label{tab:lu}
\begin{tabular}{@{}rrr@{}}
\toprule
{Size} & {\texttt{hmatrix}} & {\texttt{linear-massiv}} \\
\midrule
$10\times10$   & \textbf{\SI{7.70}{\micro\second}} & \SI{280}{\micro\second}   \\
$50\times50$   & \textbf{\SI{87.7}{\micro\second}} & \SI{20.4}{\milli\second}  \\
$100\times100$ & \textbf{\SI{485}{\micro\second}}  & \SI{143}{\milli\second}   \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Cholesky Solve}

\begin{table}[h]
\centering
\caption{Cholesky solve ($Ax = b$, $A$ SPD) execution time.
Includes factorisation + back-substitution.}
\label{tab:cholesky}
\begin{tabular}{@{}rrr@{}}
\toprule
{Size} & {\texttt{hmatrix}} & {\texttt{linear-massiv}} \\
\midrule
$10\times10$   & \textbf{\SI{6.08}{\micro\second}} & \SI{237}{\micro\second}   \\
$50\times50$   & \textbf{\SI{64.3}{\micro\second}} & \SI{12.9}{\milli\second}  \\
$100\times100$ & \textbf{\SI{418}{\micro\second}}  & \SI{100}{\milli\second}   \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{loglogaxis}[
  xlabel={Matrix dimension $n$},
  ylabel={Time (seconds)},
  title={Solver Performance: hmatrix vs.\ linear-massiv},
  legend pos=north west,
  legend style={font=\small},
  grid=major,
  width=0.85\textwidth,
  height=0.55\textwidth,
]
\addplot[color=hmatrixblue, mark=square*, thick] coordinates {
  (10, 7.70e-6) (50, 8.77e-5) (100, 4.85e-4)
};
\addlegendentry{hmatrix LU solve}
\addplot[color=massivred, mark=triangle*, thick] coordinates {
  (10, 2.80e-4) (50, 2.04e-2) (100, 1.43e-1)
};
\addlegendentry{linear-massiv LU solve}
\addplot[color=hmatrixblue, mark=square, thick, dashed] coordinates {
  (10, 6.08e-6) (50, 6.43e-5) (100, 4.18e-4)
};
\addlegendentry{hmatrix Cholesky solve}
\addplot[color=massivred, mark=triangle, thick, dashed] coordinates {
  (10, 2.37e-4) (50, 1.29e-2) (100, 1.00e-1)
};
\addlegendentry{linear-massiv Cholesky solve}
\end{loglogaxis}
\end{tikzpicture}
\caption{LU and Cholesky solve scaling (log--log). Both algorithms are
$O(n^3)$; hmatrix calls DGESV/DPOTRS directly.}
\label{fig:solve}
\end{figure}

For both LU and Cholesky solvers, \texttt{hmatrix} is approximately $36\times$
faster at $10 \times 10$ and $240\text{--}300\times$ faster at $100 \times 100$.
The ratio increases with dimension because OpenBLAS's cache-blocked implementations
benefit more from larger working sets. Cholesky is consistently faster than LU
for both libraries, as expected (Cholesky requires roughly half the floating-point
operations of LU factorisation for symmetric positive definite matrices).

\subsection{Orthogonal Factorisations}
\label{sec:qr}

\begin{table}[h]
\centering
\caption{QR factorisation (Householder) execution time (mean, single-threaded).}
\label{tab:qr}
\begin{tabular}{@{}rrr@{}}
\toprule
{Size} & {\texttt{hmatrix}} & {\texttt{linear-massiv}} \\
\midrule
$10\times10$   & \textbf{\SI{217}{\micro\second}} & \SI{11.1}{\milli\second} \\
$50\times50$   & \textbf{\SI{18.4}{\milli\second}} & \SI{7.01}{\second}       \\
$100\times100$ & \textbf{\SI{214}{\milli\second}}  & (estimated $\approx$\SI{56}{\second}) \\
\bottomrule
\end{tabular}
\end{table}

QR factorisation shows the largest gap between the two libraries. At
$50 \times 50$, \texttt{hmatrix} takes \SI{18.4}{\milli\second} while
\texttt{linear-massiv} requires \SI{7.01}{\second}---a ratio of $381\times$.
The \texttt{linear-massiv} QR implementation constructs full explicit $Q$ and
$R$ matrices at each Householder step using \texttt{makeMatrix}, while LAPACK's
\texttt{DGEQRF} uses an implicit representation of $Q$ as a product of
Householder reflectors stored in-place, dramatically reducing both memory
allocation and floating-point work. The $100 \times 100$ benchmark for
\texttt{linear-massiv} was too slow to complete within a reasonable time
budget and is estimated by extrapolation.

\subsection{Eigenvalue Problems and SVD}
\label{sec:eigen}

\subsubsection{Symmetric Eigenvalue Decomposition}

\begin{table}[h]
\centering
\caption{Symmetric eigenvalue decomposition execution time (mean, single-threaded).}
\label{tab:eigen}
\begin{tabular}{@{}rrr@{}}
\toprule
{Size} & {\texttt{hmatrix}} & {\texttt{linear-massiv}} \\
\midrule
$10\times10$ & \textbf{\SI{17.4}{\micro\second}} & \SI{15.6}{\milli\second}  \\
$50\times50$ & \textbf{\SI{555}{\micro\second}}  & \SI{8.89}{\second}        \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Singular Value Decomposition}

\begin{table}[h]
\centering
\caption{SVD execution time (mean, single-threaded).}
\label{tab:svd}
\begin{tabular}{@{}rrr@{}}
\toprule
{Size} & {\texttt{hmatrix}} & {\texttt{linear-massiv}} \\
\midrule
$10\times10$ & \textbf{\SI{37.7}{\micro\second}} & \SI{33.4}{\milli\second} \\
$50\times50$ & \textbf{\SI{806}{\micro\second}}  & \SI{17.2}{\second}       \\
\bottomrule
\end{tabular}
\end{table}

The eigenvalue and SVD results show the most dramatic ratios: $896\times$
for eigenvalues at $10 \times 10$ and $16{,}000\times$ at $50 \times 50$;
$886\times$ and $21{,}400\times$ for SVD. These operations are dominated
by iterative QR sweeps; hmatrix calls LAPACK's \texttt{DSYEV} and
\texttt{DGESVD}, which use divide-and-conquer algorithms with
cache-oblivious recursive structure. The \texttt{linear-massiv}
implementation uses the classical tridiagonal QR algorithm
(GVL4~\cite{gvl4} Algorithm 8.3.3) with explicit matrix construction at
each iteration step, which is algorithmically sound but suffers from
excessive allocation and the lack of in-place updates that LAPACK exploits.

\subsection{Parallel Scalability}
\label{sec:parallel}

A distinguishing feature of \texttt{linear-massiv} is user-controllable
parallelism inherited from the massiv array library~\cite{massiv}.
Operations that construct result arrays via \texttt{makeArray} can specify
a computation strategy: \texttt{Seq} (sequential), \texttt{Par} (automatic,
all available cores), or \texttt{ParN~$n$} (exactly $n$ worker threads).
Neither \texttt{hmatrix} nor \texttt{linear} offer comparable user-level
control over thread-level parallelism within the Haskell runtime.

Table~\ref{tab:parallel} shows GEMM timings at $100 \times 100$ and
$200 \times 200$ across thread counts, and Figure~\ref{fig:speedup}
shows the corresponding speedup curves.

\begin{table}[h]
\centering
\caption{Parallel GEMM execution time (seconds) and speedup over sequential.}
\label{tab:parallel}
\begin{tabular}{@{}l SS SS @{}}
\toprule
& \multicolumn{2}{c}{$100 \times 100$} & \multicolumn{2}{c}{$200 \times 200$} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}
{Strategy} & {Time (\si{\second})} & {Speedup} & {Time (\si{\second})} & {Speedup} \\
\midrule
Seq     & 0.613  & 1.00  & 4.750  & 1.00  \\
ParN-1  & 0.598  & 1.03  & 4.656  & 1.02  \\
ParN-2  & 0.319  & 1.92  & 3.224  & 1.47  \\
ParN-4  & 0.201  & 3.05  & 1.847  & 2.57  \\
ParN-8  & 0.282  & 2.17  & 1.329  & 3.57  \\
ParN-16 & 0.0856 & 7.16  & 2.569  & 1.85  \\
ParN-20 & 0.0979 & 6.26  & 1.976  & 2.40  \\
Par     & 0.0883 & 6.94  & 1.408  & 3.37  \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
  xlabel={Number of threads},
  ylabel={Speedup ($T_\text{seq} / T_n$)},
  title={GEMM Parallel Speedup},
  legend pos=north west,
  legend style={font=\small},
  grid=major,
  width=0.85\textwidth,
  height=0.55\textwidth,
  xmin=0, xmax=21,
  ymin=0, ymax=10,
]
\addplot[color=massivred, mark=triangle*, thick] coordinates {
  (1, 1.03) (2, 1.92) (4, 3.05) (8, 2.17) (16, 7.16) (20, 6.26)
};
\addlegendentry{$100\times100$}
\addplot[color=hmatrixblue, mark=square*, thick] coordinates {
  (1, 1.02) (2, 1.47) (4, 2.57) (8, 3.57) (16, 1.85) (20, 2.40)
};
\addlegendentry{$200\times200$}
\addplot[dashed, black, thin, domain=1:20] {x};
\addlegendentry{Ideal linear}
\end{axis}
\end{tikzpicture}
\caption{Parallel speedup for GEMM. The dashed line shows ideal linear
scaling. Actual speedup is limited by Amdahl's law, memory bandwidth
contention, and GHC runtime scheduling overhead.}
\label{fig:speedup}
\end{figure}

The parallel scaling results reveal several important characteristics:

\begin{itemize}
\item \textbf{Peak speedup.} At $100 \times 100$, peak speedup of
  $7.2\times$ is achieved with \texttt{ParN-16}, while at $200 \times 200$
  peak speedup of $3.6\times$ occurs at \texttt{ParN-8}. The \texttt{Par}
  (automatic) strategy achieves $6.9\times$ and $3.4\times$ respectively,
  demonstrating that massiv's automatic scheduling is effective.

\item \textbf{Non-monotonic scaling.} Speedup does not increase monotonically
  with thread count. The $200 \times 200$ case shows degradation at 16 and
  20 threads, likely due to memory bandwidth saturation and NUMA effects on
  this 20-core system. At $100 \times 100$, the anomalous dip at 8 threads
  followed by improvement at 16 suggests that GHC's work-stealing scheduler
  interacts non-trivially with cache hierarchy.

\item \textbf{Amdahl's law.} Even the best parallel GEMM (\SI{85.6}{\milli\second}
  at $100 \times 100$ with 16 threads) remains $56\times$ slower than
  hmatrix's single-threaded \SI{1.53}{\milli\second}. Parallelism narrows
  but does not close the gap with BLAS.
\end{itemize}

\subsection{Performance Summary}
\label{sec:discussion}

Table~\ref{tab:ratios} summarises the baseline performance ratios between libraries.

\begin{table}[h]
\centering
\caption{Performance ratio: \texttt{linear-massiv} time / \texttt{hmatrix} time.
Values $> 1$ indicate hmatrix is faster.}
\label{tab:ratios}
\begin{tabular}{@{}lrrr@{}}
\toprule
{Operation} & {$n = 10$} & {$n = 50$} & {$n = 100$} \\
\midrule
GEMM            & $291\times$ & $316\times$ & $329\times$ \\
Dot product     & ---         & ---         & $46\times$  \\
Matrix--vector  & ---         & $330\times$ & $334\times$ \\
LU solve        & $36\times$  & $233\times$ & $295\times$ \\
Cholesky solve  & $39\times$  & $201\times$ & $240\times$ \\
QR              & $51\times$  & $382\times$ & $\approx 260\times$ \\
Eigenvalue (SH) & $897\times$ & $16{,}020\times$ & --- \\
SVD             & $887\times$ & $21{,}400\times$ & --- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis of the Performance Gap}
\label{sec:perf-gap-analysis}

The performance gap between \texttt{linear-massiv} and \texttt{hmatrix}
arises from several compounding factors:

\begin{enumerate}
\item \textbf{SIMD and microarchitectural optimisation.} OpenBLAS uses
  hand-written assembly kernels for each target microarchitecture,
  exploiting AVX-512, fused multiply-add, and optimal register tiling.
  GHC's native code generator does not emit SIMD instructions for
  general Haskell code.

\item \textbf{Cache blocking.} LAPACK algorithms are designed around
  cache-oblivious or cache-tiled recursive decomposition, minimising
  cache misses. The \texttt{linear-massiv} implementations use textbook
  algorithms (GVL4) without cache-level optimisation.

\item \textbf{In-place mutation.} LAPACK routines operate in-place on
  mutable Fortran arrays, while \texttt{linear-massiv}'s pure functional
  approach allocates a new array for each intermediate result. For
  iterative algorithms (eigenvalue, SVD), this is particularly costly.

\item \textbf{Allocation pressure.} Each \texttt{makeMatrix} call in
  \texttt{linear-massiv} allocates a new massiv array. For algorithms
  like QR (which constructs explicit $Q$ and $R$ at each Householder
  step) and iterative eigensolvers, this dominates runtime.
\end{enumerate}

\subsection{Initial Optimisation Roadmap}
\label{sec:remedies}

The following roadmap was identified after the baseline assessment.
Part~III details the implementation and results of these optimisations
over eighteen subsequent rounds, ultimately achieving parity with and
then exceeding LAPACK performance in all nine categories.  The proposals
are presented in their original form to preserve the reasoning that
guided the optimisation process.

\subsubsection{In-place Factorisation via the ST Monad}
\label{sec:remedy-inplace}

The single largest source of overhead in the QR, eigenvalue, and SVD
routines is the allocation of a fresh \texttt{Matrix} at every
iteration step.  Currently, each Householder reflection in the QR
factorisation calls \texttt{applyHouseholderLeftRect} and
\texttt{applyHouseholderRightQ}, both of which invoke
\texttt{makeMatrix} to reconstruct the entire $m \times n$ (or
$m \times m$) result.  Similarly, the symmetric QR algorithm rebuilds
the tridiagonal matrix from diagonal and subdiagonal vectors at each
implicit QR step, and the Jacobi eigenvalue method reconstructs the
full matrix for each of its $O(n^2)$ rotations per sweep.

The remedy is straightforward: the LU solver (\texttt{luFactor}) already
demonstrates the pattern.  It wraps the input in
\texttt{M.withMArrayST}, allocates a mutable pivot vector via
\texttt{M.newMArray}, and performs all elimination steps in the
\texttt{ST}~monad using \texttt{M.readM} / \texttt{M.write\_}---with
zero intermediate allocation.  Applying the same technique to
Householder QR, the tridiagonal QR iteration, and the Jacobi method
would:

\begin{itemize}
\item Reduce the $n$ Householder steps of QR from $n$ full-matrix
  allocations to a single mutable copy of $R$ plus an accumulated $Q$,
  both updated in-place.  This alone should bring the $381\times$ gap at
  $50 \times 50$ down by roughly an order of magnitude, since the
  dominant cost becomes floating-point work rather than GC pressure.

\item Eliminate the per-iteration matrix reconstruction in the symmetric
  QR algorithm.  LAPACK's \texttt{DSYEV} stores only the diagonal and
  subdiagonal as mutable vectors and applies Givens rotations in-place;
  the same approach in Haskell's \texttt{ST} monad would remove the
  $O(n^2)$ allocation at each of the $O(n)$ iterations.

\item Reduce the Jacobi method's cost from $O(n^2)$ matrix copies per
  sweep to $O(n^2)$ element-level reads and writes per sweep---a factor
  of $\sim n^2$ fewer allocations.
\end{itemize}

\subsubsection{Implicit Householder Representation (Compact WY)}
\label{sec:remedy-wy}

The current QR implementation forms the explicit $Q$ matrix by
accumulating each Householder reflector $H_k = I - 2 v_k v_k^T$
into a running product.  LAPACK instead stores the reflector vectors
$v_1, \ldots, v_n$ and, when the full $Q$ is needed, applies them in
reverse order (or uses the compact WY representation
$Q = I - V T V^T$, GVL4~\cite{gvl4} Section~5.1.6).

The compact WY form has two advantages: (a)~the $Q$ factor is never
formed until explicitly requested, reducing QR itself to an $O(n^3)$
in-place update of $R$; and (b)~subsequent operations that need $Q^T b$
(e.g.\ least squares) can apply the reflectors directly without ever
forming the $m \times m$ matrix $Q$.  This would transform QR from a
bottleneck ($381\times$ gap) into a routine on par with LU solve
($\sim 200\text{--}300\times$), and further in-place optimisation
(Section~\ref{sec:remedy-inplace}) would close the gap still further.

\subsubsection{Cache-Blocked GEMM}
\label{sec:remedy-blocking}

The current GEMM implementation is the textbook three-loop inner product
form (GVL4~\cite{gvl4} Algorithm~1.1.5, ijk variant):

\[
C_{ij} = \sum_{k=0}^{K-1} A_{ik}\, B_{kj}
\]

\noindent
where each element $C_{ij}$ performs a \texttt{foldl'} over the shared
dimension.  This accesses $A$ by rows and $B$ by columns, with stride-$n$
column access patterns that are hostile to the CPU cache hierarchy for
$n > \sqrt{L_1 / 8}$ (typically $n > 40$ on modern x86).

GVL4 Algorithm~1.3.1 describes a six-loop tiled variant that partitions
$A$, $B$, and $C$ into $b \times b$ sub-blocks (where $b$ is chosen so
that three blocks fit in L1/L2 cache) and performs small \emph{block}
matrix multiplies at each step.  Implementing this in pure Haskell would
not match OpenBLAS's hand-tuned assembly, but experience from other
languages suggests tiled GEMM typically yields $3\text{--}10\times$
improvement over the na\"ive loop for $n \geq 100$, which would narrow
the current $300\times$ gap to $30\text{--}100\times$.

A simpler first step is loop reordering: changing from the ijk variant
to the ikj (row-outer-product) or kij variant, which accesses $C$ and
$B$ with unit stride.  This alone can yield $2\text{--}4\times$
improvement on cache-unfriendly sizes and requires only changing the
loop nesting order in the existing \texttt{foldl'} computation.

\subsubsection{Divide-and-Conquer Eigenvalue and SVD}
\label{sec:remedy-dc}

The current eigenvalue solver uses the classical tridiagonal QR
algorithm (GVL4~\cite{gvl4} Algorithm~8.3.3), which has $O(n^2)$
cost per eigenvalue in the worst case and $O(n^3)$ overall.  LAPACK's
\texttt{DSYEVD} uses a divide-and-conquer approach (GVL4
Algorithm~8.4.2) that recursively splits the tridiagonal matrix and
solves the secular equation at each merge step.  In practice,
divide-and-conquer is $2\text{--}5\times$ faster than the QR algorithm
for dense matrices with $n > 25$, and it is also more amenable to
parallelisation since the two sub-problems at each recursion level are
independent.

Similarly, the current SVD uses iterated QR sweeps with Wilkinson
shifts; LAPACK's \texttt{DGESDD} uses a divide-and-conquer SVD.
Implementing these would address the $16{,}000\text{--}21{,}000\times$
gaps at $50 \times 50$ (Table~\ref{tab:ratios}), which are inflated by
the iterative algorithms' per-step allocation cost compounding with
algorithmic inefficiency.

\subsubsection{SIMD Primitives}
\label{sec:remedy-simd}

GHC provides experimental SIMD support via the \texttt{ghc-prim}
package, exposing 128-bit and 256-bit vector types
(\texttt{DoubleX2\#}, \texttt{DoubleX4\#}) with fused multiply-add
operations.  While the interface is low-level and requires careful
manual vectorisation, it could be applied to the innermost loops of
GEMM, dot product, and matrix--vector multiply.  A 4-wide
\texttt{DoubleX4\#} FMA would process four $C_{ij}$ accumulations per
cycle, giving a theoretical $4\times$ throughput improvement on the
inner loop---significant for Level~1 and Level~2 BLAS operations where
the gap is dominated by per-element overhead rather than cache effects.

Alternatively, the \texttt{primitive-simd} or \texttt{simd} packages
provide portable wrappers around GHC's SIMD primops.  The
\texttt{vector} library (which underlies massiv's \texttt{P}rimitive
representation) stores \texttt{Double} in contiguous pinned memory,
making it compatible with SIMD load/store patterns.

\subsubsection{Optional FFI Backend}
\label{sec:remedy-ffi}

For users who can accept an FFI dependency, \texttt{linear-massiv}
could provide an optional backend that delegates Level~3 BLAS operations
to the system BLAS/LAPACK via \texttt{hmatrix} or direct
\texttt{cblas\_dgemm} FFI calls, while preserving the type-safe
\texttt{KnownNat}-indexed interface.  This is architecturally
straightforward: the \texttt{Matrix m n r e} type wraps a massiv array
whose underlying \texttt{P}rimitive representation is a pinned
\texttt{ByteArray}, which can be passed to C via
\texttt{unsafeWithPtr} or copied into an hmatrix \texttt{Matrix Double}
with a single \texttt{memcpy}.

This approach would offer the best of both worlds---compile-time
dimensional safety with BLAS-level performance---while keeping the
pure Haskell implementation as the default for portability.  A Cabal
flag (e.g.\ \texttt{-f blas-backend}) could control which backend is
linked, similar to how \texttt{vector-algorithms} provides optional
C-accelerated sort routines.

\subsubsection{Summary of Expected Impact}
\label{sec:remedy-summary}

Table~\ref{tab:remedies} estimates the cumulative effect of each
proposed optimisation on the GEMM performance ratio at $100 \times 100$.

\begin{table}[h]
\centering
\caption{Estimated impact of proposed optimisations on the
$100 \times 100$ GEMM performance ratio (current: $329\times$).}
\label{tab:remedies}
\begin{tabular}{@{}llr@{}}
\toprule
{Optimisation} & {Mechanism} & {Est.\ ratio} \\
\midrule
Current baseline           & na\"ive ijk, pure allocation & $329\times$ \\
+ Loop reorder (ikj)       & unit-stride access           & $\sim 100\text{--}160\times$ \\
+ Cache-blocked tiling     & L1/L2 reuse                  & $\sim 30\text{--}50\times$ \\
+ SIMD (DoubleX4\#)        & 4-wide FMA inner loop        & $\sim 8\text{--}15\times$ \\
+ FFI backend (OpenBLAS)   & delegate to DGEMM            & $\sim 1\times$ \\
\bottomrule
\end{tabular}
\end{table}

\noindent
For factorisation and iterative algorithms (QR, eigenvalue, SVD), the
in-place ST~monad refactoring (Section~\ref{sec:remedy-inplace}) and
implicit Householder representation (Section~\ref{sec:remedy-wy}) are
the highest-priority items, as they address the dominant allocation
overhead that accounts for much of the $300\text{--}21{,}000\times$
gaps.  The divide-and-conquer algorithms
(Section~\ref{sec:remedy-dc}) would further reduce the gap for
eigenvalue and SVD problems, particularly at moderate-to-large
dimensions.


%% ####################################################################
\part{The Optimisation Journey}
%% ####################################################################

The baseline assessment identified four compounding factors behind
\texttt{linear-massiv}'s $36\text{--}21{,}000\times$ performance
deficit: lack of SIMD exploitation, absence of cache blocking, per-step
allocation in iterative algorithms, and high-level array abstraction
overhead.  Rounds~2--6 systematically attacked these factors, progressing
from algorithmic improvements through raw \texttt{ByteArray\#} primops to
SIMD vectorisation.  Rounds~7--13 targeted the remaining eigenvalue and
SVD gaps, while Rounds~14--18 pushed all categories past LAPACK through
panel-based factorisations, register-blocked micro-kernels, and 8-wide
SIMD unrolling.

%% ====================================================================
\section{Round~2: In-Place Algorithms and Cache Blocking}
\label{sec:postopt}

Following the analysis in Section~\ref{sec:remedies}, four of the
proposed optimisations were implemented and benchmarked.  This section
presents the before/after comparison, demonstrating that the
optimisations proposed in Section~\ref{sec:discussion} yield
order-of-magnitude improvements for factorisation and iterative
algorithms.

\subsection{Optimisations Implemented}

\begin{enumerate}
\item \textbf{Cache-blocked GEMM with ikj loop reorder.}
  The na\"ive ijk inner-product GEMM was replaced with a $32 \times 32$
  block-tiled ikj variant (GVL4~\cite{gvl4} Algorithm~1.3.1).  The ikj
  loop ordering ensures unit-stride access to both $C$ and $B$, while
  the $32 \times 32$ tile size keeps three blocks within L1 cache.
  This combines the loop-reorder and cache-blocking strategies from
  Sections~\ref{sec:remedy-blocking}.

\item \textbf{In-place QR factorisation via the ST monad.}
  The Householder QR factorisation was rewritten to operate entirely
  in the \texttt{ST}~monad, as proposed in
  Section~\ref{sec:remedy-inplace}.  The $R$ factor is computed by
  mutating the input matrix in-place, and the Householder vectors are
  stored implicitly below the diagonal (compact storage), eliminating
  all intermediate matrix allocations.  The explicit $Q$ factor is
  formed only when requested, by back-accumulating the stored
  reflectors.

\item \textbf{In-place tridiagonalisation and eigenvalue QR iteration
  via the ST monad.}
  The symmetric eigenvalue solver was rewritten to perform
  tridiagonalisation and the implicit QR iteration entirely in-place
  using mutable vectors in the \texttt{ST}~monad.  Diagonal and
  subdiagonal elements are updated via direct reads and writes rather
  than reconstructing the full tridiagonal matrix at each step,
  eliminating the $O(n^2)$ per-iteration allocation overhead identified
  in Section~\ref{sec:remedy-inplace}.

\item \textbf{Sub-range QR with top/bottom/interior deflation.}
  A practical divide-and-conquer deflation strategy was added to the
  tridiagonal QR iteration: at each step, negligible subdiagonal
  entries (below machine epsilon times the local diagonal norm) are
  detected, and the iteration range is narrowed to the largest
  unreduced block.  Top deflation, bottom deflation, and interior
  splitting are all handled, as described in GVL4~\cite{gvl4}
  Section~8.3.5.  This reduces the number of QR sweeps substantially
  for well-separated eigenvalues and provides the convergence
  acceleration benefits of divide-and-conquer
  (Section~\ref{sec:remedy-dc}) without the complexity of the full
  secular-equation approach.
\end{enumerate}

\subsection{Before/After Comparison}

Table~\ref{tab:postopt-qr} shows the QR factorisation timings before
and after optimisation.  Table~\ref{tab:postopt-eigen} shows the
corresponding results for the symmetric eigenvalue decomposition, and
Table~\ref{tab:postopt-svd} for the SVD.

\begin{table}[h]
\centering
\caption{QR factorisation: before and after optimisation (single-threaded).}
\label{tab:postopt-qr}
\begin{tabular}{@{}rrrrrr@{}}
\toprule
{Size} & {\texttt{hmatrix}} & {Old \texttt{l-m}} & {New \texttt{l-m}} & {Old ratio} & {New ratio} \\
\midrule
$10\times10$   & \SI{0.140}{\milli\second} & \SI{11.06}{\milli\second} & \SI{0.54}{\milli\second}  & $51\times$       & $3.9\times$ \\
$50\times50$   & \SI{11.32}{\milli\second} & \SI{7.01}{\second}        & \SI{61.91}{\milli\second} & $382\times$      & $5.5\times$ \\
$100\times100$ & \SI{129.6}{\milli\second} & $\approx$\SI{56}{\second} & \SI{492}{\milli\second}   & $\approx 260\times$ & $3.8\times$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Symmetric eigenvalue decomposition: before and after optimisation (single-threaded).}
\label{tab:postopt-eigen}
\begin{tabular}{@{}rrrrrr@{}}
\toprule
{Size} & {\texttt{hmatrix}} & {Old \texttt{l-m}} & {New \texttt{l-m}} & {Old ratio} & {New ratio} \\
\midrule
$10\times10$ & \SI{12.18}{\micro\second} & \SI{15.63}{\milli\second} & \SI{0.60}{\milli\second} & $897\times$      & $49\times$ \\
$50\times50$ & \SI{427.7}{\micro\second} & \SI{8.89}{\second}        & \SI{50.95}{\milli\second} & $16{,}020\times$ & $119\times$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{SVD: before and after optimisation (single-threaded).}
\label{tab:postopt-svd}
\begin{tabular}{@{}rrrrrr@{}}
\toprule
{Size} & {\texttt{hmatrix}} & {Old \texttt{l-m}} & {New \texttt{l-m}} & {Old ratio} & {New ratio} \\
\midrule
$10\times10$ & \SI{24.52}{\micro\second} & $\approx$\SI{50}{\milli\second} & \SI{1.58}{\milli\second}  & $\approx 2{,}039\times$ & $65\times$ \\
$50\times50$ & \SI{518.0}{\micro\second} & (timed out)                      & \SI{187}{\milli\second}   & $> 20{,}000\times$      & $361\times$ \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:postopt-gemm} shows the GEMM results.  The
cache-blocked ikj implementation yields modest improvements at sizes
where the original loop ordering suffered the worst cache behaviour,
while introducing slight tiling overhead at intermediate sizes.

\begin{table}[h]
\centering
\caption{GEMM: before and after optimisation (single-threaded, \texttt{linear-massiv}/\texttt{hmatrix} ratio).}
\label{tab:postopt-gemm}
\begin{tabular}{@{}rrr@{}}
\toprule
{Size} & {Old ratio} & {New ratio} \\
\midrule
$4\times4$     & $53\times$  & $60\times$ \\
$10\times10$   & $291\times$ & $227\times$ \\
$50\times50$   & $316\times$ & $423\times$ \\
$100\times100$ & $329\times$ & $354\times$ \\
$200\times200$ & $297\times$ & $259\times$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Discussion of Post-Optimisation Results}

The results demonstrate that the in-place ST monad refactoring and
implicit Householder storage---the two highest-priority items from
Section~\ref{sec:remedies}---delivered transformative improvements for
factorisation and iterative algorithms:

\begin{itemize}
\item \textbf{QR factorisation} improved by $13\text{--}113\times$
  internally (i.e., comparing old to new \texttt{linear-massiv}
  timings), bringing the ratio to hmatrix down from
  $51\text{--}382\times$ to $3.8\text{--}5.5\times$.  At $100 \times
  100$, where the old implementation could not complete within a
  reasonable time budget, the optimised version runs in
  \SI{492}{\milli\second}---within $3.8\times$ of hmatrix's
  \SI{130}{\milli\second}.  This confirms the prediction in
  Section~\ref{sec:remedy-inplace} that eliminating per-step allocation
  would bring QR performance in line with LU solve.

\item \textbf{Symmetric eigenvalue decomposition} improved by
  $26\text{--}174\times$ internally.  The remaining gap to hmatrix
  ($49\text{--}119\times$) reflects the fundamental difference between
  the classical tridiagonal QR algorithm (used by
  \texttt{linear-massiv}) and LAPACK's divide-and-conquer
  \texttt{DSYEVD}, which has better asymptotic constants, combined
  with OpenBLAS's SIMD-optimised inner loops.

\item \textbf{SVD} improved by $32\text{--}200\times$ internally.  The
  $50 \times 50$ case, which previously timed out, now completes in
  \SI{187}{\milli\second}.  The remaining $65\text{--}361\times$ gap
  to hmatrix reflects the compound effect of eigenvalue and QR
  sub-steps; further improvement would require optimising the
  bidiagonalisation phase and implementing a divide-and-conquer SVD.

\item \textbf{GEMM} showed mixed results from the $32 \times 32$
  block tiling.  At $200 \times 200$, the ratio improved from
  $297\times$ to $259\times$ (a 13\% improvement), and at $10 \times
  10$ from $291\times$ to $227\times$ (a 22\% improvement).  However,
  at $50 \times 50$ the tiling overhead slightly worsened performance
  ($316\times$ to $423\times$), suggesting that the block size should
  be tuned or that tiling should be bypassed for matrices smaller than
  the tile size.  The GEMM gap remains large because the dominant
  factor is SIMD utilisation rather than cache access patterns.
\end{itemize}

\noindent
Table~\ref{tab:postopt-ratios} provides an updated summary of
performance ratios after all four optimisations, comparable to the
pre-optimisation Table~\ref{tab:ratios}.

\begin{table}[h]
\centering
\caption{Updated performance ratio after optimisation:
\texttt{linear-massiv} time / \texttt{hmatrix} time.
Operations not re-benchmarked use the original values from Table~\ref{tab:ratios}.}
\label{tab:postopt-ratios}
\begin{tabular}{@{}lrrr@{}}
\toprule
{Operation} & {$n = 10$} & {$n = 50$} & {$n = 100$} \\
\midrule
GEMM (optimised)       & $227\times$ & $423\times$ & $354\times$ \\
Dot product            & ---         & ---         & $46\times$  \\
Matrix--vector         & ---         & $330\times$ & $334\times$ \\
LU solve               & $36\times$  & $233\times$ & $295\times$ \\
Cholesky solve         & $39\times$  & $201\times$ & $240\times$ \\
QR (optimised)         & $3.9\times$ & $5.5\times$ & $3.8\times$ \\
Eigenvalue (optimised) & $49\times$  & $119\times$ & ---         \\
SVD (optimised)        & $65\times$  & $361\times$ & ---         \\
\bottomrule
\end{tabular}
\end{table}

The most striking result is that QR factorisation has moved from being
the worst-performing operation (up to $382\times$ slower) to one of
the best ($3.8\text{--}5.5\times$), validating the analysis that
allocation overhead---not algorithmic complexity---was the dominant
bottleneck.  The eigenvalue and SVD improvements are also dramatic
in absolute terms ($174\times$ internal speedup for eigenvalues at
$50 \times 50$), though the remaining gap to hmatrix is larger because
these operations compound multiple algorithmic phases, each with its
own constant-factor overhead.

%% ====================================================================
\section{Round~3: Raw ByteArray\# and AVX2 SIMD}
\label{sec:simd}

Following the analysis in Sections~\ref{sec:discussion}
and~\ref{sec:remedy-simd}, the remaining performance gap for BLAS
Level~1--3 operations was traced to massiv's per-element abstraction
layer.  Profiling the inner loop of the tiled GEMM kernel revealed that
each iteration of \texttt{M.readM}/\texttt{M.write\_}/\texttt{mapM\_}
over list ranges incurred approximately 2{,}400 cycles of overhead
(closure allocation, bounds checking, boxed intermediate values) versus
the $\sim$10 cycles expected for a raw memory load--FMA--store
sequence---a \textbf{$240\times$ per-element overhead}.

\subsection{Optimisations Implemented}

The fix was to bypass massiv's element-access layer entirely in hot
inner loops, operating directly on the underlying
\texttt{ByteArray\#}/\texttt{MutableByteArray\#} storage and using
GHC~9.14's \texttt{DoubleX4\#} AVX2 SIMD primops for 256-bit
vectorised arithmetic.  The following changes were made:

\begin{enumerate}
\item \textbf{New raw kernel module (\texttt{Internal.Kernel}).}
  A dedicated module was created containing all performance-critical
  inner loops written in terms of GHC primitive operations:
  \texttt{indexDoubleArray\#}, \texttt{readDoubleArray\#},
  \texttt{writeDoubleArray\#} for scalar access, and
  \texttt{indexDoubleArrayAsDoubleX4\#},
  \texttt{readDoubleArrayAsDoubleX4\#},
  \texttt{writeDoubleArrayAsDoubleX4\#} with
  \texttt{fmaddDoubleX4\#} for 4-wide fused multiply-add SIMD.

\item \textbf{SIMD dot product (\texttt{rawDot}).}
  The inner product accumulates four doubles per iteration using a
  \texttt{DoubleX4\#} FMA accumulator, with scalar cleanup for the
  remainder ($n \bmod 4$) and a horizontal sum via
  \texttt{unpackDoubleX4\#}.

\item \textbf{SIMD matrix--vector multiply (\texttt{rawGemv}).}
  For each row~$i$, calls \texttt{rawDot} on row~$i$ of~$A$ and
  vector~$x$, writing the result directly to the output
  \texttt{MutableByteArray\#}.

\item \textbf{SIMD tiled GEMM kernel (\texttt{rawGemmKernel}).}
  A $64 \times 64$ block-tiled ikj GEMM operating on raw arrays.
  The innermost $j$-loop processes four columns simultaneously via
  \texttt{DoubleX4\#}: load 4 elements of $B(k, j{:}j{+}3)$, load 4
  of $C(i, j{:}j{+}3)$, fused multiply-add with broadcast $A(i,k)$,
  store back.  \texttt{State\#} threading is used throughout with no
  ST~monad wrapper in the hot loop.

\item \textbf{Compiler backend.}
  GHC~9.14.1 with the LLVM~17 backend (\texttt{-fllvm}) and
  \texttt{-mavx2 -mfma} flags, which lowers \texttt{DoubleX4\#}
  primops to native \texttt{vfmadd231pd ymm} instructions.

\item \textbf{Specialised \texttt{P Double} entry points.}
  Functions \texttt{matMulP}, \texttt{dotP}, and \texttt{matvecP} are
  exported alongside the generic polymorphic versions.  These extract
  the raw \texttt{ByteArray\#} from massiv's \texttt{P}rimitive
  representation via \texttt{unwrapByteArray}/\texttt{unwrapByteArrayOffset}
  and call the SIMD kernels directly.
\end{enumerate}

\subsection{Before/After Comparison}

Table~\ref{tab:simd-blas} presents the BLAS Level~1--3 timings before
and after the SIMD optimisation, compared with hmatrix.

\begin{table}[h]
\centering
\caption{BLAS operations: before SIMD, after SIMD, and hmatrix (single-threaded).
Ratios are \texttt{linear-massiv}/\texttt{hmatrix}; values $< 1$ mean
\texttt{linear-massiv} is faster.}
\label{tab:simd-blas}
\begin{tabular}{@{}llrrrr@{}}
\toprule
{Operation} & {Size} & {\texttt{hmatrix}} & {Old \texttt{l-m}} & {New \texttt{l-m}} & {New ratio} \\
\midrule
\multirow{5}{*}{GEMM}
  & $4\times4$     & \SI{602}{\nano\second}    & \SI{34.5}{\micro\second}  & \SI{873}{\nano\second}   & $1.45\times$ \\
  & $10\times10$   & \SI{2.17}{\micro\second}  & \SI{678}{\micro\second}   & \SI{2.66}{\micro\second} & $1.23\times$ \\
  & $50\times50$   & \SI{144}{\micro\second}   & \SI{55.0}{\milli\second}  & \SI{112}{\micro\second}  & $\mathbf{0.78\times}$ \\
  & $100\times100$ & \SI{1.46}{\milli\second}  & \SI{505}{\milli\second}   & \SI{796}{\micro\second}  & $\mathbf{0.55\times}$ \\
  & $200\times200$ & \SI{12.9}{\milli\second}  & \SI{4.09}{\second}        & \SI{6.10}{\milli\second} & $\mathbf{0.47\times}$ \\
\midrule
\multirow{3}{*}{Dot}
  & $n=4$   & \SI{584}{\nano\second}   & \SI{1.67}{\micro\second}  & \SI{48}{\nano\second}    & $\mathbf{0.08\times}$ \\
  & $n=100$ & \SI{762}{\nano\second}   & \SI{34.1}{\micro\second}  & \SI{80}{\nano\second}    & $\mathbf{0.10\times}$ \\
  & $n=1000$ & \SI{2.81}{\micro\second} & \SI{379}{\micro\second}  & \SI{688}{\nano\second}   & $\mathbf{0.24\times}$ \\
\midrule
\multirow{3}{*}{Matvec}
  & $n=4$   & \SI{411}{\nano\second}   & \SI{11.2}{\micro\second} & \SI{563}{\nano\second}   & $1.37\times$ \\
  & $n=50$  & \SI{3.15}{\micro\second} & \SI{1.24}{\milli\second} & \SI{1.94}{\micro\second} & $\mathbf{0.62\times}$ \\
  & $n=100$ & \SI{13.3}{\micro\second} & \SI{4.71}{\milli\second} & \SI{5.94}{\micro\second} & $\mathbf{0.45\times}$ \\
\bottomrule
\end{tabular}
\end{table}

The internal speedups are dramatic:
\begin{itemize}
\item GEMM $100 \times 100$: \SI{505}{\milli\second} $\to$
  \SI{796}{\micro\second} = $\mathbf{635\times}$ faster.
\item GEMM $200 \times 200$: \SI{4.09}{\second} $\to$
  \SI{6.10}{\milli\second} = $\mathbf{671\times}$ faster.
\item Dot $n = 100$: \SI{34.1}{\micro\second} $\to$
  \SI{80}{\nano\second} = $\mathbf{426\times}$ faster.
\item Matvec $n = 100$: \SI{4.71}{\milli\second} $\to$
  \SI{5.94}{\micro\second} = $\mathbf{793\times}$ faster.
\end{itemize}

\subsection{Discussion of SIMD Results}

The most striking result is that \textbf{\texttt{linear-massiv} now
outperforms \texttt{hmatrix} (OpenBLAS) for BLAS Level~1--3 operations
at dimensions $\geq 50$}.  At $200 \times 200$, the SIMD GEMM kernel
completes in \SI{6.10}{\milli\second} versus hmatrix's
\SI{12.9}{\milli\second}---a $2.1\times$ advantage for pure Haskell.
This reversal (from $297\times$ slower to $2.1\times$ faster) validates
the prediction in Section~\ref{sec:remedy-simd} that SIMD primops would
be the dominant factor for closing the BLAS gap.

The advantage of the pure-Haskell SIMD approach over FFI-based BLAS is
threefold: (1)~zero FFI call overhead per invocation, which is
significant for small-to-medium matrices; (2)~the LLVM backend generates
native \texttt{vfmadd231pd ymm} instructions directly from
\texttt{DoubleX4\#} primops without the overhead of a C function call
frame; and (3)~the $64 \times 64$ tile size is well-tuned for L1 cache
residency on modern x86 microarchitectures.

For the dot product, the \SI{48}{\nano\second} timing at $n = 4$
($12\times$ faster than hmatrix's \SI{584}{\nano\second}) reflects the
elimination of FFI overhead entirely---the SIMD kernel processes all
four elements in a single \texttt{DoubleX4\#} FMA operation with no
function call boundary.

The remaining performance gaps are now confined to higher-level
algorithms that were not targeted by the SIMD kernels:
\begin{itemize}
\item LU and Cholesky solvers ($40\text{--}255\times$) still use
  massiv's per-element indexing in the factorisation and back-substitution
  phases.
\item QR factorisation ($3.9\text{--}4.9\times$) uses in-place ST
  operations but does not yet use SIMD for the Householder reflector
  application.
\item Eigenvalue ($35\text{--}142\times$) and SVD ($62\text{--}330\times$)
  combine multiple algorithmic phases, each with per-element overhead;
  additionally LAPACK uses superior divide-and-conquer algorithms.
\end{itemize}

Table~\ref{tab:simd-ratios} provides the updated summary of performance
ratios incorporating the SIMD optimisation.

\begin{table}[h]
\centering
\caption{Updated performance ratio after SIMD optimisation:
\texttt{linear-massiv} time / \texttt{hmatrix} time.
Values $< 1$ (bold) indicate \texttt{linear-massiv} is faster.}
\label{tab:simd-ratios}
\begin{tabular}{@{}lrrr@{}}
\toprule
{Operation} & {$n = 10$} & {$n = 50$} & {$n = 100$} \\
\midrule
GEMM (SIMD)            & $1.2\times$            & $\mathbf{0.78\times}$  & $\mathbf{0.55\times}$ \\
Dot product (SIMD)     & ---                    & ---                    & $\mathbf{0.10\times}$ \\
Matrix--vector (SIMD)  & ---                    & $\mathbf{0.62\times}$  & $\mathbf{0.45\times}$ \\
LU solve               & $40\times$             & $233\times$            & $255\times$ \\
Cholesky solve         & $36\times$             & $175\times$            & $213\times$ \\
QR (in-place)          & $3.9\times$            & $4.9\times$            & $3.9\times$ \\
Eigenvalue             & $35\times$             & $142\times$            & ---         \\
SVD                    & $62\times$             & $330\times$            & ---         \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{loglogaxis}[
  xlabel={Matrix dimension $n$},
  ylabel={Time (seconds)},
  title={GEMM: Before and After SIMD (vs.\ hmatrix)},
  legend pos=north west,
  legend style={font=\small},
  grid=major,
  width=0.85\textwidth,
  height=0.55\textwidth,
]
\addplot[color=hmatrixblue, mark=square*, thick] coordinates {
  (4, 6.02e-7) (10, 2.17e-6) (50, 1.44e-4) (100, 1.46e-3) (200, 1.29e-2)
};
\addlegendentry{hmatrix (OpenBLAS)}
\addplot[color=massivred, mark=triangle*, thick, dashed] coordinates {
  (4, 3.45e-5) (10, 6.78e-4) (50, 5.50e-2) (100, 5.05e-1) (200, 4.09)
};
\addlegendentry{linear-massiv (before SIMD)}
\addplot[color=massivred, mark=triangle*, thick] coordinates {
  (4, 8.73e-7) (10, 2.66e-6) (50, 1.12e-4) (100, 7.96e-4) (200, 6.10e-3)
};
\addlegendentry{linear-massiv (AVX2 SIMD)}
\addplot[color=lineargreen, mark=o, thick] coordinates {
  (4, 1.01e-7)
};
\addlegendentry{linear (V4 only)}
\end{loglogaxis}
\end{tikzpicture}
\caption{GEMM scaling comparison after SIMD optimisation. At $n \geq 50$,
\texttt{linear-massiv}'s AVX2 kernel outperforms hmatrix (OpenBLAS),
achieving $2.1\times$ faster execution at $200 \times 200$.
The dashed line shows the pre-SIMD performance.}
\label{fig:gemm-simd}
\end{figure}

\subsection{Remaining Bottlenecks and Future Work}

With BLAS Level~1--3 now faster than OpenBLAS, the remaining performance
gaps are concentrated in higher-level algorithms:

\begin{enumerate}
\item \textbf{LU and Cholesky factorisation.}
  These solvers still use massiv's per-element \texttt{M.readM}/\texttt{M.write\_}
  for the factorisation phase.  Rewriting the inner loops of LU
  pivoting and Cholesky's column updates with raw \texttt{ByteArray\#}
  primops (analogous to the GEMM kernel) would likely yield
  $100\text{--}200\times$ speedups, bringing these within a small
  constant factor of LAPACK.

\item \textbf{QR Householder reflector application.}
  The \texttt{rawHouseholderApplyCol} and \texttt{rawQAccumCol} SIMD
  kernels were implemented in \texttt{Internal.Kernel} but not yet
  wired into the QR factorisation due to the deeply intertwined
  generic-representation loop structure.  Refactoring QR to use the
  raw kernels for the \texttt{P Double} case would close the remaining
  $3.9\text{--}4.9\times$ gap.

\item \textbf{Eigenvalue and SVD.}
  The $35\text{--}330\times$ gaps reflect both per-element overhead
  (addressable by raw kernel wiring) and algorithmic differences
  (LAPACK's divide-and-conquer vs.\ classical QR iteration).
  Implementing a divide-and-conquer tridiagonal eigensolver
  (GVL4~\cite{gvl4} Section~8.3.3) and a divide-and-conquer
  bidiagonal SVD would address the algorithmic component.

\item \textbf{Parallel SIMD GEMM.}
  The current SIMD GEMM kernel is single-threaded.  Combining the
  raw kernel with massiv's \texttt{Par}/\texttt{ParN} strategies
  (e.g., parallelising the outer block-$i$ loop) would yield
  further speedups proportional to core count.
\end{enumerate}

%% ====================================================================
\section{Round~4: Raw Kernels for Solvers and Factorisations}
\label{sec:rawkernels}

With BLAS Level~1--3 operations now outperforming OpenBLAS
(Section~\ref{sec:simd}), the dominant remaining bottleneck was
massiv's per-element \texttt{M.readM}/\texttt{M.write\_} overhead in
higher-level algorithms---LU factorisation, Cholesky factorisation, QR
Householder application, and eigenvalue Givens rotations.  This section
describes the extension of the raw \texttt{ByteArray\#} kernel technique
to these algorithms, completing the optimisation programme outlined in
Section~\ref{sec:simd}.

\subsection{Optimisations Implemented}

\begin{enumerate}
\item \textbf{LU factorisation and solve (\texttt{luSolveP}).}
  Five new raw kernels:
  \texttt{rawLUEliminateColumn} (the $O(n^3)$ elimination loop with
  \texttt{DoubleX4\#} SIMD for the contiguous $j$-loop),
  \texttt{rawSwapRows} (SIMD row swap), \texttt{rawPivotSearch}
  (partial pivoting), \texttt{rawForwardSubUnitPacked} and
  \texttt{rawBackSubPacked} (triangular solve on the packed LU factor
  without extracting separate $L$ and $U$ matrices).  The combined
  \texttt{luSolveP} performs factorisation and solve in a single pass
  over the packed representation, eliminating the costly $L$/$U$
  matrix reconstruction that dominated the previous implementation.

\item \textbf{Cholesky factorisation and solve (\texttt{choleskySolveP}).}
  Three new raw kernels:
  \texttt{rawCholColumn} (column-oriented Cholesky with
  \texttt{sqrtDouble\#}),
  \texttt{rawForwardSubCholPacked} and
  \texttt{rawBackSubCholTPacked} (back-substitution with $G^T$
  accessed implicitly as $G^T_{ij} = G_{ji}$, avoiding explicit
  transpose construction).

\item \textbf{QR factorisation (\texttt{qrP}).}
  Four new mutable-array kernels:
  \texttt{rawMutSumSqColumn} (column sum-of-squares),
  \texttt{rawMutSumProdColumns} (column dot product),
  \texttt{rawMutHouseholderApply} (Householder reflector application
  with implicit $v_k = 1$), and
  \texttt{rawMutQAccum} (Q~accumulation row update from frozen
  reflector storage).  These replace the \texttt{M.readM}-based inner
  loops in both the triangularisation and Q~accumulation phases.

\item \textbf{Symmetric eigenvalue (\texttt{symmetricEigenP}).}
  The Givens rotation application in the implicit QR iteration was
  replaced with \texttt{rawMutApplyGivensColumns}, operating directly
  on \texttt{MutableByteArray\#}.  The P-specialised eigenvalue chain
  (\texttt{symmetricEigenP} $\to$ \texttt{tridiagQRLoopP} $\to$
  \texttt{implicitQRStepInPlaceP}) avoids the overhead of the generic
  \texttt{applyGivensRightQ} for the \texttt{P Double} representation.
\end{enumerate}

\subsection{Before/After Comparison}

Table~\ref{tab:raw-lu} presents the LU solve timings; Table~\ref{tab:raw-chol}
the Cholesky solve; Table~\ref{tab:raw-qr} the QR factorisation; and
Table~\ref{tab:raw-eigen} the symmetric eigenvalue decomposition.

\begin{table}[h]
\centering
\caption{LU solve ($Ax = b$): before and after raw kernel optimisation
(single-threaded).  ``Old'' is the generic \texttt{luSolve};
``New'' is the P-specialised \texttt{luSolveP}.
Ratio $< 1$ (bold) means \texttt{linear-massiv} is faster than hmatrix.}
\label{tab:raw-lu}
\begin{tabular}{@{}rrrrrr@{}}
\toprule
{Size} & {\texttt{hmatrix}} & {Old \texttt{l-m}} & {New \texttt{l-m}} & {Old ratio} & {New ratio} \\
\midrule
$10\times10$   & \SI{4.66}{\micro\second}  & \SI{201}{\micro\second}    & \SI{1.72}{\micro\second}  & $43\times$  & $\mathbf{0.37\times}$ \\
$50\times50$   & \SI{60.2}{\micro\second}  & \SI{14.7}{\milli\second}   & \SI{31.6}{\micro\second}  & $244\times$ & $\mathbf{0.52\times}$ \\
$100\times100$ & \SI{349}{\micro\second}   & \SI{108.8}{\milli\second}  & \SI{211}{\micro\second}   & $312\times$ & $\mathbf{0.61\times}$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Cholesky solve ($Ax = b$, $A$ SPD): before and after raw kernel
optimisation (single-threaded).}
\label{tab:raw-chol}
\begin{tabular}{@{}rrrrrr@{}}
\toprule
{Size} & {\texttt{hmatrix}} & {Old \texttt{l-m}} & {New \texttt{l-m}} & {Old ratio} & {New ratio} \\
\midrule
$10\times10$   & \SI{4.81}{\micro\second} & \SI{160}{\micro\second}   & \SI{1.59}{\micro\second} & $33\times$  & $\mathbf{0.33\times}$ \\
$50\times50$   & \SI{54.7}{\micro\second} & \SI{7.82}{\milli\second}  & \SI{45.3}{\micro\second} & $143\times$ & $\mathbf{0.83\times}$ \\
$100\times100$ & \SI{251}{\micro\second}  & \SI{53.5}{\milli\second}  & \SI{261}{\micro\second}  & $213\times$ & $1.04\times$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{QR factorisation (Householder): before and after raw kernel
optimisation (single-threaded).}
\label{tab:raw-qr}
\begin{tabular}{@{}rrrrrr@{}}
\toprule
{Size} & {\texttt{hmatrix}} & {Old \texttt{l-m}} & {New \texttt{l-m}} & {Old ratio} & {New ratio} \\
\midrule
$10\times10$   & \SI{151}{\micro\second}   & \SI{497}{\micro\second}    & \SI{19.9}{\micro\second}  & $3.3\times$ & $\mathbf{0.13\times}$ \\
$50\times50$   & \SI{11.0}{\milli\second}  & \SI{64.8}{\milli\second}   & \SI{642}{\micro\second}   & $5.9\times$ & $\mathbf{0.058\times}$ \\
$100\times100$ & \SI{139}{\milli\second}   & \SI{480}{\milli\second}    & \SI{4.17}{\milli\second}  & $3.5\times$ & $\mathbf{0.030\times}$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Symmetric eigenvalue decomposition: before and after raw kernel
optimisation (single-threaded).}
\label{tab:raw-eigen}
\begin{tabular}{@{}rrrrrr@{}}
\toprule
{Size} & {\texttt{hmatrix}} & {Old \texttt{l-m}} & {New \texttt{l-m}} & {Old ratio} & {New ratio} \\
\midrule
$10\times10$ & \SI{11.9}{\micro\second}  & \SI{594}{\micro\second}    & \SI{473}{\micro\second}   & $50\times$  & $40\times$ \\
$50\times50$ & \SI{425}{\micro\second}   & \SI{49.8}{\milli\second}   & \SI{57.3}{\milli\second}  & $117\times$ & $135\times$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Discussion of Raw Kernel Results}

The results reveal a clear dichotomy between the operations where raw
kernels yielded dramatic improvements and the eigenvalue solver where
gains were marginal.

\paragraph{LU solve: $43\text{--}312\times$ slower $\to$
$1.7\text{--}2.7\times$ faster.}
The raw kernel LU solve represents the most dramatic single improvement
in this report.  At $100 \times 100$, the P-specialised
\texttt{luSolveP} completes in \SI{211}{\micro\second} versus
hmatrix's \SI{349}{\micro\second}---a $1.65\times$ advantage for pure
Haskell.  The $516\times$ internal speedup (from \SI{108.8}{\milli\second}
to \SI{211}{\micro\second}) reflects two compounding improvements:
(a)~raw primop elimination of the per-element overhead, and
(b)~packed solve that avoids the previous implementation's expensive
extraction of separate $L$ and $U$ matrices.  The SIMD-vectorised
$j$-loop in \texttt{rawLUEliminateColumn}---where elements $A[i,j]$
and $A[k,j]$ are contiguous in row-major storage---provides an
additional $\sim 3\text{--}4\times$ boost over scalar raw primops.

\paragraph{Cholesky solve: $33\text{--}213\times$ slower $\to$
$3\times$ faster to parity.}
Cholesky shows strong gains at small dimensions ($3\times$ faster than
hmatrix at $10 \times 10$) but converges to parity at $100 \times 100$
($1.04\times$).  The Cholesky column update is intrinsically
stride-$n$ (column access in row-major), preventing SIMD vectorisation
of the innermost loop.  At $n = 100$, LAPACK's column-major storage
allows unit-stride column access, giving it a small advantage.
Nevertheless, eliminating the $205\times$ overhead from massiv's
abstraction layer closes the gap entirely.

\paragraph{QR: $3.3\text{--}5.9\times$ slower $\to$
$7.6\text{--}33\times$ faster.}
QR factorisation shows the most remarkable absolute performance:
\texttt{qrP} is \textbf{$33\times$ faster than LAPACK's
\texttt{DGEQRF}} at $100 \times 100$ (\SI{4.17}{\milli\second} vs.\
\SI{139}{\milli\second}).  This surprising result likely reflects that
hmatrix calls LAPACK's \texttt{DGEQRF} followed by \texttt{DORGQR}
to form the explicit $Q$ matrix, while \texttt{qrP} performs both
triangularisation and Q~accumulation in a single ST~monad pass with
raw primops.  The raw kernel Householder application avoids the
abstraction overhead that previously dominated.

\paragraph{Eigenvalue: marginal improvement ($1.3\times$ at best).}
The P-specialised eigenvalue solver showed negligible improvement, and
was actually slightly slower at $50 \times 50$.  This is because the
Givens rotation application---the only phase converted to raw
kernels---represents a small fraction of the total cost.  The dominant
bottleneck is the tridiagonal QR iteration loop itself, which uses
\texttt{M.readM}/\texttt{M.write\_} on mutable vectors for the
diagonal and subdiagonal elements, and computes Givens parameters
($c$, $s$) using boxed arithmetic.  Additionally, LAPACK's
\texttt{DSYEVD} uses a fundamentally different algorithm
(divide-and-conquer) with better asymptotic constants.  Closing the
eigenvalue gap would require either converting the entire QR iteration
to raw primops or implementing a divide-and-conquer eigensolver.

\subsection{Updated Summary}

Table~\ref{tab:raw-ratios} presents the comprehensive performance ratio
after all four rounds of optimisation.

\begin{table}[h]
\centering
\caption{Final performance ratio after all optimisations:
\texttt{linear-massiv} time / \texttt{hmatrix} time.
Values $< 1$ (bold) indicate \texttt{linear-massiv} is faster.}
\label{tab:raw-ratios}
\begin{tabular}{@{}lrrr@{}}
\toprule
{Operation} & {$n = 10$} & {$n = 50$} & {$n = 100$} \\
\midrule
GEMM (SIMD)            & $1.0\times$            & $\mathbf{0.62\times}$  & $\mathbf{0.60\times}$ \\
Dot product (SIMD)     & ---                    & ---                    & $\mathbf{0.12\times}$ \\
Matrix--vector (SIMD)  & $1.4\times$            & $\mathbf{0.65\times}$  & $\mathbf{0.49\times}$ \\
LU solve (raw)         & $\mathbf{0.37\times}$  & $\mathbf{0.52\times}$  & $\mathbf{0.61\times}$ \\
Cholesky solve (raw)   & $\mathbf{0.33\times}$  & $\mathbf{0.83\times}$  & $1.04\times$ \\
QR (raw)               & $\mathbf{0.13\times}$  & $\mathbf{0.058\times}$ & $\mathbf{0.030\times}$ \\
Eigenvalue (raw)       & $40\times$             & $135\times$            & ---         \\
SVD                    & $74\times$             & $292\times$            & ---         \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{loglogaxis}[
  xlabel={Matrix dimension $n$},
  ylabel={Time (seconds)},
  title={Solver Performance: Final Comparison},
  legend pos=north west,
  legend style={font=\footnotesize},
  grid=major,
  width=0.85\textwidth,
  height=0.6\textwidth,
]
\addplot[color=hmatrixblue, mark=square*, thick] coordinates {
  (10, 4.66e-6) (50, 6.02e-5) (100, 3.49e-4)
};
\addlegendentry{hmatrix LU}
\addplot[color=massivred, mark=triangle*, thick] coordinates {
  (10, 1.72e-6) (50, 3.16e-5) (100, 2.11e-4)
};
\addlegendentry{l-m LU (raw)}
\addplot[color=hmatrixblue, mark=square, thick, dashed] coordinates {
  (10, 4.81e-6) (50, 5.47e-5) (100, 2.51e-4)
};
\addlegendentry{hmatrix Cholesky}
\addplot[color=massivred, mark=triangle, thick, dashed] coordinates {
  (10, 1.59e-6) (50, 4.53e-5) (100, 2.61e-4)
};
\addlegendentry{l-m Cholesky (raw)}
\addplot[color=hmatrixblue, mark=diamond*, thick, dotted] coordinates {
  (10, 1.51e-4) (50, 1.10e-2) (100, 1.39e-1)
};
\addlegendentry{hmatrix QR}
\addplot[color=massivred, mark=pentagon*, thick, dotted] coordinates {
  (10, 1.99e-5) (50, 6.42e-4) (100, 4.17e-3)
};
\addlegendentry{l-m QR (raw)}
\end{loglogaxis}
\end{tikzpicture}
\caption{Final solver performance comparison (log--log).
\texttt{linear-massiv}'s raw kernel implementations (solid/dashed red)
outperform hmatrix (solid/dashed blue) for LU and Cholesky,
and dominate dramatically for QR.}
\label{fig:solve-final}
\end{figure}

\subsection{Remaining Bottlenecks and Future Work}
\label{sec:round4-future}

The remaining performance gaps are now confined to eigenvalue and SVD:

\begin{enumerate}
\item \textbf{Eigenvalue ($40\text{--}135\times$).}
  The tridiagonal QR iteration's inner loop
  (diagonal/subdiagonal updates, Givens parameter computation)
  still uses massiv's per-element abstraction.  Converting the
  \emph{entire} QR iteration---not just the Givens application---to
  raw \texttt{ByteArray\#} primops would likely yield
  $10\text{--}50\times$ improvement.  A divide-and-conquer
  tridiagonal eigensolver (GVL4~\cite{gvl4} Section~8.4) would
  address the remaining algorithmic gap.

\item \textbf{SVD ($74\text{--}292\times$).}
  SVD performance is bottlenecked by the eigenvalue sub-step (which
  calls the generic \texttt{symmetricEigen}) and the
  bidiagonalisation phase.  Wiring \texttt{symmetricEigenP} into the
  SVD pipeline and converting bidiagonalisation to raw primops would
  yield substantial gains.

\item \textbf{Parallel GEMM.}
  The SIMD GEMM kernel is single-threaded.  Parallelising the outer
  block-$i$ loop across cores would multiply throughput proportionally,
  extending the advantage over hmatrix.
\end{enumerate}

%% ====================================================================
\section{Round~5: SVD Pipeline and Parallel GEMM}
\label{sec:round5}

Following the proposals in Section~\ref{sec:round4-future}, Round~5
targets the three remaining bottlenecks: eigenvalue ($40\text{--}135\times$
slower), SVD ($74\text{--}292\times$ slower), and single-threaded GEMM.
Three optimisations were implemented:

\subsection{Optimisations Implemented}

\paragraph{1. Raw primop QR iteration (eigenvalue).}
The tridiagonal QR iteration in \texttt{symmetricEigenP} was rewritten
to use raw \texttt{ByteArray\#} primops for all diagonal and subdiagonal
reads/writes.  Two \texttt{INLINE} helpers, \texttt{readRawD} and
\texttt{writeRawD}, wrap \texttt{readDoubleArray\#} /
\texttt{writeDoubleArray\#} in the \texttt{ST} monad while preserving
readable code structure.  Three new functions replace the generic QR
iteration chain:
\begin{itemize}
\item \texttt{rawTridiagQRLoop}: deflation and shift logic via raw
  reads/writes;
\item \texttt{rawImplicitQRStep}: bulge-chasing Givens rotations with
  raw diagonal/subdiagonal updates and direct
  \texttt{rawMutApplyGivensColumns} calls;
\item \texttt{rawFindSplit}: interior deflation search via raw reads.
\end{itemize}
This eliminates $\sim$12 \texttt{M.readM}/\texttt{M.write\_} calls
per chase step and $\sim$11 per deflation check, removing the
$\sim$240$\times$ per-element overhead of massiv's indexing abstraction.

\paragraph{2. P-specialised SVD pipeline.}
A new \texttt{svdP} function wires together the optimised components:
\begin{itemize}
\item \texttt{matMulP} (SIMD GEMM) for the $A^T A$ computation,
  replacing the generic \texttt{matMul};
\item \texttt{symmetricEigenP} (raw primop QR iteration) for the
  eigendecomposition, replacing the generic \texttt{symmetricEigen};
\item \texttt{matvecP} (SIMD matrix--vector product) for computing
  each left singular vector $u_j = Av_j / \sigma_j$, replacing the
  scalar fold.
\end{itemize}
This eliminates three separate abstraction-overhead penalties in the
SVD pipeline.

\paragraph{3. Parallel GEMM.}
The SIMD GEMM kernel was refactored to expose \texttt{rawGemmBISlice},
which processes a specified row range $[\text{biStart}, \text{biEnd})$
of the output matrix.  A new \texttt{matMulPPar} function partitions
the row range across $\min(\text{cores}, m)$ threads using
\texttt{forkIO} + \texttt{MVar} barrier synchronisation.  Thread
safety is guaranteed because each thread writes exclusively to
non-overlapping rows of~$C$, while reading shared immutable
arrays~$A$ and~$B$.

\subsection{Before/After Comparison}

Table~\ref{tab:round5-comparison} compares Round~4 and Round~5
results.  All measurements are single-threaded ($\texttt{+RTS -N1}$)
for fair comparison against \texttt{hmatrix}.

\begin{table}[htbp]
\centering
\caption{Round~4 vs.\ Round~5 performance (single-threaded)}
\label{tab:round5-comparison}
\begin{tabular}{@{} l S[table-format=3.1] S[table-format=3.1] S[table-format=2.1] @{}}
\toprule
{Benchmark} & {Round~4 Ratio} & {Round~5 Ratio} & {Improvement} \\
            & {(lm/hmatrix)} & {(lm/hmatrix)} & {Factor} \\
\midrule
\multicolumn{4}{@{}l}{\emph{Eigenvalue}} \\
\quad 10$\times$10  & 39.6  & 40.9  & 1.0 \\
\quad 50$\times$50  & 134.6 & 148.8 & 0.9 \\
\midrule
\multicolumn{4}{@{}l}{\emph{SVD}} \\
\quad 10$\times$10  & 74.2  & 22.4  & 3.3 \\
\quad 50$\times$50  & 291.6 & 94.2  & 3.1 \\
\quad 100$\times$100 & {---} & 138.3 & {---} \\
\midrule
\multicolumn{4}{@{}l}{\emph{SVD (generic vs.\ P-specialised)}} \\
\quad 10$\times$10  & {---} & {3.1$\times$} & {---} \\
\quad 50$\times$50  & {---} & {3.6$\times$} & {---} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:round5-parallel} shows the parallel GEMM results with
all 20~cores enabled ($\texttt{+RTS -N}$).

\begin{table}[htbp]
\centering
\caption{Parallel GEMM performance (20 cores, \texttt{+RTS -N})}
\label{tab:round5-parallel}
\begin{tabular}{@{} l
  S[table-format=3.1]
  S[table-format=2.1]
  S[table-format=2.1]
  S[table-format=2.1] @{}}
\toprule
{Size} & {hmatrix (ms)} & {lm-single (ms)} & {lm-parallel (ms)} & {lm-par / hm} \\
\midrule
200$\times$200  & 12.0  & 6.5   & 2.2  & 0.19 \\
500$\times$500  & 263.0 & 92.3  & 19.4 & 0.07 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Discussion of Round~5 Results}

\paragraph{Eigenvalue: marginal impact.}
The raw primop conversion of the QR iteration loop had essentially no
measurable effect on eigenvalue performance (ratio unchanged at
$40\text{--}149\times$).  This confirms that the bottleneck is not in
the QR iteration's scalar read/write operations but in the
\emph{tridiagonalisation} phase (\texttt{tridiagonalize}), which is
$O(n^3)$ and still uses massiv's per-element abstraction via Haskell
lists.  The tridiagonalisation accounts for roughly half the total
eigendecomposition time at $n = 50$ and dominates at larger~$n$.

\paragraph{SVD: 3$\times$ improvement from pipeline wiring.}
Replacing the generic \texttt{matMul}, \texttt{symmetricEigen}, and
scalar fold with their P-specialised counterparts (\texttt{matMulP},
\texttt{symmetricEigenP}, \texttt{matvecP}) reduced the SVD penalty by
a factor of~3 across all tested sizes.  The SVD 10$\times$10 ratio
improved from $74\times$ to $22\times$; SVD 50$\times$50 improved from
$292\times$ to $94\times$.  This confirms that a significant fraction
of the SVD overhead was due to calling generic (non-SIMD) routines
rather than the eigenvalue sub-step alone.

\paragraph{Parallel GEMM: 13.5$\times$ faster than OpenBLAS.}
The \texttt{matMulPPar} function achieves a parallel speedup of
$4.8\times$ over single-threaded \texttt{matMulP} at $500 \times 500$
on 20~cores.  Combined with the $2.3\times$ single-threaded advantage,
this yields a total \textbf{13.5$\times$ speedup over hmatrix
(OpenBLAS)} at $500 \times 500$.  At $200 \times 200$, the parallel
speedup is $2.9\times$ over single-threaded, yielding a total
$5.3\times$ speedup over hmatrix.  The sub-linear scaling (4.8$\times$
on 20~cores) reflects the small per-thread work granularity at
$200 \times 200$ ($\sim$10 rows per thread) and memory bandwidth
saturation; larger matrices would benefit more.

\subsection{Updated Summary}

Table~\ref{tab:round5-summary} consolidates the performance of
\texttt{linear-massiv} relative to \texttt{hmatrix} (OpenBLAS/LAPACK)
after five rounds of optimisation.

\begin{table}[htbp]
\centering
\caption{Performance summary after Round~5 (best variant per operation)}
\label{tab:round5-summary}
\begin{tabular}{@{} l l c @{}}
\toprule
{Operation} & {Best Size} & {lm / hmatrix} \\
\midrule
GEMM (single-thread)     & 200$\times$200  & \textbf{0.49$\times$} \\
GEMM (parallel, 20 cores)& 500$\times$500  & \textbf{0.07$\times$} \\
Dot product               & 1000            & \textbf{0.33$\times$} \\
Matrix--vector            & 100             & \textbf{0.42$\times$} \\
LU solve                  & 100$\times$100  & \textbf{0.57$\times$} \\
Cholesky solve            & 10$\times$10    & \textbf{0.33$\times$} \\
QR factorisation          & 100$\times$100  & \textbf{0.03$\times$} \\
Eigenvalue                & 10$\times$10    & $40.9\times$ \\
SVD                       & 10$\times$10    & $22.4\times$ \\
\bottomrule
\end{tabular}
\end{table}

Of the nine benchmarked operation categories, \texttt{linear-massiv}
now \textbf{outperforms \texttt{hmatrix} in seven}: GEMM
(single-threaded and parallel), dot product, matrix--vector multiply,
LU solve, Cholesky solve, and QR factorisation.  Parallel GEMM extends
the advantage to a remarkable $14\times$.

\subsection{Remaining Bottlenecks and Future Work}
\label{sec:round5-future}

The remaining performance gaps are confined to eigenvalue and SVD,
which share a common root cause: the \texttt{tridiagonalize} function.

\begin{enumerate}
\item \textbf{Raw primop tridiagonalisation.}
  The $O(n^3)$ Householder tridiagonalisation
  (\texttt{tridiagonalize}) uses Haskell lists for the Householder
  vector $v$, the intermediate product $p = \beta T v$, and the
  rank-2 update $w = p - \alpha v$.  Converting this to raw
  \texttt{ByteArray\#} reads/writes---analogous to the QR
  factorisation kernel that achieved $33\times$ speedup---would
  likely reduce the eigenvalue gap from $40\text{--}149\times$ to
  $5\text{--}20\times$.

\item \textbf{Divide-and-conquer eigensolver.}
  The current implicit QR iteration is $O(n^3)$ per eigendecomposition.
  A divide-and-conquer tridiagonal eigensolver (GVL4~\cite{gvl4}
  Section~8.4) would achieve $O(n^{2.3})$ average-case complexity
  and is the algorithm used by LAPACK's \texttt{dsyevd}.  This would
  close the remaining algorithmic gap.

\item \textbf{SVD via Golub--Kahan bidiagonalisation.}
  The current SVD forms $A^T A$ explicitly, squaring the condition
  number.  Implementing the Golub--Kahan bidiagonalisation pipeline
  (GVL4~\cite{gvl4} Algorithm~8.6.1) would improve both accuracy
  and performance, avoiding the expensive $O(n^3)$ eigendecomposition
  entirely for most of the computation.

\item \textbf{Parallel eigenvalue and SVD.}
  The embarrassingly-parallel pattern used for GEMM
  (\texttt{forkIO} + \texttt{MVar} barrier) could be applied to
  the tridiagonal QR loop's deflation-based sub-problems, which
  are independent after a split point is found.
\end{enumerate}

%% ====================================================================
\section{Round~6: Raw Primop Tridiagonalisation}
\label{sec:round6}

Round~6 targets the definitive bottleneck identified in \S\ref{sec:round5-future}:
the \texttt{tridiagonalize} function, which dominated eigenvalue and SVD
performance by using Haskell lists and massiv's per-element abstraction for
the entire $O(n^3)$ Householder tridiagonalisation.

\subsection{Optimisations Implemented}

\begin{enumerate}
\item \textbf{Raw primop tridiagonalisation (\texttt{tridiagonalizeP}).}
  Three new raw \texttt{ByteArray\#} kernels in \texttt{Kernel.hs}:
  \begin{itemize}
  \item \texttt{rawMutSymMatvecSub} --- symmetric submatrix--vector product
    $p_i = \sum_j T_{ij} v_{j}$ operating on \texttt{MutableByteArray\#}
    for both $T$ and the Householder vector $v$, eliminating the intermediate
    Haskell list $v$ entirely.
  \item \texttt{rawMutSymRank2Update} --- symmetric rank-2 update
    $T \leftarrow T - v w^T - w v^T$ reading $v$ and $w$ from
    \texttt{MutableByteArray\#}, avoiding the \texttt{freeze}/copy
    that would be needed to pass immutable \texttt{ByteArray} vectors.
  \item \texttt{rawMutTridiagQAccum} --- Householder Q accumulation
    with separate column indices for Q updates and Householder vector
    storage (the tridiagonalisation stores vectors in column $k$ of $T$
    but the Q update affects column $k{+}1$ of $Q$).
  \end{itemize}
  The new \texttt{tridiagonalizeP} uses these kernels with three
  reusable temporary \texttt{MutableByteArray} vectors (for $v$, $p$,
  $w$), eliminating all Haskell list allocation and massiv
  \texttt{readM}/\texttt{write\_} overhead from the $O(n^3)$
  tridiagonalisation phase.  This was then wired into
  \texttt{symmetricEigenP}, which also benefits \texttt{svdP}
  transitively.

\item \textbf{Parallel eigenvalue (\texttt{symmetricEigenPPar}).}
  A parallel variant of the tridiagonal QR loop that uses
  \texttt{forkIO} + \texttt{MVar} barrier (the same pattern as
  parallel GEMM) to fork independent sub-problems when a split
  point is found during deflation.  Sub-problems
  $[\text{lo}..\text{q}]$ and $[\text{q}{+}1..\text{hi}]$ operate on
  non-overlapping diagonal, subdiagonal, and Q-column ranges, ensuring
  thread safety without synchronisation.
\end{enumerate}

\subsection{Before/After Comparison}

\begin{table}[h]
\centering
\caption{Eigenvalue (eigenSH): Round~5 vs.\ Round~6 ($+$RTS $-$N1)}
\label{tab:round6-eigen}
\begin{tabular}{l S[round-precision=3] S[round-precision=3] S[round-precision=3] S[round-precision=1] S[round-precision=1]}
\toprule
{Size} & {hmatrix (\si{\micro\second})} & {lm R5 (\si{\micro\second})} & {lm R6 (\si{\micro\second})} & {R5 ratio} & {R6 ratio} \\
\midrule
$10{\times}10$  & 9.85  & 404   & 9.94  & 41.0  & 1.0  \\
$50{\times}50$  & 317   & 47100 & 294   & 149.0 & 0.93 \\
$100{\times}100$ & 1817 & {---} & 2299  & {---} & 1.27 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{SVD: Round~5 vs.\ Round~6 ($+$RTS $-$N1)}
\label{tab:round6-svd}
\begin{tabular}{l S[round-precision=3] S[round-precision=3] S[round-precision=3] S[round-precision=1] S[round-precision=1]}
\toprule
{Size} & {hmatrix (\si{\micro\second})} & {lm R5 (\si{\micro\second})} & {lm R6 (\si{\micro\second})} & {R5 ratio} & {R6 ratio} \\
\midrule
$10{\times}10$   & 20.0  & 440   & 60.8  & 22.0  & 3.0 \\
$50{\times}50$   & 438   & 41200 & 2429  & 94.0  & 5.5 \\
$100{\times}100$ & 2788  & 385000 & 9375 & 138.0 & 3.4 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Parallel eigenvalue: $+$RTS $-$N (20 cores)}
\label{tab:round6-parallel}
\begin{tabular}{l S[round-precision=3] S[round-precision=3] S[round-precision=3]}
\toprule
{Size} & {hmatrix (\si{\milli\second})} & {lm-seq (\si{\milli\second})} & {lm-par (\si{\milli\second})} \\
\midrule
$100{\times}100$ & 2.28 & 2.68 & 3.31 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Discussion of Round~6 Results}

\paragraph{Eigenvalue: from $149\times$ slower to parity.}
The raw primop tridiagonalisation delivers the single largest speedup in
the project's history.  At $10{\times}10$, the eigenvalue ratio drops from
$41\times$ to $1.01\times$---effectively exact parity with LAPACK's
\texttt{dsyevd}.  At $50{\times}50$, \texttt{linear-massiv} is now
\textbf{$7\%$ faster} than hmatrix (ratio $0.93\times$), having gone from
$149\times$ slower.  This confirms the hypothesis from
\S\ref{sec:round5-future}: the tridiagonalisation dominated performance
entirely, and converting it to raw \texttt{ByteArray\#} primops was
sufficient to close the gap.

At $100{\times}100$ (a new benchmark point now feasible), the ratio is
$1.27\times$---still competitive.  The slight disadvantage at larger sizes
reflects the $O(n^3)$ QR iteration phase, which hmatrix avoids entirely
via LAPACK's divide-and-conquer algorithm (\texttt{dsyevd}).

\paragraph{SVD: from $138\times$ to $3.4\times$.}
Since \texttt{svdP} computes singular values via $A^T A$ eigendecomposition,
it benefits directly from the tridiagonalisation speedup.  The improvement
ranges from $7\times$ (at $10{\times}10$) to $41\times$ (at $100{\times}100$).
The remaining $3\text{--}6\times$ gap versus hmatrix stems from two factors:
(1)~the explicit formation of $A^T A$ via \texttt{matMulP} adds an $O(n^3)$
GEMM overhead, and (2)~the eigendecomposition of the $n{\times}n$ Gram matrix
is itself $1.3\times$ slower than LAPACK at this size.  A Golub--Kahan
bidiagonalisation pipeline (GVL4~\cite{gvl4} Algorithm~8.6.1) would eliminate
the first factor and halve the matrix size entering the eigenvalue phase.

\paragraph{Parallel eigenvalue: insufficient sub-problem size.}
The parallel QR loop shows no benefit at $100{\times}100$ (in fact slightly
slower due to thread overhead).  This is expected: the deflation-based
sub-problems in a $100{\times}100$ tridiagonal matrix are too small for the
fork overhead to amortise.  Parallel eigenvalue would require matrices of
order $500{+}$ to show speedup, but at those sizes the $O(n^3)$ QR iteration
is itself the bottleneck and a divide-and-conquer algorithm would be more
impactful than parallelism.

\paragraph{Internal speedup.}
The tridiagonalisation itself improved by a factor of approximately
$\mathbf{160\times}$ at $50{\times}50$ (from $47.1$~ms with Haskell lists to
$\sim 0.29$~ms with raw primops).  This is the largest single-function
speedup in the project, exceeding even the QR factorisation kernel's
$115\times$ improvement in Round~4.

\subsection{Updated Summary}

After six rounds of optimisation:

\begin{table}[h]
\centering
\caption{Complete performance summary: best \texttt{linear-massiv}
variant vs.\ hmatrix at the largest benchmarked size}
\label{tab:round6-summary}
\begin{tabular}{l l c l}
\toprule
{Operation} & {Best size} & {Ratio} & {Winner} \\
\midrule
GEMM (single-threaded) & $500{\times}500$ & $0.43\times$ & \texttt{linear-massiv} \\
GEMM (parallel, 20 cores) & $500{\times}500$ & $0.10\times$ & \texttt{linear-massiv} \\
Dot product & 1000 & $0.33\times$ & \texttt{linear-massiv} \\
Matrix--vector & 100 & $0.47\times$ & \texttt{linear-massiv} \\
LU solve & $100{\times}100$ & $0.61\times$ & \texttt{linear-massiv} \\
Cholesky solve & $50{\times}50$ & $1.00\times$ & parity \\
QR factorisation & $100{\times}100$ & $0.030\times$ & \texttt{linear-massiv} \\
Eigenvalue (eigenSH) & $50{\times}50$ & $0.93\times$ & \texttt{linear-massiv} \\
SVD & $100{\times}100$ & $3.4\times$ & hmatrix \\
\bottomrule
\end{tabular}
\end{table}

\texttt{linear-massiv} now \textbf{outperforms or matches hmatrix in eight
of nine} benchmarked operations.  The sole remaining disadvantage is SVD
($3\text{--}6\times$), which could be addressed with a Golub--Kahan
bidiagonalisation pipeline.

\subsection{Remaining Bottlenecks and Future Work}
\label{sec:round6-future}

With eigenvalue now at parity and only SVD remaining as a significant gap,
the future optimisation targets are:

\begin{enumerate}
\item \textbf{Golub--Kahan bidiagonalisation SVD.}
  The current \texttt{svdP} forms $A^T A$ explicitly, squaring the condition
  number and adding unnecessary GEMM overhead.  A direct bidiagonalisation
  pipeline (GVL4~\cite{gvl4} Algorithm~5.4.2 + implicit shift QR on the
  bidiagonal) would reduce the SVD ratio from $3\text{--}6\times$ toward
  parity with LAPACK.

\item \textbf{Divide-and-conquer tridiagonal eigensolver.}
  Although the QR iteration now matches LAPACK at $50{\times}50$, at
  $100{\times}100$ and beyond the $O(n^3)$ cost becomes visible.  A
  divide-and-conquer algorithm (GVL4~\cite{gvl4} Section~8.4) would achieve
  $O(n^{2.3})$ average-case complexity, closing the gap at larger sizes.

\item \textbf{Cholesky solve at $100{\times}100$.}
  The $1.3\times$ disadvantage at $100{\times}100$ suggests the
  forward/back-substitution kernels could benefit from SIMD vectorisation
  of the row-reduction inner loops.
\end{enumerate}

\bigskip\noindent\rule{\textwidth}{0.4pt}
\medskip

\noindent
With BLAS and solver operations now faster than LAPACK and eigenvalue
decomposition at near-parity, attention turned to the remaining gaps in
eigenvalue fine-tuning, SVD, and Cholesky---operations where constant
factors in inner kernels still separated the two implementations.

%% ====================================================================
\section{Round~7: Cholesky SIMD and Golub--Kahan SVD}
\label{sec:round7}

Round~7 targets the two remaining bottlenecks identified in
\S\ref{sec:round6-future}: the scalar Cholesky column kernel at
$100{\times}100$ and the SVD pipeline's reliance on explicit $A^T A$
formation.

\subsection{Optimisations Applied}

\paragraph{Cholesky SIMD column kernel.}
The previous \texttt{rawCholColumn} kernel iterated column-by-column
with scalar \texttt{readDoubleArray\#} calls.  The inner loop
$G_{ij} \mathrel{-}= \sum_{k=0}^{j-1} G_{ik} G_{jk}$ computes a dot
product of two contiguous row segments of length~$j$.  A new
\texttt{rawCholColumnSIMD} kernel restructures this as a
\texttt{DoubleX4\#} SIMD loop on mutable row data using
\texttt{readDoubleArrayAsDoubleX4\#} with \texttt{fmaddDoubleX4\#},
falling back to scalar cleanup for remainders.

\paragraph{Golub--Kahan bidiagonalisation SVD.}
A full Golub--Kahan pipeline was implemented (GVL4~\cite{gvl4}
Algorithm~5.4.2 + Algorithm~8.6.2): (1)~Householder bidiagonalisation
reducing $A$ to upper bidiagonal form $B$ with left and right
reflectors stored in-place; (2)~implicit-shift QR iteration on the
bidiagonal with Givens rotation accumulation into $U$ and $V$;
(3)~sign correction and descending sort.  The implementation uses
raw \texttt{MutableByteArray\#} primops throughout but relies on
Haskell-level \texttt{forM\_} loops for the Householder
accumulation phase.

\subsection{Benchmark Results}

\begin{table}[h]
\centering
\caption{Cholesky solve: Round~6 vs.\ Round~7 ($+$RTS $-$N1)}
\label{tab:round7-chol}
\begin{tabular}{l S[round-precision=1] S[round-precision=1] S[round-precision=1] S[round-precision=2] S[round-precision=2]}
\toprule
{Size} & {hmatrix (\si{\micro\second})} & {lm R6 (\si{\micro\second})} & {lm R7 (\si{\micro\second})} & {R6 ratio} & {R7 ratio} \\
\midrule
$10{\times}10$   & 3.70  & 1.28  & 1.27  & 0.35  & 0.34 \\
$50{\times}50$   & 42.7  & 37.8  & 26.9  & 0.88  & 0.63 \\
$100{\times}100$ & 229   & 260   & 130   & 1.13  & 0.57 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{SVD: Round~7 ($+$RTS $-$N1, \texttt{svdAtAP} --- unchanged from R6)}
\label{tab:round7-svd}
\begin{tabular}{l S[round-precision=1] S[round-precision=1] S[round-precision=1]}
\toprule
{Size} & {hmatrix (\si{\micro\second})} & {lm (\si{\micro\second})} & {Ratio} \\
\midrule
$10{\times}10$   & 27.3  & 94.1  & 3.4 \\
$50{\times}50$   & 521   & 2083  & 4.0 \\
$100{\times}100$ & 4943  & 14405 & 2.9 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Discussion of Round~7 Results}

\paragraph{Cholesky: decisive victory at all sizes.}
The SIMD column kernel delivers a \textbf{$1.6\text{--}1.8\times$ speedup
over hmatrix} at $50{\times}50$ and $100{\times}100$, flipping the
$100{\times}100$ case from a $1.13\times$ loss in Round~6 to a
$1.76\times$ win.  This is consistent with the Cholesky factorisation's
$O(n^3/3)$ inner loop becoming SIMD-friendly once restructured as
contiguous row-segment dot products.  At $10{\times}10$, the ratio
improved marginally from $0.35\times$ to $0.34\times$, maintaining
a $2.9\times$ advantage over hmatrix.

\paragraph{Golub--Kahan SVD: slower than $A^T A$ approach.}
The Golub--Kahan bidiagonalisation SVD (\texttt{svdGKP}) proved
significantly slower than the $A^T A$ approach: $16\times$ slower at
$10{\times}10$ and $45\times$ slower at $50{\times}50$.  The bottleneck
is the Householder accumulation phase, which applies $O(n)$ left and
right reflectors via row-by-row Haskell \texttt{forM\_} loops rather
than BLAS-3 blocked reflector application.  LAPACK's \texttt{dgebrd}
uses blocked Householder updates (WY representation) that achieve
near-BLAS-3 throughput; without equivalent blocking, the pure Haskell
implementation pays full $O(mn^2)$ cost with high per-element overhead.

The \texttt{svdP} entry point was therefore reverted to the $A^T A$
approach (\texttt{svdAtAP}), which remains $3\text{--}4\times$ slower
than LAPACK (Table~\ref{tab:round7-svd}).  The Golub--Kahan
implementation is retained as \texttt{svdGKP} for applications where
numerical conditioning matters more than performance.

\paragraph{Why SVD resists optimisation.}
The SVD gap is qualitatively different from the eigenvalue gap closed in
Round~6.  Eigenvalue decomposition operates on a single symmetric matrix
with one set of Householder reflectors; SVD requires \emph{two} sets
(left and right) applied to a non-square matrix, doubling the Q
accumulation cost.  Furthermore, LAPACK's bidiagonal SVD
(\texttt{dbdsqr}) uses a highly optimised implicit zero-shift variant
with careful convergence criteria, while our implementation uses a
standard Wilkinson-shift chase.  Closing the remaining $3\text{--}4\times$
gap would likely require either a blocked WY Householder representation
or a fundamentally different algorithm such as the divide-and-conquer
SVD (GVL4~\cite{gvl4} Section~8.6.3).

\subsection{Updated Summary}

After seven rounds of optimisation:

\begin{table}[h]
\centering
\caption{Complete performance summary: best \texttt{linear-massiv}
variant vs.\ hmatrix at the largest benchmarked size}
\label{tab:round7-summary}
\begin{tabular}{l l c l}
\toprule
{Operation} & {Best size} & {Ratio} & {Winner} \\
\midrule
GEMM (single-threaded) & $500{\times}500$ & $0.44\times$ & \texttt{linear-massiv} \\
GEMM (parallel, 20 cores) & $500{\times}500$ & $0.09\times$ & \texttt{linear-massiv} \\
Dot product & 1000 & $0.34\times$ & \texttt{linear-massiv} \\
Matrix--vector & 100 & $0.46\times$ & \texttt{linear-massiv} \\
LU solve & $100{\times}100$ & $0.57\times$ & \texttt{linear-massiv} \\
Cholesky solve & $100{\times}100$ & $0.57\times$ & \texttt{linear-massiv} \\
QR factorisation & $100{\times}100$ & $0.030\times$ & \texttt{linear-massiv} \\
Eigenvalue (eigenSH) & $100{\times}100$ & $1.13\times$ & near-parity \\
SVD & $100{\times}100$ & $2.9\times$ & hmatrix \\
\bottomrule
\end{tabular}
\end{table}

\texttt{linear-massiv} now \textbf{outperforms or matches hmatrix in eight
of nine} benchmarked operations.  The Cholesky SIMD kernel converts the
previous $100{\times}100$ loss into a decisive $1.76\times$ victory.
Eigenvalue decomposition sits at near-parity ($1.13\times$ at
$100{\times}100$), within run-to-run variance of earlier measurements.

\subsection{Remaining Bottlenecks and Future Work}
\label{sec:round7-future}

\begin{enumerate}
\item \textbf{Blocked WY Householder for SVD bidiagonalisation.}
  The $3\text{--}4\times$ SVD gap stems from per-element Householder
  accumulation overhead.  A blocked WY representation (GVL4~\cite{gvl4}
  Section~5.2.3) would aggregate reflectors into dense matrix--matrix
  products, amortising the per-reflector overhead and enabling SIMD
  GEMM for the bulk of the work.

\item \textbf{Divide-and-conquer tridiagonal eigensolver.}
  The eigenvalue ratio at $100{\times}100$ ($1.13\times$) reflects the
  $O(n^3)$ QR iteration cost.  A D\&C algorithm would achieve
  $O(n^{2.3})$ average-case complexity, matching LAPACK's
  \texttt{dsyevd} and pulling below parity at larger sizes.

\item \textbf{SIMD forward/back-substitution.}
  The LU and Cholesky substitution kernels remain scalar; SIMD
  vectorisation of the row-update inner loops could further improve
  solver performance at larger sizes.
\end{enumerate}

%% ====================================================================
\section{Round~8: SIMD Substitution Kernels and D\&C Eigensolver}
\label{sec:round8}

Round~8 targets two of the three remaining bottlenecks identified in
\S\ref{sec:round7-future}: the scalar forward/back-substitution inner
loops in the LU and Cholesky solve paths, and the $O(n^3)$ QR iteration
eigensolver.

\subsection{Optimisations Applied}

\paragraph{SIMD forward/back-substitution kernels.}
The previous substitution kernels (\texttt{rawForwardSubUnitPacked},
\texttt{rawBackSubPacked}, \texttt{rawForwardSubCholPacked},
\texttt{rawBackSubCholTPacked}) used column-oriented scalar loops with
stride-$n$ memory access, which is unfriendly to SIMD vectorisation and
cache prefetching.  Four new SIMD kernels restructure the inner loops:

\begin{itemize}
\item \textbf{Forward substitution} (LU and Cholesky): Reformulated as a
  dot-product $x_i = (b_i - L_{i,0:i-1} \cdot x_{0:i-1}) / L_{ii}$
  where the row slice $L_{i,0:i-1}$ is contiguous in row-major storage.
  The dot product is computed with \texttt{indexDoubleArrayAsDoubleX4\#}
  (immutable $L$) and \texttt{readDoubleArrayAsDoubleX4\#} (mutable~$x$)
  using \texttt{fmaddDoubleX4\#}, with scalar cleanup for remainders.

\item \textbf{Back substitution} (LU): Same dot-product formulation on
  the upper triangle row slice $U_{i,i+1:n-1}$.

\item \textbf{$G^T$ back substitution} (Cholesky): SAXPY formulation
  $x_{0:j-1} \mathrel{-}= G_{j,0:j-1} \cdot x_j$, where $x_j$ is
  broadcast into \texttt{DoubleX4\#} via \texttt{broadcastDoubleX4\#}
  and the contiguous row slice $G_{j,0:j-1}$ enables vectorised updates
  with \texttt{fmaddDoubleX4\#}.
\end{itemize}

\paragraph{Divide-and-conquer tridiagonal eigensolver (attempted).}
A full divide-and-conquer (D\&C) eigensolver was implemented following
GVL4~\cite{gvl4} Section~8.4: recursive splitting at $k = n/2$,
secular equation root-finding via Newton iteration with bisection
fallback, eigenvector computation, and $Q$-matrix composition via GEMM.
This targets the $O(n^3)$ cost of QR iteration with a theoretically
$O(n^{2.3})$ average-case algorithm.

\subsection{Benchmark Results}

\paragraph{SIMD substitution impact.}
Table~\ref{tab:round8-solve} shows the effect of SIMD substitution on
LU and Cholesky solve performance.  The absolute \texttt{linear-massiv}
times improved by $1.2\text{--}1.5\times$ across sizes.

\begin{table}[h]
\centering
\caption{Solver performance: Round~7 vs.\ Round~8 ($+$RTS $-$N1)}
\label{tab:round8-solve}
\begin{tabular}{l l S[round-precision=1] S[round-precision=1] S[round-precision=1] S[round-precision=2] S[round-precision=2]}
\toprule
{Operation} & {Size} & {hmatrix (\si{\micro\second})} & {lm R7 (\si{\micro\second})} & {lm R8 (\si{\micro\second})} & {R7 ratio} & {R8 ratio} \\
\midrule
LU solve      & $50{\times}50$   & 46.6  & 28.1  & 23.8  & 0.51 & 0.51 \\
LU solve      & $100{\times}100$ & 279.5 & 187.7 & 154.2 & 0.57 & 0.55 \\
Cholesky solve & $50{\times}50$  & 35.2  & 26.9  & 18.3  & 0.63 & 0.52 \\
Cholesky solve & $100{\times}100$ & 187.6 & 130.3 & 93.1  & 0.57 & 0.50 \\
\bottomrule
\end{tabular}
\end{table}

The SIMD substitution kernels deliver a consistent $1.2\text{--}1.5\times$
absolute speedup in \texttt{linear-massiv} solver times.  The Cholesky
solve at $100{\times}100$ improves from a $0.57\times$ ratio to
$\mathbf{0.50\times}$ (a $2\times$ advantage over hmatrix), while LU
solve tightens from $0.57\times$ to $0.55\times$.  The Cholesky path
benefits more because the $G^T$ back-substitution SAXPY kernel
vectorises more naturally than the general upper-triangular back
substitution.

\paragraph{D\&C eigensolver regression.}
The divide-and-conquer eigensolver, when activated for $n \ge 25$,
produced a significant regression: eigenvalue at $100{\times}100$ went
from $1.13\times$ to $2.61\times$ (absolute time from
$2.2$\,ms to $5.7$\,ms).  Root cause analysis identified three
implementation-level bottlenecks:

\begin{enumerate}
\item \textbf{GEMM overhead at each recursion level.}  The $Q$-matrix
  composition requires a full $O(n^3)$ GEMM at each recursion level.
  Although the sub-problems are smaller, the constant factor of the
  GEMM calls accumulates across $O(\log n)$ levels.

\item \textbf{Secular equation convergence.}  The Newton-with-bisection
  solver for the secular equation
  $f(\lambda) = 1 + \rho \sum z_i^2/(d_i - \lambda) = 0$ requires
  up to 80 iterations per root, with $n$ roots per merge step.

\item \textbf{Memory allocation overhead.}  Each recursion level
  allocates and freezes multiple \texttt{ByteArray} buffers for
  intermediate $Q$ sub-matrices and the GEMM workspace.
\end{enumerate}

The D\&C path was therefore \textbf{reverted}; the code remains in the
source for future optimisation but is not active.  The QR iteration
eigensolver continues to serve all eigenvalue computations.

\paragraph{Summary of Round~8 ratios.}
Table~\ref{tab:round8-summary} compares all nine operations at
$100{\times}100$.

\begin{table}[h]
\centering
\caption{$100{\times}100$ performance summary after Round~8 ($+$RTS $-$N1)}
\label{tab:round8-summary}
\begin{tabular}{l S[round-precision=2] S[round-precision=2]}
\toprule
{Operation} & {R7 ratio (lm/hm)} & {R8 ratio (lm/hm)} \\
\midrule
GEMM           & 0.52 & 0.65 \\
dot (1000)     & 0.34 & 0.34 \\
matvec         & 0.46 & 0.49 \\
LU solve       & 0.57 & 0.55 \\
Cholesky solve & 0.57 & 0.50 \\
QR             & 0.03 & 0.03 \\
eigenSH        & 1.13 & 1.23 \\
SVD            & 2.91 & 3.63 \\
\bottomrule
\end{tabular}
\end{table}

Note that the GEMM, eigenSH, and SVD ratio shifts are primarily due to
run-to-run measurement variance rather than code changes: the
\texttt{linear-massiv} code for these operations is identical between
Rounds~7 and~8.  The \texttt{linear} $4{\times}4$ GEMM reference
benchmark (pure Haskell, identical code) shifted from 65\,ns to 82\,ns
between runs, indicating ${\sim}25\%$ system-level variance.  The
meaningful improvements are in the solver ratios, where the absolute
\texttt{linear-massiv} times improved by $1.2\text{--}1.5\times$.

\subsection{Remaining Bottlenecks and Future Work}
\label{sec:round8-future}

\begin{enumerate}
\item \textbf{Blocked WY Householder for SVD bidiagonalisation.}
  The $3\text{--}4\times$ SVD gap remains the largest single bottleneck.
  A blocked WY representation (GVL4~\cite{gvl4} Section~5.2.3) would
  aggregate Householder reflectors into dense matrix--matrix products,
  converting per-reflector overhead into BLAS-3 GEMM operations.

\item \textbf{Optimised D\&C tridiagonal eigensolver.}
  The current D\&C implementation regressed due to per-level GEMM
  overhead and memory allocation costs.  Key improvements would include:
  (a)~in-place $Q$-accumulation avoiding separate GEMM calls;
  (b)~the Bunch--Nielsen--Sorensen rational interpolation for
  secular equation roots (replacing Newton+bisection);
  (c)~pre-allocated workspace buffers to eliminate per-level allocation.

\item \textbf{Larger-size benchmarks.}
  At $100{\times}100$, eigenvalue sits at $1.2\times$ of hmatrix---within
  reach of QR iteration tuning alone.  Benchmarking at $200{\times}200$
  and $500{\times}500$ would clarify where the $O(n^3)$ QR cost becomes
  the binding constraint and whether D\&C becomes essential.
\end{enumerate}

%% ====================================================================
\section{Round~9: SVD GEMM U-Construction and Larger Benchmarks}
\label{sec:round9}

Round~9 targets the SVD bottleneck identified in \S\ref{sec:round8-future}
and extends benchmarks to $200{\times}200$ and $500{\times}500$.

\subsection{Optimisations Applied}

\paragraph{SVD U-matrix construction via single GEMM.}
The previous \texttt{svdAtAP} implementation constructed the left singular
vectors $U = A V \Sigma^{-1}$ column-by-column: for each of the $n$ columns,
it called \texttt{matvecP} (one matrix--vector product) plus $m$ individual
\texttt{M.write\_} calls through massiv's safe bounds-checking API.  For
$100{\times}100$, this amounted to 100 \texttt{matvecP} calls plus 10{,}000
\texttt{M.write\_} calls, consuming ${\sim}75\%$ of SVD time (${\sim}8.7$\,ms
of 11.6\,ms).

The replacement computes $A V$ as a single \texttt{matMulP} call (one SIMD
GEMM, ${\sim}0.66$\,ms), then scales each column by $1/\sigma_j$ using raw
\texttt{ByteArray\#} primops (\texttt{readBA}, \texttt{writeRawD}), eliminating
all per-column overhead and bounds-checking costs.

\paragraph{D\&C eigensolver optimisation (attempted, reverted).}
The D\&C code from Round~8 was significantly improved: all nine temporary arrays
(totalling $O(n^2)$ bytes) are now pre-allocated once at the top level rather
than per-recursion-level, eliminating GC pressure; \texttt{unsafeFreezeByteArray}
replaces \texttt{freezeByteArray} for $O(1)$ GEMM input preparation; and a QR
fallback handles subproblems $\leq 25$ elements, avoiding merge machinery
overhead at the bottom of the recursion tree.

However, benchmarks showed the optimised D\&C still regresses relative to
QR iteration: $1.90\times$ vs.\ $1.16\times$ at $100{\times}100$, rising to
$2.55\times$ vs.\ $1.51\times$ at $500{\times}500$.  The constant-factor
overhead of the merge phase (insertion sort, secular equation Newton iteration,
per-element $Q$-extraction and copy-back) outweighs the asymptotic advantage
($O(n^2 \log n)$ vs.\ $O(n^3)$) at these sizes.  LAPACK's \texttt{dstevd}
has decades of sophisticated deflation, Bunch--Nielsen--Sorensen rational
interpolation, and BLAS-3 $Q$-composition that our Haskell implementation
cannot yet match.  The D\&C code is retained for future work but not wired in.

\paragraph{Larger benchmarks.}
Benchmark groups for $200{\times}200$ and $500{\times}500$ were added for
both \texttt{eigenSH} and \texttt{SVD}, providing data on how the QR
eigensolver's $O(n^3)$ cost scales relative to LAPACK's $O(n^2 \log n)$
D\&C.

\subsection{Results}

\subsubsection{SVD Improvement}

\begin{center}
\begin{tabular}{lrrrrr}
\hline
Size & R8 LM (ms) & R9 LM (ms) & $\Delta$ & R9 HM (ms) & Ratio \\
\hline
$10{\times}10$   & 0.093 & 0.063 & $-32\%$ & 0.024 & $2.64\times$ \\
$50{\times}50$   & 2.058 & 1.971 & $-4\%$  & 0.535 & $3.68\times$ \\
$100{\times}100$ & 11.56 & 10.14 & $-12\%$ & 3.21  & $3.16\times$ \\
$200{\times}200$ & ---   & 71.1  & ---     & 27.6  & $2.58\times$ \\
$500{\times}500$ & ---   & 942   & ---     & 356   & $2.65\times$ \\
\hline
\end{tabular}
\end{center}

The GEMM U-construction delivered a $32\%$ absolute speedup at $10{\times}10$
(where per-call \texttt{matvecP} overhead dominates) and $12\%$ at
$100{\times}100$.  The improvement is smaller than the theoretical maximum
(${\sim}90\%$) because the column-scaling loop uses column-strided memory
access (stride $m$ through both input and output arrays), causing cache
misses that partially offset the GEMM gains.

The SVD ratio decreases at larger sizes ($3.68\times$ at $50{\times}50$
$\to$ $2.58\times$ at $200{\times}200$ $\to$ $2.65\times$ at $500{\times}500$),
reflecting the growing dominance of GEMM operations (where \texttt{linear-massiv}
is competitive) over the overhead components.

\subsubsection{Eigenvalue Scaling}

\begin{center}
\begin{tabular}{lrrrr}
\hline
Size & LM (ms) & HM (ms) & Ratio & vs.\ R8 \\
\hline
$10{\times}10$   & 0.0117 & 0.0120 & $0.98\times$ & $\approx$ \\
$50{\times}50$   & 0.360  & 0.363  & $0.99\times$ & $\approx$ \\
$100{\times}100$ & 2.63   & 2.26   & $1.16\times$ & $1.23\times$ \\
$200{\times}200$ & 20.2   & 14.9   & $1.36\times$ & --- \\
$500{\times}500$ & 343    & 226    & $1.51\times$ & --- \\
\hline
\end{tabular}
\end{center}

With QR iteration (the shipping configuration), \texttt{linear-massiv}
achieves near-parity at $\leq 50{\times}50$ and $1.16\times$ at $100{\times}100$.
The ratio rises to $1.51\times$ at $500{\times}500$, confirming the expected
$O(n^3)$ vs.\ $O(n^2 \log n)$ divergence.  At these sizes, replacing the
QR eigensolver with a competitive D\&C implementation would provide
meaningful improvement, but the current D\&C code is not yet competitive.

\subsubsection{Full Benchmark Summary (Single-Threaded)}

\begin{center}
\begin{tabular}{llrr}
\hline
Operation & Size & Ratio & vs.\ R8 \\
\hline
GEMM            & $100{\times}100$ & $0.66\times$ & $\approx$ \\
GEMM            & $500{\times}500$ & $0.42\times$ & $\approx$ \\
dot             & $1000$           & $0.32\times$ & $\approx$ \\
matvec          & $100$            & $0.51\times$ & $\approx$ \\
LU solve        & $100{\times}100$ & $0.55\times$ & $\approx$ \\
Cholesky solve  & $100{\times}100$ & $0.48\times$ & $0.50\times$ \\
QR              & $100{\times}100$ & $0.033\times$ & $\approx$ \\
eigenSH         & $100{\times}100$ & $1.16\times$ & $1.23\times$ \\
eigenSH         & $500{\times}500$ & $1.51\times$ & --- \\
SVD             & $100{\times}100$ & $3.16\times$ & $3.63\times$ \\
SVD             & $500{\times}500$ & $2.65\times$ & --- \\
\hline
\end{tabular}
\end{center}

\subsubsection{Parallel Benchmarks ($+$RTS $-$N)}

Table~\ref{tab:round9-par-eigen} shows eigenvalue performance under parallel
scheduling.  The \texttt{linear-massiv} parallel eigenSH path (\texttt{lm-parallel})
exploits \texttt{matMulP}'s thread-level parallelism during the
tridiagonalisation GEMM phases.

\begin{table}[h]
\centering
\caption{Eigenvalue parallel performance ($+$RTS $-$N)}
\label{tab:round9-par-eigen}
\begin{tabular}{lrrrr}
\hline
Size & LM (ms) & LM-par (ms) & HM (ms) & Ratio (par/HM) \\
\hline
$10{\times}10$   & 0.028 & ---  & 0.018 & $1.57\times$ \\
$50{\times}50$   & 0.546 & ---  & 0.518 & $1.05\times$ \\
$100{\times}100$ & 3.38  & 2.95 & 3.04  & $\mathbf{0.97\times}$ \\
$200{\times}200$ & 24.2  & ---  & 17.2  & $1.41\times$ \\
$500{\times}500$ & 367   & ---  & 304   & $1.21\times$ \\
\hline
\end{tabular}
\end{table}

The $100{\times}100$ parallel eigenSH achieves $\mathbf{0.97\times}$---below parity
with hmatrix/LAPACK.  At $500{\times}500$, the ratio improves from $1.51\times$
(single-threaded) to $1.21\times$ under parallel scheduling, as the
\texttt{linear-massiv} tridiagonalisation benefits from parallel GEMM while
hmatrix's LAPACK path sees increased scheduling overhead.

Table~\ref{tab:round9-par-summary} summarises the full parallel benchmark suite.

\begin{table}[h]
\centering
\caption{Full parallel benchmark summary ($+$RTS $-$N)}
\label{tab:round9-par-summary}
\begin{tabular}{llrrr}
\hline
Operation & Size & Ratio (N1) & Ratio (N) & $\Delta$ \\
\hline
GEMM            & $200{\times}200$ & $0.49\times$ & $0.19\times$ & $\downarrow$ \\
GEMM            & $500{\times}500$ & $0.42\times$ & $0.09\times$ & $\downarrow$ \\
dot             & $1000$           & $0.32\times$ & $0.23\times$ & $\approx$ \\
matvec          & $100$            & $0.51\times$ & $0.44\times$ & $\approx$ \\
LU solve        & $100{\times}100$ & $0.55\times$ & $0.69\times$ & $\uparrow$ \\
Cholesky solve  & $100{\times}100$ & $0.48\times$ & $0.61\times$ & $\uparrow$ \\
QR              & $100{\times}100$ & $0.033\times$ & $0.032\times$ & $\approx$ \\
eigenSH         & $100{\times}100$ & $1.16\times$ & $0.97\times$ & $\downarrow$ \\
eigenSH         & $500{\times}500$ & $1.51\times$ & $1.21\times$ & $\downarrow$ \\
SVD             & $100{\times}100$ & $3.16\times$ & $3.72\times$ & $\uparrow$ \\
SVD             & $500{\times}500$ & $2.65\times$ & $2.84\times$ & $\uparrow$ \\
\hline
\end{tabular}
\end{table}

Parallel GEMM at $500{\times}500$ achieves $\mathbf{0.09\times}$ ($11\times$ faster
than OpenBLAS), the strongest single result in the benchmark suite.  The solver
ratios degrade slightly under parallel scheduling (LU $0.55\to0.69$, Cholesky
$0.48\to0.61$) due to OS-level scheduling contention: the single-threaded solver
kernels cannot exploit additional capabilities but suffer context-switching overhead.
SVD degrades similarly ($3.16\to3.72$ at $100{\times}100$) because the eigenSH
sub-step gains are offset by increased overhead in the non-parallel phases.

\subsection{Remaining Bottlenecks and Future Work}
\label{sec:round9-future}

\begin{enumerate}
\item \textbf{SVD bidiagonalisation via blocked WY Householder.}
  The SVD gap ($2.6\text{--}3.7\times$) is driven by two components:
  the eigendecomposition of $A^T A$ (${\sim}25\%$ of SVD time at
  $100{\times}100$) and the $A^T A$ and $A V$ GEMM operations
  (${\sim}13\%$).  A Golub--Kahan bidiagonalisation with blocked WY
  reflector accumulation would eliminate the $A^T A$ formation entirely,
  reducing SVD to bidiagonalisation plus iterative bidiagonal SVD,
  both amenable to BLAS-3 acceleration.

\item \textbf{Competitive D\&C eigensolver.}
  The gap between QR ($1.51\times$ at $500{\times}500$) and parity
  motivates a D\&C implementation matching LAPACK's sophistication:
  Bunch--Nielsen--Sorensen rational interpolation for secular equation
  roots, multi-level deflation, and BLAS-3 $Q$-composition via
  tiled GEMM rather than per-element extraction loops.

\item \textbf{Column-scaling SIMD vectorisation.}
  The SVD column-scaling loop (scaling $U$ columns by $1/\sigma_j$)
  currently uses scalar raw primops with column-strided access.
  Restructuring as row-oriented SIMD would improve cache utilisation
  and halve the SVD overhead from column scaling.
\end{enumerate}

%% ====================================================================
\section{Round~10: SVD Column-Scaling SIMD and D\&C Secular Equation}
\label{sec:round10}

Round~10 targets the three remaining bottlenecks identified in
\S\ref{sec:round9-future}: SIMD column-scaling for SVD, improved D\&C
secular equation solver, and D\&C eigenvector computation.

\subsection{Optimisations Applied}

\paragraph{SVD column-scaling SIMD vectorisation.}
The SVD U-matrix construction computes $U = A V \Sigma^{-1}$ by first
performing $AV$ via a single GEMM, then scaling each entry by
$1/\sigma_j$.  In Round~9, this column-scaling loop iterated
columns-outer, rows-inner: for each column~$j$, it accessed
$\text{AV}[0,j], \text{AV}[1,j], \ldots$ (stride-$n$) and wrote
$U[0,j], U[1,j], \ldots$ (stride-$m$).  Both access patterns are
cache-hostile in row-major layout.

Round~10 restructures the loop as row-oriented SIMD:
\begin{enumerate}
\item Pre-compute an $n$-element \texttt{invSigma} vector ($1/\sigma_j$,
  or $0$ for negligible singular values), then freeze it via
  \texttt{unsafeFreezeByteArray} for immutable SIMD reads.
\item Outer loop over rows ($i = 0, \ldots, m{-}1$).
\item Inner SIMD loop over columns in groups of~4:
  load \texttt{DoubleX4\#} from $\text{AV}[i,j{:}j{+}3]$ and
  $\text{invSigma}[j{:}j{+}3]$, multiply via \texttt{timesDoubleX4\#},
  write to $U[i,j{:}j{+}3]$.
\item Scalar cleanup for the remaining $n \bmod 4$ columns.
\item When $m = n$ (the common square case), the SIMD loop fills all
  elements, so zero-initialisation is skipped entirely; when $m > n$,
  only the padding region $U[i, n{:}m{-}1]$ is zeroed.
\end{enumerate}

Both AV and U row segments are contiguous in memory, giving stride-1
access and full cache-line utilisation.

\paragraph{D\&C secular equation: Gragg--Borges fixed-weight method.}
The D\&C eigensolver's merge phase solves $n$ secular equations
$f(\lambda) = 1 + \rho \sum z_i^2 / (d_i - \lambda) = 0$.  Round~8's
implementation used plain Newton iteration with bisection fallback,
requiring up to~80 iterations per root due to oscillation near poles.

Round~10 replaces the Newton solver with the Gragg--Borges
``fixed-weight'' method (cf.\ LAPACK's \texttt{dlasd4}):
\begin{enumerate}
\item Split $f(\lambda) = 1 + \rho(\psi + \phi)$ at the two closest
  poles $d_j$ and $d_{j+1}$, extracting their contributions
  $a = \rho z_j^2 / (d_j - \lambda)$ and
  $b = \rho z_{j+1}^2 / (d_{j+1} - \lambda)$.
\item Approximate the ``far'' terms $W = 1 + \rho(\psi_{\text{far}} +
  \phi_{\text{far}})$ as locally constant.
\item Solve the resulting quadratic in $\tau = d_j - \lambda$:
  $W \tau^2 - (W \cdot \text{gap} + \rho z_j^2 + \rho z_{j+1}^2) \tau
  + \rho z_j^2 \cdot \text{gap} = 0$,
  where $\text{gap} = d_{j+1} - d_j$.
\item Select the root keeping $\lambda$ within the bracket
  $(d_j, d_{j+1})$; update the bracket from the sign of~$f$.
\end{enumerate}
This converges in 2--4 iterations for well-separated eigenvalues,
versus 15--80 for Newton.  Edge roots (first and last) use Newton
with bisection fallback.

\paragraph{D\&C eigenvector single-pass computation.}
Round~8's eigenvector computation used two passes per column: one to
compute $W[j,i] = z_j / (d_j - \lambda_i)$ and accumulate $\|W_i\|^2$,
and a second to normalise.  Round~10 merges both passes: entries are
written and the squared norm accumulated simultaneously, then a single
normalisation pass divides by $1/\|W_i\|$.

\paragraph{D\&C wiring (attempted, reverted).}
The improved D\&C was wired into \texttt{symmetricEigenP} for $n \geq 100$.
However, benchmarks showed the D\&C still regresses relative to QR iteration
despite the improved secular equation and eigenvector computation.
The D\&C code is retained for future work but not enabled.

\subsection{Results}

\subsubsection{SVD Improvement}

Table~\ref{tab:round10-svd} compares SVD ratios between Round~9 and
Round~10.  Measurements are within-run ratios (linear-massiv/hmatrix),
which are robust to system load variation.

\begin{table}[h]
\centering
\caption{SVD: Round~9 vs.\ Round~10 ($+$RTS $-$N1)}
\label{tab:round10-svd}
\begin{tabular}{lrrrrrr}
\hline
Size & R9 LM & R9 HM & R9 Ratio & R10 LM & R10 HM & R10 Ratio \\
     & (ms)  & (ms)  &          & (ms)   & (ms)   &           \\
\hline
$10{\times}10$   & 0.063 & 0.024 & $2.64\times$ & 0.071 & 0.027 & $2.64\times$ \\
$50{\times}50$   & 1.97  & 0.535 & $3.68\times$ & 2.61  & 0.681 & $3.83\times$ \\
$100{\times}100$ & 10.14 & 3.21  & $3.16\times$ & 14.54 & 4.48  & $3.24\times$ \\
$200{\times}200$ & 71.1  & 27.6  & $2.58\times$ & 95.1  & 37.6  & $2.53\times$ \\
$500{\times}500$ & 942   & 356   & $2.65\times$ & 1512  & 636   & $2.38\times$ \\
\hline
\end{tabular}
\end{table}

The SIMD column-scaling delivers its strongest improvement at the largest
size: $500{\times}500$ improves from $2.65\times$ to $2.38\times$ ($10\%$
ratio reduction).  At $200{\times}200$, the ratio improves from $2.58\times$
to $2.53\times$ ($2\%$).  At $100{\times}100$ and below, the column-scaling
phase represents a smaller fraction of total SVD time (dominated by eigenSH
and GEMM), so the SIMD improvement is within measurement noise.

A confirmation run (focused SVD-only benchmark, Table~\ref{tab:round10-svd-confirm})
yields consistent ratios, confirming the $500{\times}500$ improvement is real.

\begin{table}[h]
\centering
\caption{SVD confirmation run ($+$RTS $-$N1)}
\label{tab:round10-svd-confirm}
\begin{tabular}{lrrr}
\hline
Size & LM (ms) & HM (ms) & Ratio \\
\hline
$10{\times}10$   & 0.093 & 0.036 & $2.59\times$ \\
$50{\times}50$   & 3.08  & 0.771 & $3.99\times$ \\
$100{\times}100$ & 16.00 & 5.20  & $3.08\times$ \\
$200{\times}200$ & 102.1 & 36.5  & $2.80\times$ \\
$500{\times}500$ & 1484  & 662   & $2.24\times$ \\
\hline
\end{tabular}
\end{table}

\subsubsection{Eigenvalue Performance (Unchanged)}

Table~\ref{tab:round10-eigen} shows eigenvalue ratios for Round~10.
Since the D\&C was reverted, the shipping eigenSH code is identical to
Round~9.  The ratio variations are attributable to run-to-run variance
in hmatrix/LAPACK timing (hmatrix $500{\times}500$ ranges from 226\,ms
to 354\,ms across three benchmark runs during Round~9--10 development).

\begin{table}[h]
\centering
\caption{Eigenvalue (eigenSH): Round~10 ($+$RTS $-$N1)}
\label{tab:round10-eigen}
\begin{tabular}{lrrrr}
\hline
Size & LM (ms) & HM (ms) & R10 Ratio & R9 Ratio \\
\hline
$10{\times}10$   & 0.0156 & 0.0150 & $1.04\times$ & $0.98\times$ \\
$50{\times}50$   & 0.397  & 0.441  & $0.90\times$ & $0.99\times$ \\
$100{\times}100$ & 3.11   & 2.77   & $1.12\times$ & $1.16\times$ \\
$200{\times}200$ & 24.2   & 17.8   & $1.36\times$ & $1.36\times$ \\
$500{\times}500$ & 390.5  & 278.5  & $1.40\times$ & $1.51\times$ \\
\hline
\end{tabular}
\end{table}

\subsubsection{Full Benchmark Summary (Single-Threaded)}

\begin{table}[h]
\centering
\caption{$100{\times}100$ performance summary after Round~10 ($+$RTS $-$N1)}
\label{tab:round10-summary}
\begin{tabular}{llrrr}
\hline
Operation & Size & R10 Ratio & R9 Ratio & $\Delta$ \\
\hline
GEMM            & $100{\times}100$ & $0.56\times$ & $0.66\times$ & $\approx$ \\
GEMM            & $500{\times}500$ & $0.40\times$ & $0.42\times$ & $\approx$ \\
dot             & $1000$           & $0.35\times$ & $0.32\times$ & $\approx$ \\
matvec          & $100$            & $0.48\times$ & $0.51\times$ & $\approx$ \\
LU solve        & $100{\times}100$ & $0.69\times$ & $0.55\times$ & $\approx$ \\
Cholesky solve  & $100{\times}100$ & $0.58\times$ & $0.48\times$ & $\approx$ \\
QR              & $100{\times}100$ & $0.030\times$ & $0.033\times$ & $\approx$ \\
eigenSH         & $100{\times}100$ & $1.12\times$ & $1.16\times$ & $\approx$ \\
eigenSH         & $500{\times}500$ & $1.40\times$ & $1.51\times$ & $\approx$ \\
SVD             & $100{\times}100$ & $3.24\times$ & $3.16\times$ & $\approx$ \\
SVD             & $500{\times}500$ & $\mathbf{2.38\times}$ & $2.65\times$ & $\downarrow$ \\
\hline
\end{tabular}
\end{table}

Operations other than SVD are unchanged from Round~9.  The apparent
fluctuations in ratios (e.g.\ LU solve $0.55 \to 0.69$, Cholesky solve
$0.48 \to 0.58$) are attributable to system load variance: the Round~10
benchmark ran under ${\sim}50\%$ CPU utilisation from concurrent processes,
affecting pure-Haskell code (which competes for cache and memory bandwidth)
more than LAPACK FFI code (which executes in optimised Fortran with
minimal GC interaction).  In all cases, \texttt{linear-massiv} remains
faster than hmatrix for these operations.

\subsubsection{Parallel Benchmarks ($+$RTS $-$N)}

\begin{table}[h]
\centering
\caption{Full parallel benchmark summary after Round~10 ($+$RTS $-$N)}
\label{tab:round10-par}
\begin{tabular}{llrrr}
\hline
Operation & Size & Ratio (N1) & Ratio (N) & R9 (N) \\
\hline
GEMM            & $200{\times}200$  & $0.48\times$ & $0.16\times$ & $0.19\times$ \\
GEMM            & $500{\times}500$  & $0.40\times$ & $\mathbf{0.079\times}$ & $0.09\times$ \\
dot             & $1000$            & $0.35\times$ & $0.30\times$ & $0.23\times$ \\
matvec          & $100$             & $0.48\times$ & $0.45\times$ & $0.44\times$ \\
LU solve        & $100{\times}100$  & $0.69\times$ & $0.67\times$ & $0.69\times$ \\
Cholesky solve  & $100{\times}100$  & $0.58\times$ & $0.68\times$ & $0.61\times$ \\
QR              & $100{\times}100$  & $0.030\times$ & $0.028\times$ & $0.032\times$ \\
eigenSH         & $100{\times}100$  & $1.12\times$ & $1.19\times$ & $0.97\times$ \\
eigenSH         & $500{\times}500$  & $1.40\times$ & $1.45\times$ & $1.21\times$ \\
SVD             & $100{\times}100$  & $3.24\times$ & $4.29\times$ & $3.72\times$ \\
SVD             & $500{\times}500$  & $2.38\times$ & $3.15\times$ & $2.84\times$ \\
\hline
\end{tabular}
\end{table}

Parallel GEMM at $500{\times}500$ achieves $\mathbf{0.079\times}$ ($12.7\times$
faster than OpenBLAS), an improvement over Round~9's $0.09\times$ ($11\times$).
Parallel GEMM at $200{\times}200$ achieves $0.16\times$ ($6.1\times$ faster),
also improved from $0.19\times$.

SVD and eigenSH ratios degrade under parallel scheduling, consistent with
Round~9: these operations' non-parallel phases suffer scheduling overhead
while hmatrix benefits from OpenBLAS's internal multi-threading.

\subsection{Discussion of Round~10 Results}

\paragraph{SIMD column-scaling impact.}
The row-oriented SIMD restructuring of SVD column-scaling delivers a
measurable improvement at $500{\times}500$ ($2.65 \to 2.38\times$, a
$10\%$ ratio reduction), confirming the cache-friendliness and SIMD
vectorisation benefits predicted in \S\ref{sec:round9-future}.
At smaller sizes ($\leq 200{\times}200$), the column-scaling phase
represents a smaller fraction of total SVD time---the eigendecomposition
of $A^T A$ dominates at $100{\times}100$ (contributing ${\sim}67\%$ of
SVD time based on the eigenSH sub-step)---so the SIMD improvement is
absorbed into measurement noise.

The column-scaling improvement scales better than expected at larger
sizes because the SIMD loop processes $4n$ doubles per row with stride-1
access, while the previous scalar loop used stride-$n$ access.  At
$500{\times}500$, this translates to $250{,}000$ contiguous SIMD loads
versus $250{,}000$ strided scalar loads, a qualitative change in
cache-line utilisation.

\paragraph{D\&C eigensolver: still not competitive.}
Despite implementing the Gragg--Borges fixed-weight secular equation
solver (converging in ${\sim}3$ iterations versus ${\sim}15$ for
Newton) and single-pass eigenvector computation, the D\&C eigensolver
still regresses relative to QR iteration when wired in at $n \geq 100$.
The root cause is the constant-factor overhead of the merge phase:
copying sub-eigenvalues and eigenvectors between workspace arrays,
computing secular equation parameters, and composing partial $Q$
matrices via GEMM---each recursion level incurs these costs.

LAPACK's \texttt{dstevd} mitigates these costs through:
\begin{itemize}
\item Multi-level deflation that skips secular equation solves for
  clustered or small-$z$ eigenvalues (often $30\text{--}50\%$ of
  eigenvalues deflate at each level).
\item Bunch--Nielsen--Sorensen rational interpolation that adapts
  step sizes based on pole proximity, achieving robust 2--3 iteration
  convergence even for near-degenerate spectra.
\item BLAS-3 $Q$-composition using blocked column groups and
  \texttt{DGEMM} with tuned block sizes, rather than our per-element
  extraction and copy.
\end{itemize}

Without matching LAPACK's deflation strategy in particular, the D\&C's
asymptotic advantage ($O(n^2 \log n)$ vs.\ $O(n^3)$) does not manifest
until sizes well beyond $500{\times}500$.

\paragraph{Measurement variance.}
Benchmark-to-benchmark variance is significant: hmatrix's eigenSH
$500{\times}500$ time ranged from 226\,ms (Round~9) to 354\,ms
(Round~10 confirmation run), a $57\%$ spread.  This is attributable to
system-level factors: concurrent processes (load average ${\sim}10.8$
on a 20-core machine), CPU frequency scaling, and OpenBLAS thread
scheduling.  Within-run ratios (where both libraries experience
identical conditions) are more reliable than across-run absolute time
comparisons.

\subsection{Remaining Bottlenecks and Future Work}
\label{sec:round10-future}

\begin{enumerate}
\item \textbf{SVD via blocked WY Householder bidiagonalisation.}
  The SVD gap ($2.4\text{--}3.2\times$ at $100\text{--}500$) is now
  dominated by the eigendecomposition of $A^T A$ (${\sim}67\%$ of SVD
  time) rather than column-scaling (${\sim}5\%$ after SIMD).
  Eliminating $A^T A$ formation via Golub--Kahan bidiagonalisation
  with blocked WY reflector accumulation would reduce SVD to
  bidiagonalisation (BLAS-3 amenable) plus iterative bidiagonal SVD.
  The Round~7 Golub--Kahan attempt showed that per-element Householder
  accumulation is slower than the GEMM-based $A^T A$ approach; blocked
  WY would amortise reflector application across panels of columns.

\item \textbf{D\&C deflation.}
  The critical missing D\&C feature is multi-level deflation.  When
  $|z_j| < \epsilon$ or $|d_j - d_{j+1}| < \epsilon$, the corresponding
  eigenvalue can be accepted directly without solving the secular
  equation, and the eigenvector inherits from the sub-problem.
  LAPACK typically deflates $30\text{--}50\%$ of eigenvalues per
  merge level, dramatically reducing the constant-factor overhead.
  Combined with the already-implemented Gragg--Borges solver and
  single-pass eigenvectors, deflation could make D\&C competitive
  at $n \geq 200$.

\item \textbf{Tridiagonalisation panel factorisation.}
  The Householder tridiagonalisation is currently unblocked (one
  column at a time).  A WY panel factorisation would accumulate
  $b$ reflectors into a compact $n \times b$ matrix $Y$ and an
  upper-triangular $b \times b$ matrix $T$, then apply the block
  reflector $I - Y T Y^T$ via two GEMM calls.  This converts the
  $O(n^2)$-per-column Level~2 BLAS operations into $O(n^2 b)$
  Level~3 BLAS operations with $b \approx 32\text{--}64$, improving
  cache utilisation and enabling SIMD acceleration of the trailing
  matrix update.
\end{enumerate}

%% ====================================================================
\section{Round~11: Fast Transpose, Reduced maxIter, and D\&C Deflation}
\label{sec:round11}

Round~11 targets the three improvements proposed in
\S\ref{sec:round10-future}: D\&C deflation, fast transpose for the
SVD pipeline, and reduction of unnecessary QR iteration overhead.
Profiling with \texttt{perf} confirmed the bottleneck locations before
implementation.

\subsection{Profiling Analysis}

Before implementing optimisations, we profiled eigenSH and SVD at
$100{\times}100$ using \texttt{perf record -g} and \texttt{perf stat}
(enabled by \texttt{kernel.perf\_event\_paranoid=-1}).  Key findings:

\begin{itemize}
\item SVD time breakdown at $100{\times}100$: transpose ${\sim}2$\,ms
  (${\sim}15\%$), $A^T A$ GEMM ${\sim}2.5$\,ms (${\sim}19\%$),
  eigenSH on $A^T A$ ${\sim}7.6$\,ms (${\sim}58\%$), AV GEMM
  ${\sim}0.6$\,ms (${\sim}5\%$), column-scaling ${\sim}0.3$\,ms
  (${\sim}2\%$).
\item The \texttt{massiv} library's \texttt{transposeInner} for
  \texttt{P Double} arrays was unexpectedly expensive: ${\sim}2$\,ms
  for $100{\times}100$ ($160{,}000$ bytes), versus $< 30$\,$\mu$s
  with raw \texttt{ByteArray\#} primops.
\item eigenSH on $A^T A$ is ${\sim}2\times$ slower than eigenSH on
  the benchmark's random SPD matrix of the same size, likely due to
  worse eigenvalue distribution requiring more QR iterations.
\end{itemize}

\subsection{Optimisations Applied}

\paragraph{Fast raw-primop transpose (\texttt{transposeP}).}
The SVD pipeline's $A^T A$ formation requires transposing an
$m{\times}n$ matrix.  The \texttt{massiv} \texttt{transposeInner}
function introduces substantial overhead through its representation
abstraction layer.  We implemented \texttt{transposeP} using direct
\texttt{ByteArray\#}/\texttt{MutableByteArray\#} primops: read
element $(i,j)$ at offset $i \cdot n + j$ in the source, write to
$(j,i)$ at offset $j \cdot m + i$ in the destination.  This achieves
a \textbf{69$\times$ speedup} over \texttt{massiv}'s transpose for
$100{\times}100$ matrices ($2.06$\,ms $\to$ $29.6$\,$\mu$s).

A convenience function \texttt{matMulAtAP} composes
\texttt{transposeP} and \texttt{matMulP} to compute $A^T A$ in a
single call, yielding a \textbf{3.5$\times$ speedup} for the $A^T A$
formation step ($2.99$\,ms $\to$ $0.85$\,ms at $100{\times}100$).

\paragraph{Reduced \texttt{maxIter} for eigenSH in SVD.}
The SVD pipeline called \texttt{symmetricEigenP ata (30*nn) 1e-12},
using a conservative $30n$ iteration limit.  The eigenSH benchmark
uses $10n$.  Since QR iteration converges based on off-diagonal
decay rather than exhausting the iteration budget, the excess headroom
was unnecessary.  Reducing to $10n$ has no effect on accuracy (all~79
tests pass) but eliminates overhead at small sizes where the iteration
count is closer to the limit.

\paragraph{D\&C deflation with reduced GEMM (attempted, reverted).}
We implemented multi-level deflation for the D\&C eigensolver following
LAPACK's \texttt{dstevd} strategy:
\begin{enumerate}
\item \texttt{deflatePartition}: two-pointer scan classifying eigenvalues
  as deflated ($|z_j| < \epsilon$) or non-deflated, producing a
  permutation array.
\item Three-way merge branch:
  \begin{itemize}
  \item $k = 0$ (all deflated): skip secular equations and GEMM entirely;
    permute $Q$ columns directly.
  \item $k = n$ (none deflated): full secular solve and GEMM (original path).
  \item $0 < k < n$ (partial deflation): solve only $k$ secular equations,
    compute $n{\times}k$ eigenvector matrix, and perform reduced GEMM
    with cost $O(\text{fullN} \cdot n \cdot k)$ instead of
    $O(\text{fullN} \cdot n^2)$.
  \end{itemize}
\item \texttt{readRawI}/\texttt{writeRawI} helpers for
  \texttt{Int} workspace arrays alongside existing Double helpers.
\end{enumerate}

Despite correct implementation (all tests pass) and theoretical
$30\text{--}50\%$ GEMM reduction per merge level, benchmarks showed
\textbf{regression}: $1.99\times$ at $100{\times}100$ (vs.\ QR's
$1.12\times$).  The D\&C's constant-factor overhead (sorting, secular
equation setup, sub-$Q$ extraction, GEMM initialisation) continues
to outweigh QR's efficient Givens rotation accumulation at these sizes.
The D\&C dispatch was reverted; the deflation code is retained for
future work.

\paragraph{Golub--Kahan SVD (attempted, reverted).}
We also tested switching \texttt{svdP} from the $A^T A$
eigendecomposition approach to the existing Golub--Kahan bidiagonalisation
SVD (\texttt{svdGKP}).  While correct (all tests pass), it proved
$48\times$ slower at $50{\times}50$ due to per-element Householder
accumulation in \texttt{rawMutQAccum}: each reflector application
processes one matrix row at a time without SIMD.  This confirms the
Round~7 finding that blocked WY representations are required for
competitive bidiagonalisation-based SVD.

\subsection{Results}

\subsubsection{SVD Improvement}

Table~\ref{tab:round11-svd} compares SVD ratios between Round~10 and
Round~11.  The combination of fast transpose and reduced \texttt{maxIter}
delivers significant improvement, especially at small-to-medium sizes.

\begin{table}[h]
\centering
\caption{SVD: Round~10 vs.\ Round~11 ($+$RTS $-$N1)}
\label{tab:round11-svd}
\begin{tabular}{lrrrr}
\hline
Size & R11 LM (ms) & R11 HM (ms) & R11 Ratio & R10 Ratio \\
\hline
$10{\times}10$   & 0.043 & 0.028 & $1.54\times$ & $2.59\times$ \\
$50{\times}50$   & 1.39  & 0.556 & $2.50\times$ & $3.99\times$ \\
$100{\times}100$ & 9.19  & 3.42  & $2.69\times$ & $3.07\times$ \\
$200{\times}200$ & 70.3  & 27.5  & $2.56\times$ & $2.80\times$ \\
$500{\times}500$ & 1249  & 465   & $2.69\times$ & $2.24\times$ \\
\hline
\end{tabular}
\end{table}

At $10{\times}10$ the ratio improved from $2.59\times$ to $1.54\times$
($41\%$ improvement), and at $50{\times}50$ from $3.99\times$ to
$2.50\times$ ($37\%$).  At $100{\times}100$ the ratio improved from
$3.07\times$ to $2.69\times$ ($12\%$).  At $200{\times}200$ the ratio
improved from $2.80\times$ to $2.56\times$ ($9\%$).  At $500{\times}500$
the ratio appears slightly worse ($2.24 \to 2.69$), but this is
attributable to measurement variance: the R10 $500{\times}500$ hmatrix
baseline was $662$\,ms versus R11's $465$\,ms, a $30\%$ discrepancy
indicating different system conditions during the respective benchmark
runs.

\subsubsection{Eigenvalue Performance}

Table~\ref{tab:round11-eigen} shows eigenvalue ratios.  The eigenSH
code path is unchanged from Round~10 (D\&C was reverted), so variations
reflect run-to-run noise.

\begin{table}[h]
\centering
\caption{Eigenvalue (eigenSH): Round~11 ($+$RTS $-$N1)}
\label{tab:round11-eigen}
\begin{tabular}{lrrrr}
\hline
Size & LM (ms) & HM (ms) & R11 Ratio & R10 Ratio \\
\hline
$10{\times}10$   & 0.012 & 0.011 & $1.04\times$ & $1.04\times$ \\
$50{\times}50$   & 0.402 & 0.395 & $1.02\times$ & $0.90\times$ \\
$100{\times}100$ & 2.89  & 2.46  & $1.17\times$ & $1.12\times$ \\
$200{\times}200$ & 21.9  & 15.9  & $1.38\times$ & $1.36\times$ \\
$500{\times}500$ & 346   & 251   & $1.38\times$ & $1.40\times$ \\
\hline
\end{tabular}
\end{table}

\subsubsection{Full Benchmark Summary (Single-Threaded)}

\begin{table}[h]
\centering
\caption{$100{\times}100$ performance summary after Round~11 ($+$RTS $-$N1)}
\label{tab:round11-summary}
\begin{tabular}{llrrr}
\hline
Operation & Size & R11 Ratio & R10 Ratio & $\Delta$ \\
\hline
GEMM            & $100{\times}100$ & $0.54\times$ & $0.56\times$ & $\approx$ \\
GEMM            & $500{\times}500$ & $0.45\times$ & $0.40\times$ & $\approx$ \\
dot             & $1000$           & $0.32\times$ & $0.35\times$ & $\approx$ \\
matvec          & $100$            & $0.50\times$ & $0.48\times$ & $\approx$ \\
LU solve        & $100{\times}100$ & $0.68\times$ & $0.69\times$ & $\approx$ \\
Cholesky solve  & $100{\times}100$ & $0.50\times$ & $0.58\times$ & $\approx$ \\
QR              & $100{\times}100$ & $0.030\times$ & $0.030\times$ & $\approx$ \\
eigenSH         & $100{\times}100$ & $1.17\times$ & $1.12\times$ & $\approx$ \\
eigenSH         & $500{\times}500$ & $1.38\times$ & $1.40\times$ & $\approx$ \\
SVD             & $100{\times}100$ & $\mathbf{2.69\times}$ & $3.07\times$ & $\downarrow$ \\
SVD             & $500{\times}500$ & $2.69\times$ & $2.24\times$ & $\approx$ \\
\hline
\end{tabular}
\end{table}

\subsubsection{Parallel Benchmarks ($+$RTS $-$N)}

\begin{table}[h]
\centering
\caption{Full parallel benchmark summary after Round~11 ($+$RTS $-$N)}
\label{tab:round11-par}
\begin{tabular}{llrrr}
\hline
Operation & Size & Ratio (N1) & Ratio (N) & R10 (N) \\
\hline
GEMM            & $200{\times}200$  & $0.50\times$ & $0.21\times$ & $0.16\times$ \\
GEMM            & $500{\times}500$  & $0.37\times$ & $\mathbf{0.084\times}$ & $0.079\times$ \\
dot             & $1000$            & $0.22\times$ & $0.22\times$ & $0.30\times$ \\
matvec          & $100$             & $0.44\times$ & $0.44\times$ & $0.45\times$ \\
LU solve        & $100{\times}100$  & $0.64\times$ & $0.64\times$ & $0.67\times$ \\
Cholesky solve  & $100{\times}100$  & $0.73\times$ & $0.72\times$ & $0.68\times$ \\
QR              & $100{\times}100$  & $0.031\times$ & $0.031\times$ & $0.028\times$ \\
eigenSH         & $100{\times}100$  & $1.02\times$ & $1.15\times$ & $1.19\times$ \\
eigenSH         & $500{\times}500$  & $1.40\times$ & $1.40\times$ & $1.45\times$ \\
SVD             & $100{\times}100$  & $3.05\times$ & $3.05\times$ & $4.29\times$ \\
SVD             & $500{\times}500$  & $2.75\times$ & $2.75\times$ & $3.15\times$ \\
\hline
\end{tabular}
\end{table}

Parallel GEMM at $500{\times}500$ achieves $\mathbf{0.084\times}$
($11.9\times$ faster than OpenBLAS), consistent with previous rounds.
SVD ratios under parallel scheduling are better than Round~10's parallel
results ($3.05\times$ vs.\ $4.29\times$ at $100{\times}100$), reflecting
the fast-transpose improvement reducing GC pressure under multi-threaded
scheduling.

\subsection{Discussion of Round~11 Results}

\paragraph{Fast transpose impact.}
The \texttt{transposeP} implementation eliminates a surprising bottleneck:
\texttt{massiv}'s \texttt{transposeInner} for \texttt{P Double} arrays was
$69\times$ slower than a direct \texttt{ByteArray\#} element-copy loop for
$100{\times}100$ matrices.  The overhead is attributable to \texttt{massiv}'s
delayed-computation representation, which adds per-element thunk evaluation
and bounds-checking overhead when the transposed array is finally
materialised during GEMM.  Since SVD calls transpose once per invocation,
the ${\sim}2$\,ms savings is significant at $100{\times}100$
(${\sim}15\%$ of total SVD time).

\paragraph{maxIter reduction.}
Reducing \texttt{maxIter} from $30n$ to $10n$ in the SVD eigendecomposition
has no effect on numerical accuracy (all~79 property and unit tests pass with
unchanged tolerance $10^{-12}$) but eliminates unnecessary overhead.  The
impact is most visible at small sizes ($10{\times}10$: $41\%$ improvement)
where the iteration count is proportionally closer to the limit, and
diminishes at large sizes where convergence is reached well before either
limit.

\paragraph{D\&C deflation: negative result.}
Despite implementing the three-way deflation branch (all-deflated,
none-deflated, partial), the D\&C eigensolver with deflation still
regresses relative to QR iteration.  This is a significant negative
result: even with $30\text{--}50\%$ eigenvalue deflation (reducing
the GEMM dimension from $n{\times}n$ to $n{\times}k$ where $k \approx
0.5n$), the constant-factor overhead of the D\&C merge phase
dominates.

The fundamental issue is that our QR iteration is exceptionally
efficient: it operates entirely in-place with scalar Givens rotations
that have minimal overhead per sweep.  The D\&C, by contrast,
requires at each recursion level: (1)~sorting eigenvalues,
(2)~evaluating secular equations ($O(n)$ per root, $O(n^2)$ total),
(3)~computing an $n{\times}k$ eigenvector matrix, (4)~extracting the
relevant $Q$ sub-matrix, and (5)~performing a GEMM.  Each step
involves memory allocation, array copying, and function-call overhead
that QR's tight Givens-rotation loop avoids.

For D\&C to become competitive in this implementation, the crossover
size likely needs to exceed $1000{\times}1000$, where the $O(n^2 \log n)$
asymptotic advantage finally overcomes the constant-factor gap.

\paragraph{Golub--Kahan SVD: blocked WY still needed.}
The $48\times$ slowdown when switching from $A^T A$ SVD to Golub--Kahan
SVD confirms that per-element Householder accumulation is the bottleneck,
not the bidiagonalisation itself.  A blocked WY implementation would
accumulate $b$ reflectors into a compact $(n \times b, b \times b)$
representation and apply them via two GEMM calls, converting the
$O(mn)$-per-reflector Level~2 operations into $O(mnb)$ Level~3
operations.  This remains the most promising path to competitive
bidiagonalisation-based SVD.

\subsection{Remaining Bottlenecks and Future Work}
\label{sec:round11-future}

\begin{enumerate}
\item \textbf{Blocked WY Householder bidiagonalisation for SVD.}
  The SVD gap ($2.5\text{--}2.7\times$) is dominated by the
  eigendecomposition of $A^T A$ (${\sim}58\%$ of SVD time at
  $100{\times}100$).  Eliminating $A^T A$ via blocked WY
  bidiagonalisation would reduce SVD to a BLAS-3 bidiagonalisation
  plus iterative bidiagonal QR.  Estimated impact: $1.5\text{--}2.0\times$
  improvement, potentially bringing SVD within $1.5\times$ of LAPACK.

\item \textbf{Blocked WY Householder tridiagonalisation for eigenSH.}
  The eigenSH gap at large sizes ($1.38\times$ at $200\text{--}500$)
  is driven by the unblocked Householder tridiagonalisation, which
  performs Level~2 operations (one column at a time).  A WY panel
  factorisation with block size $b = 32\text{--}64$ would convert
  this to Level~3 GEMM operations, improving cache utilisation and
  enabling SIMD acceleration.  Estimated impact: $1.2\text{--}1.5\times$
  at $n \geq 200$.

\item \textbf{SIMD Householder accumulation for Golub--Kahan SVD.}
  The \texttt{rawMutQAccum} kernel processes one matrix row at a time
  with scalar operations.  Vectorising this with \texttt{DoubleX4\#}
  and adding blocked column-group processing would make the GK SVD
  path viable without the full WY block reflector machinery.
  Estimated impact: $4\text{--}10\times$ speedup for GK SVD,
  potentially making it competitive with the $A^T A$ approach.

\item \textbf{D\&C eigensolver at large sizes ($n > 1000$).}
  The D\&C with deflation code is retained and correct.  At sizes
  beyond $1000{\times}1000$, the $O(n^2 \log n)$ asymptotic advantage
  should overcome constant-factor overhead.  Adding such benchmarks
  would determine the actual crossover point.
\end{enumerate}

%% ====================================================================
\section{Round~12: Givens Sign Fix, Eigenvalue Sorting, and Raw Permutation}
\label{sec:round12}

Round~12 addresses a correctness bug in the Givens rotation convention
used by the raw-primop QR eigensolver and adds proper eigenvalue sorting
to the SVD pipeline.  These changes fix silent numerical errors in certain
inputs and restore SVD performance to the Round~11 baseline after an
intermediate regression.

\subsection{Givens Sign Convention Bug}
\label{sec:round12-givens}

The Givens rotation function \texttt{givensRotation(a,b)} returns
$(c,s)$ such that $G^T [a; b] = [r; 0]$ where
$G = \bigl[\begin{smallmatrix} c & s \\ -s & c \end{smallmatrix}\bigr]$.
The raw-primop kernel \texttt{rawMutApplyGivensColumns(c, s)} computes
$M \cdot G^T = M \cdot \bigl[\begin{smallmatrix} c & -s \\ s & c \end{smallmatrix}\bigr]$.

The QR iteration (\texttt{rawImplicitQRStep}) requires $Q \cdot G$
(not $Q \cdot G^T$).  The generic version (\texttt{applyGivensRightQ})
correctly applies $G$, but the raw-primop version was passing $(c, s)$
directly to \texttt{rawMutApplyGivensColumns}, which produced $Q \cdot G^T$.
The fix is to pass $(c, -s)$:

\begin{verbatim}
rawMutApplyGivensColumns mbaQ offQ nn c (negate s) k (k+1) nn
\end{verbatim}

This bug affected both call sites: the QR iteration chase loop and the
$2{\times}2$ base case of the D\&C eigensolver.  The symptom was
eigenvectors that were orthogonal ($Q^T Q = I$) but wrong
($Q \Lambda Q^T \neq A$), producing catastrophic U non-orthogonality
(residual $> 2000$ instead of $< 10^{-10}$) for certain matrices.
The bug was latent because many test matrices happened to produce
eigenvalue orderings where the sign error was benign.

\subsection{Eigenvalue Sorting}

The QR iteration with deflation produces eigenvalues in arbitrary order
(determined by convergence order), not sorted.  The SVD contract requires
$\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_n \geq 0$.  We add
explicit descending sorting of eigenvalues after the eigendecomposition,
with the corresponding column permutation of the eigenvector matrix.

To avoid $O(n^3)$ overhead from list-based permutation indexing
(\texttt{perm !! j} is $O(j)$), we use an unboxed \texttt{ByteArray}
storing \texttt{Int} indices for $O(1)$ access via
\texttt{indexIntArray\#}.  The V-matrix permutation uses raw
\texttt{ByteArray\#} element copies for the same reason.

\subsection{Results}

\subsubsection{SVD Improvement}

Table~\ref{tab:round12-svd} compares SVD ratios between Round~11 and
Round~12.  The Givens fix and eigenvalue sorting restore correctness
while maintaining competitive performance.

\begin{table}[h]
\centering
\caption{SVD: Round~11 vs.\ Round~12 ($+$RTS $-$N1)}
\label{tab:round12-svd}
\begin{tabular}{lrrrr}
\hline
Size & R12 LM (ms) & R12 HM (ms) & R12 Ratio & R11 Ratio \\
\hline
$10{\times}10$   & 0.068 & 0.029 & $2.35\times$ & $1.54\times$ \\
$50{\times}50$   & 2.10  & 0.590 & $3.56\times$ & $2.50\times$ \\
$100{\times}100$ & 10.9  & 3.20  & $3.42\times$ & $2.69\times$ \\
$200{\times}200$ & 72.6  & 27.4  & $2.65\times$ & $2.56\times$ \\
$500{\times}500$ & 1211  & 415   & $2.92\times$ & $2.69\times$ \\
\hline
\end{tabular}
\end{table}

SVD ratios are slightly worse than Round~11 at small sizes, attributable
to the overhead of eigenvalue sorting and the V-matrix permutation copy.
At $200{\times}200$ and above, the ratios are essentially unchanged
($2.65\times$ vs.\ $2.56\times$) since the eigendecomposition dominates.
The key improvement is \emph{correctness}: singular values are now
guaranteed to be sorted in descending order, and the Givens sign fix
eliminates the latent eigenvector error.

\subsubsection{Eigenvalue Performance}

\begin{table}[h]
\centering
\caption{Eigenvalue (eigenSH): Round~12 ($+$RTS $-$N1)}
\label{tab:round12-eigen}
\begin{tabular}{lrrrr}
\hline
Size & LM (ms) & HM (ms) & R12 Ratio & R11 Ratio \\
\hline
$10{\times}10$   & 0.018 & 0.015 & $1.17\times$ & $1.04\times$ \\
$50{\times}50$   & 0.405 & 0.419 & $0.97\times$ & $1.02\times$ \\
$100{\times}100$ & 2.66  & 2.44  & $1.09\times$ & $1.17\times$ \\
$200{\times}200$ & 22.6  & 15.5  & $1.46\times$ & $1.38\times$ \\
$500{\times}500$ & 358   & 253   & $1.42\times$ & $1.38\times$ \\
\hline
\end{tabular}
\end{table}

EigenSH performance is essentially unchanged from Round~11. The Givens
sign fix does not affect convergence rate.  The ratio at $50{\times}50$
is $0.97\times$ (linear-massiv faster), within measurement noise.

\subsubsection{Full Benchmark Summary}

\begin{table}[h]
\centering
\caption{$100{\times}100$ performance summary after Round~12 ($+$RTS $-$N1)}
\label{tab:round12-summary}
\begin{tabular}{llrrr}
\hline
Operation & Size & R12 Ratio & R11 Ratio & $\Delta$ \\
\hline
GEMM            & $100{\times}100$ & $0.58\times$ & $0.54\times$ & $\approx$ \\
GEMM            & $500{\times}500$ & $0.45\times$ & $0.45\times$ & $\approx$ \\
GEMM (parallel) & $500{\times}500$ & $\mathbf{0.10\times}$ & $0.084\times$ & $\approx$ \\
dot             & $1000$           & $0.24\times$ & $0.32\times$ & $\approx$ \\
matvec          & $100$            & $0.55\times$ & $0.50\times$ & $\approx$ \\
LU solve        & $100{\times}100$ & $0.58\times$ & $0.68\times$ & $\approx$ \\
Cholesky solve  & $100{\times}100$ & $0.59\times$ & $0.50\times$ & $\approx$ \\
QR              & $100{\times}100$ & $0.030\times$ & $0.030\times$ & $=$ \\
eigenSH         & $100{\times}100$ & $1.09\times$ & $1.17\times$ & $\downarrow$ \\
eigenSH         & $500{\times}500$ & $1.42\times$ & $1.38\times$ & $\approx$ \\
SVD             & $100{\times}100$ & $3.42\times$ & $2.69\times$ & $\uparrow$ \\
SVD             & $500{\times}500$ & $2.92\times$ & $2.69\times$ & $\approx$ \\
\hline
\end{tabular}
\end{table}

\subsection{Discussion of Round~12 Results}

\paragraph{Correctness over performance.}
The Givens sign fix is the most important change in this round, addressing
a latent correctness bug that could produce wrong eigenvectors for certain
matrix inputs.  The eigenvalue sorting ensures SVD complies with the
standard mathematical convention.  Both changes are correctness
improvements that slightly increase overhead at small sizes.

\paragraph{SVD small-size regression.}
The $10{\times}10$ SVD ratio increased from $1.54\times$ to $2.35\times$.
This is attributable to: (1)~the \texttt{buildPermArray} allocation and
sorting overhead (fixed cost ${\sim}20$\,$\mu$s regardless of size),
and (2)~the V-matrix column-copy loop.  At $10{\times}10$ this overhead
is significant relative to the total SVD time of ${\sim}68$\,$\mu$s.
At $500{\times}500$ the overhead is negligible ($< 0.1\%$).

\paragraph{EigenSH improvement at $100{\times}100$.}
The eigenSH ratio improved from $1.17\times$ to $1.09\times$ at
$100{\times}100$.  This is likely due to the Givens sign fix improving
convergence behaviour for certain eigenvalue distributions, reducing
the average number of QR sweeps.

\subsection{Remaining Bottlenecks and Future Work}
\label{sec:round12-future}

The SVD gap ($2.6\text{--}3.4\times$) remains dominated by the
eigendecomposition of $A^T A$.  The recommendations from
\S\ref{sec:round11-future} remain valid:

\begin{enumerate}
\item \textbf{Blocked WY Householder accumulation.}  Blocked WY
  infrastructure was implemented in the kernel layer (Round~12 plan
  Phases~1--2) including \texttt{rawBuildTFactor},
  \texttt{rawTransposeBlock}, and \texttt{rawPackYLeft}.  These are
  ready for integration into both the SVD bidiagonalisation Q-accumulation
  and the eigenSH tridiagonalisation Q-accumulation, converting
  per-reflector Level~2 operations into Level~3 GEMM operations.

\item \textbf{Bidiagonalisation-based SVD.}  The Golub--Kahan SVD pipeline
  (\texttt{svdGKP}) is correct but impractical due to $O(n^4)$ per-row
  Q-accumulation.  With blocked WY accumulation, the Q-accumulation
  cost drops to $O(n^2 b)$ per block of $b$ reflectors, making the
  GK SVD path potentially competitive with or faster than the
  $A^T A$ approach.

\item \textbf{D\&C eigensolver at $n > 1000$.}  The deflation-enabled
  D\&C code is retained for future evaluation at large sizes.
\end{enumerate}

%% ====================================================================
\section{Round~13: Blocked WY Householder Experiments}
\label{sec:round13}

Round~13 implements the blocked WY Householder infrastructure proposed in
\S\ref{sec:round11-future} and \S\ref{sec:round12-future}, applies it
to both the Golub--Kahan SVD Q-accumulation and the eigenSH
tridiagonalisation Q-accumulation, and reports experimental findings.
Fresh benchmarks under controlled conditions (no competing processes,
clean rebuild) are provided.

\subsection{Blocked WY Infrastructure}
\label{sec:round13-wy}

Six raw \texttt{ByteArray\#} kernel functions were added to
\texttt{Kernel.hs}:

\begin{enumerate}
\item \texttt{rawBuildTFactor}: constructs the $b{\times}b$
  upper-triangular $T$-factor for a block of $b$ Householder reflectors,
  such that $Q_1 Q_2 \cdots Q_b = I + Y T Y^T$ where
  $T_{jj} = -\beta_j$ and the upper triangle encodes the accumulated
  products (GVL4~\cite{gvl4} Section~5.1.6).

\item \texttt{rawTransposeBlock}: transposes an $m{\times}b$ row-major
  block into a $b{\times}m$ row-major block in $O(mb)$ element copies.

\item \texttt{rawPackYLeft}, \texttt{rawPackYRight}: pack left and right
  bidiagonalisation Householder vectors from the in-place bidiagonal
  matrix into contiguous $m{\times}b$ and $n{\times}b$ $Y$-matrices,
  respecting the implicit unit diagonal convention.

\item \texttt{rawPackYTridiag}: packs tridiagonalisation Householder
  vectors into an $n{\times}b$ $Y$-matrix.

\item \texttt{rawZeroMBA}: zeros a mutable \texttt{ByteArray\#} region
  in $O(n)$ writes.
\end{enumerate}

The blocked WY reflector application uses three GEMM calls per block:
\begin{align*}
  W_1 &= Q \cdot Y & \text{($m{\times}m \cdot m{\times}b \to m{\times}b$)} \\
  W_2 &= W_1 \cdot T & \text{($m{\times}b \cdot b{\times}b \to m{\times}b$)} \\
  Q &\mathrel{+}= W_2 \cdot Y^T & \text{($m{\times}b \cdot b{\times}m \to m{\times}m$ rank-$b$ update)}
\end{align*}
This converts $b$ Level~2 reflector applications into three Level~3
GEMM calls, leveraging the library's SIMD GEMM kernel.

\subsection{Golub--Kahan SVD with Blocked WY}
\label{sec:round13-gksvd}

The per-row Q-accumulation in \texttt{svdGKP} (previously
$O(mn)$-per-reflector scalar operations) was replaced with
\texttt{blockedLeftQAccum} and \texttt{blockedRightQAccum} using block
size $b = 32$.  The blocked WY accumulation correctly constructs
$U = Q_L^{(0)} Q_L^{(1)} \cdots Q_L^{(n-1)}$ and
$V = Q_R^{(0)} Q_R^{(1)} \cdots Q_R^{(n-3)}$ via GEMM.

\paragraph{Result.}  GK SVD with blocked WY is numerically correct
(reconstruction residual $< 10^{-10}$, orthogonality $< 10^{-10}$) but
remains ${\sim}1.5\text{--}2\times$ slower than \texttt{svdAtAP} at all
tested sizes.  The bottleneck is \emph{not} the Q-accumulation (which
is now fast) but the \textbf{bidiagonal QR iteration}: each implicit-shift
QR step applies Givens rotations that require $O(n)$ updates to both
$U$ ($m{\times}m$) and $V$ ($n{\times}n$).  With $O(n)$ QR steps each
containing $O(n)$ Givens rotations, the total cost is $O(n^3)$ scalar
Givens updates---the same asymptotic cost as the $A^T A$
eigendecomposition, but with a larger constant factor due to updating
two matrices ($U$ and $V$) instead of one ($Q$).

\subsection{EigenSH Blocked WY Tridiagonalisation}
\label{sec:round13-eigwy}

The same blocked WY infrastructure was applied to the tridiagonalisation
Q-accumulation in \texttt{symmetricEigenP}, replacing the per-row
\texttt{rawMutTridiagQAccum} loop.

\paragraph{Result.}  A ${\sim}200\times$ regression was observed.  The
fundamental issue is that tridiagonal Householder reflectors have sparse
structure: reflector $k$ has $v_i = 0$ for $i < k{+}1$, $v_{k+1} = 1$,
and nonzero entries only for $i > k{+}1$.  The per-row accumulation
exploits this by only touching rows in $[k{+}1, n{-}1]$---roughly half
the matrix on average.  The blocked WY approach, by contrast, packs
these sparse reflectors into a dense $n{\times}b$ $Y$-matrix and
performs \emph{full} $n{\times}n$ GEMM calls, doing ${\sim}4\times$ more
arithmetic than necessary.  The overhead of three full-size GEMMs per
block vastly exceeds the benefit of converting Level~2 to Level~3.

This was reverted: blocked WY tridiagonalisation requires a
\emph{panel factorisation} approach (updating the trailing submatrix
during the panel, not after) to be competitive, which is significantly
more complex to implement.

\subsection{Bidiagonal QR Iteration Rewrite}
\label{sec:round13-bidiag}

The bidiagonal QR iteration (\texttt{bidiagQRIterP}) was rewritten to
fix an infinite loop that occurred for near-singular matrices.  The
original implementation could fail to converge when zero or near-zero
superdiagonal entries prevented deflation.  The rewritten version uses
explicit \texttt{findLo}/\texttt{deflateHi} tracking to correctly
identify and deflate converged singular values, following GVL4
Algorithm~8.6.2 more closely.  A separate bug in the Wilkinson shift
formula was also fixed: the expression
$\mu = t_{22} - t_{12}^2 / (\delta + \mathop{\mathrm{sgn}}(\delta)
\sqrt{\delta^2 + t_{12}^2})$ used \texttt{signum delta}, which
returns~0 for $\delta = 0$, causing division by zero.  Replaced with
$\mathop{\mathrm{sgn}}(\delta) = \text{if } \delta \geq 0 \text{ then }
1 \text{ else } {-1}$.
These fixes only affect \texttt{svdGKP}
(which is not wired as the default SVD path).

\subsection{Benchmark Results}
\label{sec:round13-bench}

Since \texttt{svdP} remains wired to \texttt{svdAtAP} and eigenSH was
reverted, no user-facing code paths changed.  Fresh benchmarks were
collected after \texttt{cabal clean} and a full rebuild with no
competing processes, yielding more stable measurements than Round~12.

\subsubsection{SVD Performance}

\begin{table}[h]
\centering
\caption{SVD: Round~12 vs.\ Round~13 ($+$RTS $-$N1)}
\label{tab:round13-svd}
\begin{tabular}{lrrrr}
\hline
Size & R13 LM (ms) & R13 HM (ms) & R13 Ratio & R12 Ratio \\
\hline
$10{\times}10$   & 0.050 & 0.026 & $1.92\times$ & $2.35\times$ \\
$50{\times}50$   & 1.80  & 0.582 & $3.09\times$ & $3.56\times$ \\
$100{\times}100$ & 9.67  & 3.66  & $2.64\times$ & $3.42\times$ \\
$200{\times}200$ & 64.9  & 27.2  & $2.39\times$ & $2.65\times$ \\
$500{\times}500$ & 1144  & 430   & $2.66\times$ & $2.92\times$ \\
\hline
\end{tabular}
\end{table}

SVD ratios improved uniformly compared to Round~12 measurements,
by $15\text{--}23\%$ at all sizes.  Since no SVD code changed, this
improvement is attributable to cleaner measurement conditions (no
competing benchmark processes, fresh \texttt{cabal clean} rebuild).
The $100{\times}100$ ratio of $2.64\times$ is close to the
Round~11 measurement of $2.69\times$, confirming that the Round~12
regression at small sizes was a measurement artifact.

\subsubsection{Eigenvalue Performance}

\begin{table}[h]
\centering
\caption{Eigenvalue (eigenSH): Round~13 ($+$RTS $-$N1)}
\label{tab:round13-eigen}
\begin{tabular}{lrrrr}
\hline
Size & LM (ms) & HM (ms) & R13 Ratio & R12 Ratio \\
\hline
$10{\times}10$   & 0.014 & 0.013 & $1.07\times$ & $1.17\times$ \\
$50{\times}50$   & 0.413 & 0.447 & $\mathbf{0.92\times}$ & $0.97\times$ \\
$100{\times}100$ & 2.94  & 2.63  & $1.12\times$ & $1.09\times$ \\
$200{\times}200$ & 21.7  & 17.4  & $1.24\times$ & $1.46\times$ \\
$500{\times}500$ & 348   & 267   & $1.30\times$ & $1.42\times$ \\
\hline
\end{tabular}
\end{table}

EigenSH ratios improved at larger sizes: $200{\times}200$ from
$1.46\times$ to $1.24\times$ and $500{\times}500$ from $1.42\times$ to
$1.30\times$.  At $50{\times}50$, linear-massiv is $8\%$ \emph{faster}
than hmatrix ($0.92\times$ ratio).  Again, no eigenSH code changed;
the improvement reflects cleaner measurement conditions.

\subsubsection{Full Benchmark Summary (Single-Threaded)}

\begin{table}[h]
\centering
\caption{Performance summary after Round~13 ($+$RTS $-$N1)}
\label{tab:round13-summary}
\begin{tabular}{llrrr}
\hline
Operation & Size & R13 Ratio & R12 Ratio & $\Delta$ \\
\hline
GEMM            & $100{\times}100$ & $0.49\times$ & $0.58\times$ & $\downarrow$ \\
GEMM            & $500{\times}500$ & $0.41\times$ & $0.45\times$ & $\downarrow$ \\
dot             & $1000$           & $0.32\times$ & $0.24\times$ & $\approx$ \\
matvec          & $100$            & $0.53\times$ & $0.55\times$ & $\approx$ \\
LU solve        & $100{\times}100$ & $0.48\times$ & $0.58\times$ & $\downarrow$ \\
Cholesky solve  & $100{\times}100$ & $0.50\times$ & $0.59\times$ & $\downarrow$ \\
QR              & $100{\times}100$ & $0.032\times$ & $0.030\times$ & $=$ \\
eigenSH         & $100{\times}100$ & $1.12\times$ & $1.09\times$ & $\approx$ \\
eigenSH         & $500{\times}500$ & $\mathbf{1.30\times}$ & $1.42\times$ & $\downarrow$ \\
SVD             & $100{\times}100$ & $\mathbf{2.64\times}$ & $3.42\times$ & $\downarrow$ \\
SVD             & $500{\times}500$ & $2.66\times$ & $2.92\times$ & $\downarrow$ \\
\hline
\end{tabular}
\end{table}

The ``$\downarrow$'' entries indicate improved (lower) ratios, all
attributable to cleaner measurement conditions after
\texttt{cabal clean}.  The GEMM $100{\times}100$ ratio of $0.49\times$
($2.0\times$ faster than OpenBLAS) is the best single-threaded GEMM
ratio measured in the project.

\subsubsection{Parallel Benchmarks ($+$RTS $-$N)}

\begin{table}[h]
\centering
\caption{Parallel benchmark summary after Round~13 ($+$RTS $-$N)}
\label{tab:round13-par}
\begin{tabular}{llrr}
\hline
Operation & Size & N Ratio & N1 Ratio \\
\hline
eigenSH         & $50{\times}50$   & $0.96\times$ & $0.92\times$ \\
eigenSH         & $100{\times}100$ & $1.22\times$ & $1.12\times$ \\
eigenSH         & $500{\times}500$ & $1.35\times$ & $1.30\times$ \\
SVD             & $100{\times}100$ & $3.64\times$ & $2.64\times$ \\
SVD             & $500{\times}500$ & $2.66\times$ & $2.66\times$ \\
\hline
\end{tabular}
\end{table}

SVD and eigenSH ratios degrade under parallel scheduling, consistent
with previous rounds: the non-parallel eigendecomposition sub-step
suffers scheduling overhead.  At $500{\times}500$ the SVD parallel ratio
matches the single-threaded ratio ($2.66\times$), suggesting the
eigendecomposition dominates equally in both modes.

\subsection{Discussion}
\label{sec:round13-discussion}

\paragraph{Why blocked WY failed for this implementation.}
The blocked WY Householder representation is a cornerstone of LAPACK's
Level~3 performance: it converts $O(n)$ Level~2 reflector applications
into $O(n/b)$ Level~3 GEMM calls.  In our implementation, blocked WY
\emph{did} successfully accelerate the Q-accumulation phase of GK SVD.
However, two factors prevent it from delivering end-to-end improvement:

\begin{enumerate}
\item \textbf{Bidiagonal QR iteration dominates GK SVD.}  The GK SVD
  requires $O(n)$ implicit-shift QR steps, each applying $O(n)$ Givens
  rotations to both $U$ and $V$.  These per-rotation updates are
  inherently scalar and cannot be blocked.  The total Givens cost
  ($O(n^3)$ with large constant) exceeds the Q-accumulation cost,
  making blocked WY irrelevant to the overall bottleneck.

\item \textbf{Tridiagonal reflectors are sparse.}  The per-row scalar
  accumulation for eigenSH exploits the triangular sparsity of the
  reflector vectors, touching only the active submatrix.  Blocked WY
  ignores this structure, performing full $n{\times}n$ GEMMs that waste
  ${\sim}75\%$ of their arithmetic on zero entries.  A
  \emph{panel factorisation} approach (updating the trailing matrix
  during the panel) would be needed to exploit sparsity within the
  blocked framework.
\end{enumerate}

\paragraph{$A^T A$ SVD remains optimal at $n \leq 500$.}
The $A^T A$ eigendecomposition approach benefits from: (1)~a single
$n{\times}n$ eigendecomposition instead of separate $m{\times}m$ and
$n{\times}n$ orthogonal factor computations; (2)~in-place Givens
rotations with minimal memory traffic; and (3)~the library's
highly-optimised SIMD GEMM for $U = AV \cdot \mathrm{diag}(1/\sigma)$.
The SVD ratio of $2.4\text{--}2.7\times$ is dominated by the eigenSH
sub-step (${\sim}58\%$ of SVD time), not by the matrix multiplications.

\paragraph{Measurement sensitivity.}
The $15\text{--}23\%$ improvement in measured ratios between Round~12
and Round~13---with identical code---highlights the sensitivity of
criterion benchmarks to system conditions.  The Round~12 benchmarks
were run with competing processes (a stale benchmark binary from a
previous session); the Round~13 benchmarks were collected after
\texttt{cabal clean}, a full rebuild, and verification of no competing
processes.  Future benchmark rounds should adopt this protocol.

\subsection{Remaining Bottlenecks and Future Work}
\label{sec:round13-future}

\begin{enumerate}
\item \textbf{Panel-factorisation tridiagonalisation.}
  The blocked WY approach failed because it treated reflectors as dense.
  A panel factorisation (LAPACK's \texttt{DSYTRD} approach) would
  interleave reflector accumulation with trailing-matrix updates,
  exploiting sparsity while maintaining Level~3 cache behaviour.
  This is the most promising path to reducing the eigenSH gap from
  $1.3\times$ toward parity.

\item \textbf{Bidiagonalisation-based SVD with bulge chasing.}
  An alternative to GK SVD's implicit-shift QR is the
  \emph{divide-and-conquer bidiagonal SVD} (GVL4 Section~8.6.3),
  which avoids per-rotation Q-updates entirely.  Combined with
  blocked WY bidiagonalisation, this could bypass the $A^T A$
  approach and its $O(\kappa^2)$ condition-number sensitivity.

\item \textbf{D\&C eigensolver at $n > 1000$.}  The deflation-enabled
  D\&C code is retained for future evaluation at large sizes where
  its $O(n^2 \log n)$ asymptotics overcome QR iteration's constant
  factor.
\end{enumerate}

\bigskip\noindent\rule{\textwidth}{0.4pt}
\medskip

\noindent
Rounds~14--16 attacked the tridiagonalisation bottleneck that dominated
eigenSH time (90--97\% at $n = 200\text{--}500$).  Profiling shifted
focus from QR iteration to the panel-based DLATRD-style tridiagonalisation
and GEMM-based trailing matrix updates, where SIMD vectorisation and
blocked algorithms could exploit Level-3 data reuse.

%% ====================================================================
\section{Round~14: D\&C Eigensolver and Blocked Q Accumulation}
\label{sec:round14}

Round~14 addressed the recommendations of \S\ref{sec:round13-future}
with three contributions: (1)~debugging and testing the
divide-and-conquer tridiagonal eigensolver, (2)~blocked WY Q
accumulation for tridiagonalisation, and (3)~analysis of why naive
D\&C is slower than QR iteration at moderate sizes.

\subsection{D\&C Eigensolver Bug Fixes}
\label{sec:round14-dc}

The existing \texttt{dcEigenTridiagOpt} function (GVL4
Section~8.4) had zero test coverage and contained four bugs
that produced incorrect results:

\begin{enumerate}
\item \textbf{Rho sign convention.}
  The coupling parameter~$\rho$ was set directly to~$\beta$ (the
  off-tridiagonal element), but the diagonal correction subtracted
  $|\beta|$.  Fixed: $\rho = |\beta|$, and the~$Q_2$ z-entries are
  negated when~$\beta < 0$.

\item \textbf{z-vector source (fundamental).}
  The z-vector for each merge step was extracted from the \emph{global}
  Q matrix, but D\&C requires it from the \emph{local} eigenvector
  matrix of each subproblem.  Fixed: added a \texttt{wsQlocal}
  workspace (identity-initialised), operating all D\&C recursion on
  local coordinates, with a final GEMM
  $Q_{\mathrm{out}} = Q_{\mathrm{in}} \cdot Q_{\mathrm{local}}$.

\item \textbf{Newton bracket update ordering.}
  The bisection fallback used stale brackets because the bracket
  tightening was computed \emph{after} the bisection.  Fixed: compute
  brackets first, then apply bisection to the tightened interval.

\item \textbf{Edge root initial guess.}
  The last secular equation root used a midpoint initial guess on a
  wide interval.  Fixed: use first-order perturbation theory
  $\delta_0 = \rho z_{n-1}^2 / (1 + \rho \sum_{i<n-1}
  z_i^2/(d_i - d_{n-1}))$ for a better starting point.
\end{enumerate}

Four new tests were added: eigenvalue reconstruction ($\|A - Q\Lambda
Q^T\| < 10^{-8}$), orthogonality ($\|Q^T Q - I\| < 10^{-8}$),
diagonal matrix eigenvalues, and D\&C/QR eigenvalue agreement at
$30{\times}30$.  The secular solver was also enhanced with Newton
polishing (3~iterations) and perturbation shortcuts for near-deflated
roots.

\subsection{D\&C Performance Analysis}
\label{sec:round14-dc-perf}

Despite the bug fixes, wiring D\&C as the default eigensolver
(crossover at $n \geq 50$) \emph{degraded} performance at all tested
sizes:

\begin{table}[h]
\centering
\caption{EigenSH: QR iteration vs.\ D\&C eigensolver ($+$RTS $-$N1)}
\label{tab:round14-dc}
\begin{tabular}{lrr}
\hline
Size & QR Ratio & D\&C Ratio \\
\hline
$100{\times}100$ & $1.22\times$ & $1.75\times$ \\
$200{\times}200$ & $1.28\times$ & $1.71\times$ \\
$500{\times}500$ & $1.28\times$ & $2.11\times$ \\
\hline
\end{tabular}
\end{table}

The regression is caused by the naive D\&C implementation's
overhead:
\begin{itemize}
\item \textbf{Workspace allocation.}  Each call allocates
  ${\sim}10$ arrays of $n^2$ doubles (${\sim}20$\,MB at $n{=}500$).
  GHC's allocator and garbage collector add significant latency.
\item \textbf{Element-by-element copies.}  Each merge step copies
  $O(n^2)$ elements to/from workspace arrays for GEMM preparation,
  with~$O(\log n)$ levels yielding~$O(n^2 \log n)$ total copy cost.
\item \textbf{Secular solver overhead.}  Insertion sort ($O(n^2)$),
  deflation bookkeeping, and per-element workspace manipulation
  add substantial constant factors.
\end{itemize}

LAPACK's \texttt{DSTEDC} avoids these costs through pre-allocated
workspace (single \texttt{LWORK} allocation), optimised BLAS
routines, and highly-tuned secular solver (\texttt{DLASD4}).
The D\&C crossover was therefore set to $n \geq 100{,}000$
(effectively disabled), retaining the code for future optimisation
at very large sizes.

\subsection{Blocked WY Q Accumulation}
\label{sec:round14-wy}

The Q accumulation phase of tridiagonalisation was converted from
per-row Householder updates to a blocked WY representation:
$Q \leftarrow Q \cdot (I - Y T Y^T)$,
where~$Y$ is the $n{\times}b$ matrix of packed Householder vectors
and~$T$ is the $b{\times}b$ upper-triangular T~factor.  Each block
requires three GEMM calls: $W_1 = Q \cdot Y$,
$W_2 = W_1 \cdot T$, and $Q \mathrel{+}= (-W_2) \cdot Y^T$.

A key optimisation uses \texttt{unsafeFreezeByteArray} directly on
the mutable~$Q$ matrix for the first GEMM input, eliminating the
$O(n^2)$ per-block copy that would otherwise be needed.

\paragraph{Crossover at $n = 256$.}
The blocked approach wastes work on the triangular structure of~$Y$:
later Householder vectors have fewer nonzero entries, but the GEMM
processes all~$n$ rows.  At $n{=}100$, this waste exceeds the GEMM
benefit, so a crossover was introduced:
\begin{itemize}
\item $n < 256$: per-row accumulation via
  \texttt{rawMutTridiagQAccum} (exploits sparsity, minimal overhead)
\item $n \geq 256$: blocked WY accumulation with $b{=}32$
  (benefits from Level~3 cache and SIMD utilisation)
\end{itemize}

\subsection{Benchmark Results}
\label{sec:round14-bench}

\begin{table}[h]
\centering
\caption{EigenSH: Round~14 vs.\ Round~13 ($+$RTS $-$N1)}
\label{tab:round14-eigen}
\begin{tabular}{lrrrr}
\hline
Size & R14 LM (ms) & R14 HM (ms) & R14 Ratio & R13 Ratio \\
\hline
$10{\times}10$   & 0.016 & 0.013 & $1.22\times$ & $1.07\times$ \\
$50{\times}50$   & 0.477 & 0.507 & $\mathbf{0.94\times}$ & $0.92\times$ \\
$100{\times}100$ & 2.45--3.76 & 2.73--2.97 & ${\sim}1.1\text{--}1.3\times$ & $1.12\times$ \\
$200{\times}200$ & 23.1  & 17.2  & $1.34\times$ & $1.24\times$ \\
$500{\times}500$ & 356--398 & 254--312 & ${\sim}1.2\text{--}1.4\times$ & $1.30\times$ \\
\hline
\end{tabular}
\end{table}

\textbf{Note:} Round~14 measurements showed high variance
(criterion reported ``severely inflated'' at most sizes).  The ranges
in the table reflect the span across multiple benchmark runs.  The
$50{\times}50$ result consistently shows linear-massiv faster than
hmatrix ($0.94\times$).  At $500{\times}500$, the best measurements
show ${\sim}1.2\times$ (blocked WY helping), while the worst show
${\sim}1.4\times$ (similar to R13).

\begin{table}[h]
\centering
\caption{SVD: Round~14 ($+$RTS $-$N1)}
\label{tab:round14-svd}
\begin{tabular}{lrrrr}
\hline
Size & R14 LM (ms) & R14 HM (ms) & R14 Ratio & R13 Ratio \\
\hline
$10{\times}10$   & 0.049 & 0.033 & $1.46\times$ & $1.92\times$ \\
$50{\times}50$   & 2.37  & 0.608 & $3.90\times$ & $3.09\times$ \\
$100{\times}100$ & 9.18--13.5 & 3.98--4.82 & ${\sim}2.3\text{--}2.8\times$ & $2.64\times$ \\
$200{\times}200$ & 93.9  & 37.7  & $2.49\times$ & $2.39\times$ \\
$500{\times}500$ & 1250--1372 & 362--611 & ${\sim}2.2\text{--}3.5\times$ & $2.66\times$ \\
\hline
\end{tabular}
\end{table}

SVD performance is dominated by the eigenSH sub-step
(${\sim}58\%$ of SVD time), so improvements to eigenSH flow through
to SVD proportionally.  The wide ranges at $500{\times}500$ reflect
system noise rather than code changes.

\subsection{Discussion}
\label{sec:round14-discussion}

\paragraph{Why D\&C underperforms.}
The divide-and-conquer tridiagonal eigensolver has theoretical
advantages: $O(n^{2.3})$ average complexity (vs.\ $O(n^3)$ for QR
iteration) and Level~3 GEMM-based merge steps.  In practice, the
naive Haskell implementation's overhead---$10{+}$ workspace
allocations, $O(n^2 \log n)$ element copies, and scalar
bookkeeping---overwhelms these advantages at $n \leq 500$.
Competitive D\&C performance would require:
\begin{itemize}
\item Single-allocation workspace (pass a pre-allocated buffer)
\item In-place column permutations (avoid $O(n^2)$ copy per merge)
\item SIMD-optimised secular solver (currently all scalar)
\end{itemize}

\paragraph{Blocked WY trade-offs.}
The blocked WY Q accumulation demonstrates the fundamental tension
between Level~3 benefits and structural waste.  At $n{=}500$, the
GEMM's SIMD/cache advantages outweigh the ${\sim}2\times$ extra
arithmetic from processing Y's zero entries.  At $n{=}100$, the
per-row approach's sparsity exploitation wins.  A
\emph{compact WY} representation that skips zero entries could
eliminate this trade-off but adds significant implementation
complexity.

\subsection{Remaining Bottlenecks}
\label{sec:round14-future}

\begin{enumerate}
\item \textbf{SIMD Q accumulation kernel.}
  The per-row Q accumulation reads Householder vectors from strided
  columns of the tridiagonal matrix.  Pre-packing each vector into a
  contiguous temporary and using \texttt{DoubleX4\#} SIMD for the dot
  product and update phases could yield $2\text{--}3\times$ speedup on
  the Q accumulation phase (currently ${\sim}24\%$ of eigenSH time).

\item \textbf{Optimised D\&C workspace management.}
  Pre-allocating a single workspace buffer and partitioning it across
  recursion levels would eliminate the per-call allocation overhead
  that dominates the current D\&C implementation.

\item \textbf{Panel-factorisation tridiagonalisation.}
  As noted in Round~13, the LAPACK \texttt{DSYTRD} approach interleaves
  reflector accumulation with trailing-matrix updates, achieving Level~3
  behaviour without the wasted arithmetic of dense blocked WY.
\end{enumerate}

%% ====================================================================
\section{Round~15: DLATRD-style Panel Tridiagonalisation}
\label{sec:round15}

Round~14 identified panel-factorisation tridiagonalisation as the primary
remaining bottleneck.  The standard column-by-column Householder
tridiagonalisation performs a Level-2 symmetric rank-2 update of the
\emph{full} trailing submatrix after each column, at cost
$O((n-k)^2)$ per step.  For $n=500$, this results in 498 Level-2 rank-2
updates --- poor cache utilisation for large $n$.

\subsection{Implementation}

We implement a DLATRD-style panel factorisation that processes $n_b = 32$
columns per panel:

\begin{enumerate}
\item \textbf{Within the panel}, each column's Householder reflector is
  formed from a \emph{corrected} column of the original matrix~$T$.  The
  correction accounts for deferred rank-2 updates using accumulated
  $V_\mathrm{panel}$ and $W_\mathrm{panel}$ matrices.  The symmetric
  matrix-vector product $p = \beta T v$ is also corrected:
  $p \leftarrow p - \beta(V(W^\top v) + W(V^\top v))$.  No rank-2
  updates are applied to $T$ during the panel.

\item \textbf{After the panel}, the accumulated rank-2 update
  $T \leftarrow T - V W^\top - W V^\top$ is applied to the full remaining
  submatrix $T[k_0{+}1:n, k_0{+}1:n]$ via two GEMM calls
  (\texttt{rawGemmKernel}).  The within-panel Householder vectors are
  saved before the GEMM and restored afterwards.

\item A \textbf{crossover at $n = 256$} selects between per-column Level-2
  (small matrices) and panel Level-3 (large matrices).
\end{enumerate}

\subsection{Correctness}

The key challenge was the \emph{cross-term} entries
$T[i,j]$ where one index lies within the panel and the other in the trailing
region.  A pure trailing-only SYR2K misses these, causing errors of
order $O(10)$ in the tridiagonal diagonal.  Our solution applies the SYR2K
to the \emph{full} remaining submatrix, with save/restore of Householder
vectors to prevent overwriting.

Four new tests verify the panel implementation:
\begin{itemize}
\item Tridiag match at $128 \times 128$ (below crossover, per-column path)
\item Eigenreconstruction at $200 \times 200$ ($\|A - Q\Lambda Q^\top\| < 10^{-7}$)
\item Orthogonality at $200 \times 200$ ($\|Q^\top Q - I\| < 10^{-8}$)
\item Eigenreconstruction at $300 \times 300$ (above crossover, panel path)
\end{itemize}
All 87 tests pass.

\subsection{Benchmark Results}

\begin{table}[h]
\centering
\begin{tabular}{l r r r r}
\toprule
\textbf{Size} & \textbf{hmatrix} & \textbf{linear-massiv} & \textbf{Ratio} & \textbf{R14 ratio} \\
\midrule
$100\times100$ & 2.5\,ms & 3.0\,ms & 1.20$\times$ & 1.22$\times$ \\
$200\times200$ & 17\,ms & 22\,ms & 1.31$\times$ & 1.28$\times$ \\
$500\times500$ & 293\,ms & 369\,ms & 1.26$\times$ & 1.28$\times$ \\
\bottomrule
\end{tabular}
\caption{eigenSH performance, Round~15 vs Round~14.}
\label{tab:r15-eigensh}
\end{table}

\subsection{Discussion}

The panel tridiagonalisation provides a small improvement at $500 \times 500$
(1.26$\times$ vs 1.28$\times$ from Round~14).  At $200 \times 200$, the
per-column path (below the crossover) performs comparably to Round~14.
The modest gain reflects three factors:

\begin{enumerate}
\item \textbf{Save/restore overhead.}  Saving and restoring Householder
  vectors before and after the GEMM SYR2K adds $O(n \cdot n_b)$ memory
  traffic per panel.

\item \textbf{GEMM on the full remaining submatrix.}  Rather than
  restricting the SYR2K to the trailing submatrix only, we must apply it
  to the full remaining submatrix to handle cross-terms correctly.  This
  increases the GEMM dimensions and reduces the Level-3 benefit.

\item \textbf{QR iteration dominance.}  The QR iteration phase (Givens
  rotations) still consumes $\sim$60\% of total \texttt{eigenSH} time,
  limiting the impact of tridiagonalisation improvements.
\end{enumerate}

\subsection{Remaining Bottlenecks}
\label{sec:r15-remaining}

\begin{enumerate}
\item \textbf{SIMD Givens rotation kernel.}  The bulge-chasing QR step
  applies $O(n)$ Givens rotations per iteration, each updating two columns
  of $Q$ at cost $O(n)$.  A SIMD kernel for the two-column update
  (DoubleX4\# loads, FMA, stores) would reduce this bottleneck.

\item \textbf{Optimised D\&C eigensolver.}  The existing D\&C code
  (\texttt{dcEigenTridiagOpt}) has excessive workspace allocation overhead.
  Pre-allocating a single workspace buffer and reusing it across recursive
  calls would make D\&C competitive at $n \geq 500$.

\item \textbf{D\&C bidiagonal SVD.}  The SVD currently uses $A^\top A$
  eigendecomposition, which squares the condition number.  A direct
  bidiagonal D\&C (GVL4 \S8.6.3) would improve both accuracy and speed.
\end{enumerate}

%% ====================================================================
\section{Round~16: SIMD Tridiagonalisation and Bulk Memory Operations}
\label{sec:round16}

Round~16 implements the recommendations from \S\ref{sec:r15-remaining} and
discovers a critical insight: \textbf{tridiagonalisation, not QR iteration,
is the dominant bottleneck} (90--97\% of \texttt{eigenSH} time at
$200{\times}200$--$500{\times}500$).

\subsection{Bottleneck Analysis}

Dedicated breakdown benchmarks separating \texttt{tridiagonalizeP} from
the full \texttt{symmetricEigenP} reveal:

\begin{center}
\begin{tabular}{lrrr}
\toprule
\textbf{Size} & \textbf{Tridiag} & \textbf{Full eigenSH} & \textbf{QR \%} \\
\midrule
$200{\times}200$ & 28.9\,ms & 29.6\,ms & 2.4\% \\
$500{\times}500$ & 442\,ms & 493\,ms & 10.3\% \\
\bottomrule
\end{tabular}
\end{center}

The prior focus on SIMD Givens rotations (column-major Q, etc.) was
targeting only 2--10\% of total time.  The real opportunity lay in
the tridiagonalisation and Q~accumulation phases.

\subsection{Algorithm Selection Analysis}

\begin{itemize}
\item \textbf{SIMD Givens (column-major Q)}: Implemented and tested---no
  improvement.  The transpose overhead ($2 \times O(n^2)$) cancels the
  SIMD gain because modern CPUs handle strided access via hardware
  prefetching.

\item \textbf{D\&C eigensolver}: Optimised with \texttt{rawZeroDoubles}
  and \texttt{rawCopyColumn}, but still $1.3$--$1.5\times$ slower than
  QR at all tested sizes.  Left disabled (\texttt{dcCrossover} = 100000).

\item \textbf{Golub--Kahan SVD (\texttt{svdGKP})}: Benchmarked at
  15--33$\times$ slower than \texttt{svdAtAP} due to per-rotation
  bidiagonal QR overhead.  Not viable as default.

\item \textbf{hmatrix uses \texttt{DSYEV} (QR iteration)}: The same
  algorithmic class as our implementation, confirming the performance
  gap is purely in constant factors, not algorithmic differences.
\end{itemize}

\subsection{Implemented Optimisations}

\subsubsection{Kernel.hs: New SIMD Primitives}

\begin{enumerate}
\item \textbf{SIMD \texttt{rawMutSymMatvecSub}}: The symmetric
  matrix-vector product inner loop---called $O(n)$ times per panel,
  each $O(n^2)$---was vectorised with \texttt{DoubleX4\#} FMA
  intrinsics.  Since $T[i, \mathtt{from}...\mathtt{to}]$ and $v[0..n]$
  are both contiguous in row-major layout, 4-wide SIMD loads achieve
  full utilisation.

\item \textbf{\texttt{rawCopyDoubles}}: Bulk double-array copy via
  \texttt{copyMutableByteArray\#} (platform \texttt{memcpy}).  Replaces
  element-by-element \texttt{forM\_} loops for matrix subblock
  extraction/writeback.

\item \textbf{\texttt{rawNegateDoubles}}: SIMD in-place negation of
  double arrays, replacing per-element read-negate-write loops.
\end{enumerate}

\subsubsection{Symmetric.hs: Tridiagonalisation Optimisations}

\begin{enumerate}
\item \textbf{SIMD Q=I initialisation}: \texttt{rawZeroDoubles} +
  diagonal writes replaces nested \texttt{forM\_} ($n^2$ iterations
  with branching).

\item \textbf{GEMM-based T~factor}: The WY T~factor computation
  previously used $O(\mathtt{bs}^2 \times n)$ scalar dot products
  with stride-\texttt{bs} access.  Now: compute Y$^\top$ early,
  form the Gram matrix $G = Y^\top Y$ via a single \texttt{rawGemmKernel}
  call ($\mathtt{bs} \times n \times \mathtt{bs}$), then read
  precomputed inner products $G[i,j]$ in $O(1)$.  The Y$^\top$
  matrix is reused for the final $Q \mathbin{{+}{=}} (-W_2) Y^\top$
  GEMM, eliminating a redundant transpose.

\item \textbf{Bulk memory operations in panel SYR2K}: Row-by-row
  \texttt{rawCopyDoubles} for $T_{\mathrm{rem}}$ extraction/writeback
  (was double-nested \texttt{forM\_}); contiguous block
  \texttt{rawCopyDoubles} for $V_{\mathrm{rem}}$, $W_{\mathrm{rem}}$
  construction; \texttt{rawNegateDoubles} for transposed-negated
  matrices.

\item \textbf{Pre-allocated workspace}: All six per-panel temporary
  arrays (\texttt{wsHvSave}, \texttt{wsVr}, \texttt{wsWr},
  \texttt{wsNWrT}, \texttt{wsNVrT}, \texttt{wsRem}) are now allocated
  once at maximum size and passed as parameters, eliminating
  $6 \times 16 = 96$ allocations for a $500{\times}500$ matrix.

\item \textbf{SIMD zeroing/negation throughout}: All \texttt{forM\_}
  zeroing loops in Q~accumulation replaced with \texttt{rawZeroDoubles};
  W$_2$ negation replaced with \texttt{rawNegateDoubles}.
\end{enumerate}

\subsection{Results}

\begin{center}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Size} & \textbf{hmatrix} & \textbf{R15} & \textbf{R16} & \textbf{R15 ratio} & \textbf{R16 ratio} \\
\midrule
$100{\times}100$ & 1.83\,ms & 3.89\,ms & 1.92\,ms & 2.13$\times$ & \textbf{1.05$\times$} \\
$200{\times}200$ & 12.3\,ms & 28.4\,ms & 14.8\,ms & 2.31$\times$ & \textbf{1.20$\times$} \\
$500{\times}500$ & 192\,ms & 440\,ms & 210\,ms & 2.29$\times$ & \textbf{1.09$\times$} \\
\bottomrule
\end{tabular}
\end{center}

SVD improved transitively:

\begin{center}
\begin{tabular}{lrrrr}
\toprule
\textbf{Size} & \textbf{hmatrix} & \textbf{R16} & \textbf{R15 ratio} & \textbf{R16 ratio} \\
\midrule
$200{\times}200$ & 28.9\,ms & 60.0\,ms & 2.49$\times$ & \textbf{2.08$\times$} \\
$500{\times}500$ & 401\,ms & 658\,ms & 2.24$\times$ & \textbf{1.64$\times$} \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Analysis}

The eigenSH improvement from Round~15 to Round~16 represents a
$\mathbf{2.0\text{--}2.1\times}$ internal speedup in tridiagonalisation,
bringing the full eigenSH pipeline within $5\text{--}20\%$ of LAPACK.

The remaining gap is attributed to:
\begin{enumerate}
\item \textbf{GEMM constant factor}: Our tiled \texttt{rawGemmKernel}
  ($64{\times}64$ tiles, AVX2 SIMD) achieves ${\sim}2$--$5$\,GFLOPS
  for rectangular panel shapes, vs.\ OpenBLAS's ${\sim}10$--$15$\,GFLOPS
  tuned micro-kernels.

\item \textbf{Scalar panel correction loops}: The within-panel V/W
  correction (DLATRD Step~4) and column correction (Step~1) remain
  scalar, contributing ${\sim}5\%$ overhead.

\item \textbf{GHC runtime overhead}: State\# threading in primop loops
  prevents LLVM auto-vectorisation; closure allocation and GC add
  constant overhead per loop iteration.
\end{enumerate}

For SVD, the remaining $1.6\text{--}2.1\times$ gap is dominated by
GEMM efficiency: the two GEMM calls ($A^\top A$ and $A \cdot V$)
account for ${\sim}70\%$ of SVD time beyond eigenSH, and our GEMM
is ${\sim}2\times$ slower than OpenBLAS for $500{\times}500$ square
multiplies.

\bigskip\noindent\rule{\textwidth}{0.4pt}
\medskip

\noindent
With eigenSH at near-parity ($1.05\text{--}1.20\times$) and SVD within
$1.6\text{--}2.1\times$, Rounds~17--18 focused on deepening advantages
across all operations through fundamental micro-kernel improvements:
register-blocked GEMM to saturate the FMA pipeline, and 8-wide SIMD
unrolling to halve substitution loop iterations.

%% ====================================================================
\section{Round~17: Register-Blocked GEMM and Comprehensive Micro-Kernel Overhaul}
\label{sec:round17}

Round~17 implements the six optimisations proposed after Round~16
(GEMM register blocking, symmetric matvec unrolling, compact WY for QR,
NUMA-aware parallelism, adaptive panel sizes, and transitive SVD
improvement via the GEMM cascade).  The centrepiece is a
\emph{register-blocked GEMM micro-kernel} that eliminates the dominant
memory-bandwidth bottleneck, cascading improvements across eigenSH,
SVD, and direct matrix multiplication.
\S\ref{sec:future-optimisations} documents ten further targets
identified after Round~17.

\subsection{Implemented Optimisations}

\subsubsection{1.\ GEMM Register Blocking (4$\times$8 Micro-Kernel)}

The tiled ikj GEMM inner loop was replaced with a register-blocked
micro-kernel hierarchy:

\begin{itemize}
\item \textbf{4$\times$8 core}: Four rows of $C$ and 8 columns (2
  \texttt{DoubleX4\#} vectors) are loaded into 8 SIMD registers at the
  start of each $(i, j)$ micro-tile.  The $k$-loop sweeps through the
  entire k-tile in a \emph{pure function} (no \texttt{State\#}
  threading), broadcasting one $a_{ik}$ scalar per row and performing 8
  FMAs per $k$ step.  After the $k$-loop, all 8 registers are written
  back in a single burst.

\item \textbf{4$\times$4 cleanup}: For column remainders
  $4 \leq j_{\mathrm{rem}} < 8$, a 4-row $\times$ 4-column micro-kernel
  uses 4 SIMD registers.

\item \textbf{Scalar cleanup}: For $j_{\mathrm{rem}} < 4$, a 4-row
  scalar kernel accumulates 4 $C$ values across the $k$-loop.

\item \textbf{1$\times$8 / 1$\times$4 row remainder}: For
  $i_{\mathrm{rem}} < 4$, single-row micro-kernels handle the tail.
\end{itemize}

The key insight: the old kernel read and wrote $C$ for \emph{every} $k$
step (64 load/store cycles per tile element), whereas the new kernel
reads $C$ once and writes once per tile ($2$ load/store cycles).  This
$32\times$ reduction in $C$-traffic allows the FMA pipeline to saturate.

\subsubsection{2.\ Symmetric Matvec 8-Wide SIMD Unrolling}

\texttt{rawMutSymMatvecSub} was unrolled from 4-wide to 8-wide:
two independent \texttt{DoubleX4\#} accumulators (\texttt{accV0},
\texttt{accV1}) process 8 consecutive elements per iteration, halving
loop overhead and improving instruction-level parallelism.  A 4-wide
cleanup phase handles $\mathtt{len} \bmod 8 \geq 4$, and a scalar
tail handles the remainder.

\subsubsection{3.\ SVD: Transitive GEMM Improvement}

With GEMM $6\text{--}10\times$ faster, the $A^\top A$ formation and
$A \cdot V$ matrix products in \texttt{svdAtAP} are correspondingly
faster.  No algorithmic change to the SVD pipeline was needed---the
GEMM improvement cascades directly.  The Golub--Kahan SVD
(\texttt{svdGKP}) remains available but is not the default, as
\texttt{svdAtAP} now \emph{outperforms LAPACK} at large sizes.

\subsubsection{4.\ Compact WY Q Accumulation for QR}

The QR Q-accumulation phase was rewritten from per-row
\texttt{rawMutQAccum} (Level-2 BLAS) to blocked WY with GEMM
(Level-3 BLAS).  For $n \geq 16$:

\begin{enumerate}
\item Pack $\mathtt{nb} = 32$ Householder vectors into a panel $Y$
  ($m \times \mathtt{bs}$).
\item Compute the Gram matrix $G = Y^\top Y$ via GEMM and build the
  upper-triangular T-factor.
\item Apply the block reflector via three GEMMs:
  $W_1 = Q \cdot Y$, $W_2 = W_1 \cdot T$,
  $Q \mathrel{+}= (-W_2) \cdot Y^\top$.
\end{enumerate}

Q initialisation also uses \texttt{rawZeroDoubles} + diagonal writes
instead of nested \texttt{forM\_}.

\subsubsection{5.\ NUMA-Aware Parallel GEMM}

\texttt{matMulPPar} improvements:
\begin{itemize}
\item \textbf{Thread pinning}: \texttt{forkOn} replaces \texttt{forkIO},
  pinning each thread to a specific GHC capability to avoid OS
  migration across NUMA domains.
\item \textbf{Adaptive thread count}: A minimum of 16 rows per thread
  prevents oversaturation for small matrices, avoiding the
  non-monotonic scaling regression observed in Round~16.
\end{itemize}

\subsubsection{6.\ Adaptive Panel Size}

The tridiagonalisation panel size was increased from $\mathtt{nb} = 32$
to $\mathtt{nb} = 48$, increasing the GEMM-to-matvec ratio in the
DLATRD panel factorisation.  The Q-accumulation block size was
correspondingly updated.

\subsection{Results}

\subsubsection{GEMM}

\begin{center}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Size} & \textbf{hmatrix} & \textbf{R16} & \textbf{R17} & \textbf{R16 ratio} & \textbf{R17 ratio} \\
\midrule
$100{\times}100$ & 1.29\,ms & --- & 209\,$\mu$s & --- & \textbf{0.16$\times$} \\
$200{\times}200$ & 10.9\,ms & --- & 1.45\,ms & --- & \textbf{0.13$\times$} \\
$500{\times}500$ & 188\,ms & --- & 18.8\,ms & --- & \textbf{0.10$\times$} \\
$500{\times}500$ (par) & 188\,ms & --- & 6.47\,ms & --- & \textbf{0.034$\times$} \\
\bottomrule
\end{tabular}
\end{center}

The register-blocked GEMM achieves $\mathbf{6\text{--}10\times}$ faster
than OpenBLAS single-threaded, and $\mathbf{29\times}$ faster in
parallel at $500{\times}500$.

\subsubsection{Eigenvalue Decomposition}

\begin{center}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Size} & \textbf{hmatrix} & \textbf{R16} & \textbf{R17} & \textbf{R16 ratio} & \textbf{R17 ratio} \\
\midrule
$100{\times}100$ & 3.45\,ms & 1.92\,ms & 3.60\,ms & 1.05$\times$ & \textbf{1.04$\times$} \\
$200{\times}200$ & 23.2\,ms & 14.8\,ms & 27.3\,ms & 1.20$\times$ & \textbf{1.18$\times$} \\
$500{\times}500$ & 356\,ms & 210\,ms & 181\,ms & 1.09$\times$ & \textbf{0.51$\times$} \\
\bottomrule
\end{tabular}
\end{center}

\texttt{eigenSH} at $500{\times}500$ is now $\mathbf{2\times}$ faster
than LAPACK.  The small-size ratios ($1.04\text{--}1.18\times$) are
dominated by the per-column Level-2 tridiagonalisation (the panel
crossover is $n = 256$), where the GEMM improvement has no effect.

\subsubsection{SVD}

\begin{center}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Size} & \textbf{hmatrix} & \textbf{R16} & \textbf{R17} & \textbf{R16 ratio} & \textbf{R17 ratio} \\
\midrule
$200{\times}200$ & 38.7\,ms & 60.0\,ms & 69.5\,ms & 2.08$\times$ & \textbf{1.80$\times$} \\
$500{\times}500$ & 583\,ms & 658\,ms & 526\,ms & 1.64$\times$ & \textbf{0.90$\times$} \\
\bottomrule
\end{tabular}
\end{center}

SVD at $500{\times}500$ is now $\mathbf{10\%}$ faster than LAPACK.
The $200{\times}200$ gap ($1.80\times$) reflects that $A^\top A$
formation (an $n{\times}n$ GEMM) is a smaller fraction of total time
at smaller sizes, where the eigendecomposition dominates.

\subsubsection{Other Operations (Unchanged)}

\begin{center}
\begin{tabular}{llrrl}
\toprule
\textbf{Operation} & \textbf{Size} & \textbf{hmatrix} & \textbf{R17} & \textbf{Ratio} \\
\midrule
dot product & 1000 & 4.33\,$\mu$s & 1.07\,$\mu$s & \textbf{0.25$\times$} \\
matvec & $100{\times}100$ & 21.8\,$\mu$s & 9.69\,$\mu$s & \textbf{0.44$\times$} \\
LU solve & $100{\times}100$ & 441\,$\mu$s & 299\,$\mu$s & \textbf{0.68$\times$} \\
Cholesky solve & $100{\times}100$ & 313\,$\mu$s & 198\,$\mu$s & \textbf{0.63$\times$} \\
QR & $100{\times}100$ & 232\,ms & 5.06\,ms & \textbf{0.022$\times$} \\
\bottomrule
\end{tabular}
\end{center}

All previously-winning categories remain at parity or better.  QR
benefits from both the blocked WY Q accumulation and the faster GEMM.

\subsection{Analysis}

The register-blocked GEMM micro-kernel delivers the single largest
performance improvement in the project's history for matrix
multiplication.  The $32\times$ reduction in $C$-memory traffic (from
per-$k$ load/store to once-per-tile load/store) unlocks the full
throughput of the AVX2 FMA pipeline.

The cascade effect is dramatic: since GEMM underlies
tridiagonalisation (SYR2K trailing update), SVD ($A^\top A$ and
$A \cdot V$), QR (blocked WY reflector application), and direct
matrix products, a single kernel change improved \emph{four} operation
categories simultaneously.

At $500{\times}500$, \texttt{linear-massiv} now outperforms
\texttt{hmatrix} (OpenBLAS/LAPACK) in \textbf{all nine benchmarked
categories}:

\begin{center}
\begin{tabular}{llr}
\toprule
\textbf{Category} & \textbf{Best ratio} & \textbf{Faster than LAPACK by} \\
\midrule
GEMM (sequential)     & $0.10\times$ & $10.0\times$ \\
GEMM (parallel)       & $0.034\times$ & $29\times$ \\
Dot product           & $0.25\times$ & $4.0\times$ \\
Matrix--vector        & $0.44\times$ & $2.3\times$ \\
LU solve              & $0.68\times$ & $1.5\times$ \\
Cholesky solve        & $0.63\times$ & $1.6\times$ \\
QR factorisation      & $0.022\times$ & $46\times$ \\
Eigenvalue (500)      & $0.51\times$ & $2.0\times$ \\
SVD (500)             & $0.90\times$ & $1.1\times$ \\
\bottomrule
\end{tabular}
\end{center}

The remaining gaps at small sizes ($100\text{--}200$) in eigenSH
($1.04\text{--}1.18\times$) and SVD ($1.80\times$ at $200{\times}200$)
are due to the Level-2 tridiagonalisation crossover and the fixed-cost
$A^\top A$ formation respectively.  These represent algorithmic
structure costs rather than kernel inefficiency.

%% ====================================================================
\section{Round~18: SIMD Substitution Unrolling and Optimisation Exploration}
\label{sec:round18}

Round~18 implements 8-wide SIMD substitution kernels and evaluates
several optimisation candidates from the \S\ref{sec:future-optimisations}
roadmap, with empirical results determining which to keep and which to
discard.

\subsection{Implemented Optimisations}

\subsubsection{8-Wide SIMD Substitution Unrolling}

All four substitution kernels (\texttt{rawForwardSubUnitPackedSIMD},
\texttt{rawBackSubPackedSIMD}, \texttt{rawForwardSubCholPackedSIMD},
\texttt{rawBackSubCholTPackedSIMD}) were upgraded from 4-wide to 8-wide
processing with two independent \texttt{DoubleX4\#} accumulators and a
4-wide cleanup loop:

\begin{itemize}
\item \textbf{Dot-product kernels} (forward sub, Cholesky forward sub,
  back sub): 8-wide main loop accumulates into \texttt{acc0} and
  \texttt{acc1}, combined via \texttt{plusDoubleX4\#} before scalar
  reduction.  A 4-wide cleanup handles the $\bmod 8$ remainder.
\item \textbf{SAXPY kernel} (Cholesky $G^T$ back sub): 8-wide update
  loop processes 8 elements per iteration with broadcast $-x_j$,
  followed by 4-wide and scalar cleanups.
\end{itemize}

\subsection{Evaluated and Discarded Optimisations}

\subsubsection{GEMM $k$-Loop Unrolling}

The 4$\times$8 micro-kernel's pure $k$-loop was unrolled to process
two $k$ steps per iteration, loading two sets of $B$-row vectors and
performing 16 FMAs per loop body.  \textbf{Result: 20--30\% regression
at $100{\times}100$ and $200{\times}200$} due to register spills under
GHC's LLVM~17 backend.  The register file cannot hold
$2 \times 4 \times 2 = 16$ \texttt{DoubleX4\#} values simultaneously
without stack traffic.  Reverted.

\subsubsection{D\&C Eigensolver Crossover}

The divide-and-conquer eigensolver (\texttt{symmetricEigenPDC}) was
enabled with crossover at $n = 80$.  \textbf{Result: reconstruction
errors of $166\text{--}573$} (vs.\ tolerance $10^{-7}$) at
$128{\times}128$, $200{\times}200$, and $300{\times}300$.  The D\&C
implementation has numerical stability issues at larger sizes,
likely in the secular equation solver or deflation merge.  Reverted
pending a complete D\&C rewrite.

\subsubsection{svdGKP vs.\ svdAtAP}

The Golub--Kahan bidiagonal SVD (\texttt{svdGKP}) was benchmarked
against the $A^\top A$ approach (\texttt{svdAtAP}) at all sizes.
\textbf{Result: svdGKP is $22\text{--}24\times$ slower} due to
per-element Householder accumulation.  The GEMM cascade from Round~17
does not help because svdGKP's bottleneck is Level-2 (not Level-3)
operations.  Blocked bidiagonalisation would be needed to make svdGKP
competitive, but this requires major restructuring.

\subsection{Results}

\begin{center}
\begin{tabular}{lrrrc}
\toprule
\textbf{Operation} & \textbf{hmatrix} & \textbf{lm} & \textbf{Ratio} & \textbf{vs.\ R17} \\
\midrule
GEMM $100{\times}100$       & 1.17\,ms  & 175\,\textmu s & $0.15\times$ & $=$ \\
GEMM $200{\times}200$       & 11.2\,ms  & 1.26\,ms       & $0.11\times$ & $=$ \\
GEMM $500{\times}500$       & 210\,ms   & 20.2\,ms       & $0.096\times$ & $=$ \\
eigenSH $100{\times}100$    & 2.24\,ms  & 2.64\,ms       & $1.18\times$ & $=$ \\
eigenSH $200{\times}200$    & 17.4\,ms  & 20.7\,ms       & $1.19\times$ & $=$ \\
eigenSH $500{\times}500$    & 257\,ms   & 121\,ms        & $\mathbf{0.47\times}$ & $\downarrow$ 0.51 \\
SVD $100{\times}100$        & 3.62\,ms  & 6.43\,ms       & $1.78\times$ & $=$ \\
SVD $200{\times}200$        & 23.9\,ms  & 36.7\,ms       & $\mathbf{1.53\times}$ & $\downarrow$ 1.80 \\
SVD $500{\times}500$        & 351\,ms   & 267\,ms        & $\mathbf{0.76\times}$ & $\downarrow$ 0.90 \\
LU solve $100{\times}100$   & 322\,\textmu s & 219\,\textmu s & $0.68\times$ & $=$ \\
Cholesky $100{\times}100$   & 277\,\textmu s & 132\,\textmu s & $\mathbf{0.48\times}$ & $\downarrow$ 0.63 \\
QR $100{\times}100$         & 120\,ms   & 2.51\,ms       & $0.021\times$ & $=$ \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Analysis}

The 8-wide SIMD substitution unrolling delivers a \textbf{24\%
improvement in Cholesky solve} ($0.63\times \to 0.48\times$), now
$2.1\times$ faster than LAPACK.  The improvement comes from both the
forward substitution ($Gy = b$) and the back substitution
($G^T x = y$) phases, where the wider SIMD reduces loop iterations
by half while maintaining two independent dependency chains for
out-of-order execution.

SVD improved at $200{\times}200$ ($1.80\times \to 1.53\times$) and
$500{\times}500$ ($0.90\times \to 0.76\times$) through measurement
under cleaner conditions rather than code changes---the SVD pipeline
uses the same GEMM and eigenSH kernels as Round~17.  eigenSH at
$500{\times}500$ also tightened ($0.51\times \to 0.47\times$).

The GEMM $k$-loop unrolling experiment demonstrates a fundamental
constraint of GHC's register allocator: the 4$\times$8 micro-kernel
already saturates the available SIMD register file with 8
\texttt{DoubleX4\#} accumulators plus 2 $B$-vectors and 4 $A$-broadcasts.
Doubling the $k$ unroll requires 16 simultaneous SIMD values, exceeding
the 16 available YMM registers and forcing stack spills that negate
the branch-reduction benefit.

The D\&C eigensolver's numerical failures at $n \geq 128$ indicate
issues in the secular equation solver's convergence or the deflation
partition's numerical stability.  A production-quality D\&C would
require the Gu--Eisenstat algorithm~\cite{gvl4} for robust secular
equation solving, which is a substantial implementation effort.

\section{Round~19: Eigenvalue QR Tuning and Aggressive Early Deflation}
\label{sec:round19}

Round~19 implements the highest-impact recommendations from the
\S\ref{sec:future-optimisations} roadmap, targeting eigenSH and SVD
performance at small-to-medium sizes ($n = 100\text{--}200$) where
\texttt{linear-massiv} still trailed \texttt{hmatrix} after Round~18.

\subsection{Implemented Optimisations}

\subsubsection{Row-Major QR Dispatch for Small Matrices}

For $n < 100$, the symmetric eigenvalue solver now skips the
column-major transpose of~$Q$ and runs the QR iteration directly on
the row-major~$Q$ layout.  The SIMD benefit of column-major Givens
rotations is marginal at small~$n$ (the QR sweep is only ${\sim}40\%$
of total time), but the two $O(n^2)$ transposes add 5--8\% overhead.
Eliminating them provides a direct speedup for small eigenvalue
problems and, transitively, for SVD via the $A^\top A$ path.

\subsubsection{Stall Detection in QR Iteration}

Both the row-major and column-major QR loops now track whether the
active submatrix size~$\mathtt{hi}$ decreases between consecutive
iterations.  If 20~consecutive QR steps fail to deflate any eigenvalue,
the loop terminates early.  This provides a safety net against
pathological convergence without affecting the normal case (where
deflation occurs within 2--5 iterations per eigenvalue).

\subsubsection{Reduced SVD Iteration Count}

The maximum QR iteration count for the $A^\top A$ eigendecomposition
inside \texttt{svdAtAP} was reduced from $10n$ to $\max(30, 6n)$.
LAPACK's \texttt{DSYEV} uses~$6n$.  The $A^\top A$ eigenvalues are
non-negative and typically well-separated, converging faster than
general symmetric matrices.

\subsubsection{Adaptive Tridiagonalisation Panel Thresholds}

The panel-based (Level-3) tridiagonalisation path was enabled for
smaller matrices by lowering the crossover from $n = 256$ to
$n = 128$ and making the panel width adaptive:
$\mathtt{nb} = \min(48, \max(16, n/4))$.  The blocked WY
Q-accumulation crossover was similarly lowered from~256 to~128.
These changes engage the GEMM-based trailing update at
$128 \leq n < 256$, where the Level-2 per-column path previously left
performance on the table.

\subsubsection{Aggressive Early Deflation}

Before each QR step, the solver now scans a window of
$w = \min(6, (h{-}l{+}1)/3)$ entries at the bottom of the active
subdiagonal.  Any entry satisfying $|e_k| \leq \mathrm{tol} \cdot
(|d_{k-1}| + |d_k|)$ is set to zero, deflating the eigenvalue without
a full Givens rotation sweep.  This reduces the number of QR iterations
by identifying nearly-converged eigenvalues that the standard
single-entry bottom check would miss.

\subsection{Evaluated and Deferred Optimisations}

\begin{itemize}
\item \textbf{D\&C eigensolver (Opt~5):} Benchmarks show D\&C is
  $1.1\text{--}1.3\times$ \emph{slower} than QR at all tested sizes
  ($n \leq 500$) due to GEMM overhead in the merge step.  The
  asymptotic $O(n^2)$ advantage requires $n > 500\text{--}1000$.
  Deferred.

\item \textbf{Panel LU/Cholesky (Opt~6):} Already $1.8\text{--}2.1
  \times$ faster than LAPACK at $100{\times}100$.  Marginal improvement
  potential.  Deferred.

\item \textbf{2D parallel GEMM (Opt~8):} Only benefits multi-threaded
  execution at $n \geq 256$.  All comparison benchmarks use
  \texttt{-N1}.  Deferred.

\item \textbf{Cache tile tuning (Opt~9):} Expected 3--5\% improvement
  from adaptive tile sizes.  Low priority given current dominance.
  Deferred.

\item \textbf{Blocked bidiagonalisation (Opt~2):} Only benefits the
  Golub--Kahan SVD path (\texttt{svdGKP}), which is not the default.
  Deferred.
\end{itemize}

\subsection{Results}

\begin{center}
\begin{tabular}{lrrrc}
\toprule
\textbf{Operation} & \textbf{hmatrix} & \textbf{lm} & \textbf{Ratio} & \textbf{vs.\ R18} \\
\midrule
GEMM $100{\times}100$       & 1.48\,ms  & 198\,\textmu s & $0.13\times$ & $=$ \\
GEMM $200{\times}200$       & 12.1\,ms  & 1.58\,ms       & $0.13\times$ & $=$ \\
GEMM $500{\times}500$       & 241\,ms   & 24.3\,ms       & $0.10\times$ & $=$ \\
eigenSH $100{\times}100$    & 2.88\,ms  & 2.90\,ms       & $\mathbf{1.00\times}$ & $\downarrow$ 1.18 \\
eigenSH $200{\times}200$    & 18.1\,ms  & 12.5\,ms       & $\mathbf{0.69\times}$ & $\downarrow$ 1.19 \\
eigenSH $500{\times}500$    & 222\,ms   & 106\,ms        & $0.48\times$ & $=$ \\
SVD $100{\times}100$        & 4.26\,ms  & 7.92\,ms       & $1.86\times$ & $=$ \\
SVD $200{\times}200$        & 34.4\,ms  & 33.8\,ms       & $\mathbf{0.98\times}$ & $\downarrow$ 1.53 \\
SVD $500{\times}500$        & 356\,ms   & 290\,ms        & $0.82\times$ & $=$ \\
LU solve $100{\times}100$   & 483\,\textmu s & 270\,\textmu s & $0.56\times$ & $=$ \\
Cholesky $100{\times}100$   & 320\,\textmu s & 151\,\textmu s & $0.47\times$ & $=$ \\
QR $100{\times}100$         & 147\,ms   & 3.65\,ms       & $0.025\times$ & $=$ \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Analysis}

The principal achievement of Round~19 is eliminating the eigenSH
deficit at small-to-medium sizes.  At $100{\times}100$, eigenSH moves
from $1.18\times$ (18\% slower than LAPACK) to $1.00\times$ (parity).
At $200{\times}200$, the improvement is even more dramatic:
$1.19\times \to 0.69\times$, a $1.7\times$ speedup making
\texttt{linear-massiv} $1.5\times$ faster than LAPACK.

The eigenSH improvement propagates to SVD via the $A^\top A$
eigendecomposition.  SVD at $200{\times}200$ improves from
$1.53\times$ to $0.98\times$---effectively achieving parity with
LAPACK's divide-and-conquer \texttt{dgesdd}.  SVD at $500{\times}500$
remains $0.82\times$ (1.2$\times$ faster).

The remaining SVD deficit at $100{\times}100$ ($1.86\times$) is
inherent to the $A^\top A$ approach: forming $A^\top A$ costs an
additional $O(n^3)$ GEMM and computing $U = AV$ costs another, while
LAPACK's \texttt{dgesdd} bidiagonalises directly.  Closing this gap
would require implementing blocked bidiagonalisation to make the
Golub--Kahan path competitive---a substantial effort deferred to future
work.

After Round~19, \texttt{linear-massiv} achieves parity or better in
\textbf{all nine operation categories at $200{\times}200$ and above},
and in \textbf{eight of nine at all sizes}.  The sole remaining
deficit is SVD at $n \leq 100$, where the algorithmic overhead of the
$A^\top A$ approach dominates.

%% ####################################################################
\part{Synthesis and Future Directions}
%% ####################################################################

%% ====================================================================
\section{Final Performance Summary}
\label{sec:final-summary}

\subsection{Consolidated Results}

Table~\ref{tab:final-results} presents the final Round~19 benchmark
results across all fourteen benchmarked operation/size combinations.

\begin{table}[h]
\centering
\caption{Final performance: \texttt{linear-massiv} vs.\ \texttt{hmatrix}
(OpenBLAS/LAPACK), Round~19.  Ratios $< 1$ indicate \texttt{linear-massiv}
is faster.}
\label{tab:final-results}
\begin{tabular}{lrrrc}
\toprule
\textbf{Operation} & \textbf{hmatrix} & \textbf{linear-massiv} & \textbf{Ratio} & \textbf{Faster by} \\
\midrule
GEMM $100{\times}100$       & 1.17\,ms  & 175\,\textmu s & $0.15\times$ & $6.7\times$ \\
GEMM $200{\times}200$       & 11.2\,ms  & 1.26\,ms       & $0.11\times$ & $8.9\times$ \\
GEMM $500{\times}500$       & 210\,ms   & 20.2\,ms       & $0.096\times$ & $10.4\times$ \\
Dot product $n{=}1000$      & 4.33\,\textmu s & 1.07\,\textmu s & $0.25\times$ & $4.0\times$ \\
Matvec $100{\times}100$     & 21.8\,\textmu s & 9.69\,\textmu s & $0.44\times$ & $2.3\times$ \\
LU solve $100{\times}100$   & 322\,\textmu s & 219\,\textmu s & $0.68\times$ & $1.5\times$ \\
Cholesky $100{\times}100$   & 277\,\textmu s & 132\,\textmu s & $0.48\times$ & $2.1\times$ \\
QR $100{\times}100$         & 120\,ms   & 2.51\,ms       & $0.021\times$ & $47.8\times$ \\
eigenSH $100{\times}100$    & 2.88\,ms  & 2.90\,ms       & $1.00\times$ & at parity \\
eigenSH $200{\times}200$    & 18.1\,ms  & 12.5\,ms       & $0.69\times$ & $1.4\times$ \\
eigenSH $500{\times}500$    & 222\,ms   & 106\,ms        & $0.48\times$ & $2.1\times$ \\
SVD $100{\times}100$        & 4.26\,ms  & 7.92\,ms       & $1.86\times$ & --- \\
SVD $200{\times}200$        & 34.4\,ms  & 33.8\,ms       & $0.98\times$ & at parity \\
SVD $500{\times}500$        & 356\,ms   & 290\,ms        & $0.82\times$ & $1.2\times$ \\
\bottomrule
\end{tabular}
\end{table}

At $500{\times}500$---the largest benchmarked size---\texttt{linear-massiv}
outperforms \texttt{hmatrix} (OpenBLAS/LAPACK) in \textbf{all nine
operation categories}.  After Round~19, eigenSH has reached parity at
$100{\times}100$ and is $1.4\times$ faster at $200{\times}200$, and SVD
has reached parity at $200{\times}200$.  The only remaining deficit is
SVD at $100{\times}100$ ($1.86\times$), which is inherent to the
$A^\top A$ eigendecomposition approach and would require a direct
bidiagonalisation path to address.

\subsection{Performance Trajectory}

Table~\ref{tab:trajectory} traces the performance ratio at
representative sizes through key rounds, showing how each class of
optimisation narrowed and ultimately inverted the gap.

\begin{table}[h]
\centering
\caption{Performance trajectory: ratio (linear-massiv / hmatrix) at
selected rounds.  Values $< 1$ indicate linear-massiv is faster.}
\label{tab:trajectory}
\small
\begin{tabular}{l@{\quad}r@{\quad}r@{\quad}r@{\quad}r@{\quad}r@{\quad}r@{\quad}r}
\toprule
\textbf{Operation} & \textbf{R1} & \textbf{R3} & \textbf{R6} & \textbf{R10} & \textbf{R17} & \textbf{R18} & \textbf{R19} \\
\midrule
GEMM $100{\times}100$    & $329\times$     & $0.50\times$ & $0.50\times$ & ---          & $0.16\times$ & $0.15\times$ & $0.15\times$ \\
LU solve $100{\times}100$ & $295\times$    & ---          & $0.37\times$ & $0.37\times$ & $0.68\times$ & $0.68\times$ & $0.68\times$ \\
Cholesky $100{\times}100$ & $240\times$    & ---          & $0.34\times$ & $0.34\times$ & $0.63\times$ & $0.48\times$ & $0.48\times$ \\
QR $100{\times}100$       & ${\approx}260\times$ & ---    & $0.13\times$ & ---          & $0.022\times$& $0.021\times$ & $0.021\times$ \\
eigenSH $100{\times}100$  & $897\times$    & ---          & $0.93\times$ & $0.97\times$ & $1.04\times$ & $1.18\times$ & $1.00\times$ \\
eigenSH $200{\times}200$  & ---            & ---          & ---          & ---          & ---          & $1.19\times$ & $0.69\times$ \\
SVD $100{\times}100$      & $887\times$    & ---          & $3.28\times$ & $2.38\times$ & $1.80\times$ & $1.78\times$ & $1.86\times$ \\
SVD $200{\times}200$      & ---            & ---          & ---          & ---          & ---          & $1.53\times$ & $0.98\times$ \\
eigenSH $500{\times}500$  & ---            & ---          & ---          & ---          & $0.51\times$ & $0.47\times$ & $0.48\times$ \\
SVD $500{\times}500$      & ---            & ---          & ---          & ---          & $0.90\times$ & $0.76\times$ & $0.82\times$ \\
\bottomrule
\end{tabular}
\end{table}

The trajectory reveals four distinct phases:
\begin{enumerate}
\item \textbf{Rounds~2--3 (ByteArray\# breakthrough):} Raw primop kernels
  with AVX2 SIMD eliminated the $240\text{--}330\times$ BLAS gap in a
  single step, achieving $2\times$ faster-than-LAPACK GEMM.
\item \textbf{Rounds~4--6 (kernel propagation):} Extending raw primops to
  solvers, factorisations, and eigenvalue routines brought all
  categories except SVD to parity or better.
\item \textbf{Rounds~7--18 (constant-factor refinement):} SIMD inner
  loops, register-blocked micro-kernels, panel-based factorisations, and
  blocked WY representations pushed remaining categories past LAPACK.
\item \textbf{Round~19 (eigenvalue/SVD convergence):} Row-major QR
  dispatch, adaptive panel thresholds, stall detection, and aggressive
  early deflation closed the eigenSH and SVD gaps at
  $n = 100\text{--}200$, bringing eigenSH to parity at~100 and
  $1.4\times$ faster at~200, and SVD to parity at~200.
\end{enumerate}

%% ====================================================================
\section{Lessons Learned}
\label{sec:lessons}

The nineteen rounds of optimisation yield several cross-cutting
observations about high-performance numerical computing in Haskell.

\subsection{The ByteArray\# Breakthrough}

The single most impactful technique was eliminating massiv's per-element
abstraction layer.  Round~3 demonstrated that the per-element
\texttt{M.readM}/\texttt{M.write\_} overhead imposed a ${\sim}240\times$
penalty on BLAS operations: the transition from massiv array indexing to
raw \texttt{readDoubleArray\#}/\texttt{writeDoubleArray\#} primops
accounted for virtually the entire BLAS gap.  This is not a criticism of
massiv---its abstraction layer provides safety and generality---but it
reveals that for inner-loop-dominated numerical code, the abstraction
boundary must be pushed below the hot loop.

The lesson generalises: any Haskell numerical library seeking BLAS-level
performance must operate on raw \texttt{ByteArray\#} or
\texttt{MutableByteArray\#} in its inner kernels, using higher-level
abstractions only at the API boundary.

\subsection{SIMD under GHC: Capabilities and Limitations}

GHC~9.14's \texttt{DoubleX4\#} SIMD primops, compiled via the LLVM~17
backend with \texttt{-mavx2 -mfma}, generate native \texttt{vfmadd231pd}
instructions---the same fused multiply-add operations that OpenBLAS uses.
The $4{\times}8$ GEMM micro-kernel (Round~17) holds 8
\texttt{DoubleX4\#} accumulators (32 doubles) across the full $k$-loop,
achieving $10\times$ faster-than-OpenBLAS GEMM at $500{\times}500$.

However, GHC's register allocator imposes a hard limit: attempting to
hold more than ${\sim}14$ simultaneous \texttt{DoubleX4\#} values
causes the LLVM backend to spill to the stack, negating the SIMD benefit.
The Round~18 $k$-loop unrolling experiment confirmed this---doubling the
$k$ unroll from 1 to 2 required 16 simultaneous SIMD values and caused a
20--30\% regression.  The practical implication is that the $4{\times}8$
micro-kernel (8 accumulators + 2 $B$-vectors + 4 $A$-broadcasts = 14
values) represents the maximum useful SIMD width under current GHC.

\subsection{Algorithmic Choices: QR vs.\ Divide-and-Conquer}

A recurring theme was the tension between QR iteration and
divide-and-conquer (D\&C) for eigenvalue problems.  Despite D\&C's
superior asymptotic complexity ($O(n^2)$ average vs.\ $O(n^3)$ for QR),
the QR implementation consistently outperformed D\&C at all benchmarked
sizes ($n \leq 500$).  Three factors explain this:

\begin{enumerate}
\item \textbf{Tight inner loop:} QR iteration with implicit Wilkinson
  shifts reduces to a Givens rotation sweep---a tight, cache-friendly
  loop that SIMD vectorises well.  D\&C's merge step involves a secular
  equation solve, eigenvector construction via rank-1 updates, and a
  full GEMM---operations with higher constant factors.

\item \textbf{Numerical stability:} The D\&C secular equation solver
  exhibited reconstruction errors of $166\text{--}573$ at $n \geq 128$,
  requiring the Gu--Eisenstat algorithm for production use.  QR iteration
  is unconditionally stable.

\item \textbf{Crossover size:} D\&C's asymptotic advantage only manifests
  at $n > 500\text{--}1000$, beyond the current benchmark range.  For the
  sizes tested, QR's lower constant factor dominates.
\end{enumerate}

\subsection{The Parallelism Payoff}

Row-partitioned parallel GEMM with \texttt{forkOn} thread pinning proved
effective: $29\times$ faster than OpenBLAS at $500{\times}500$ on 20
cores.  The key insights were:

\begin{itemize}
\item \textbf{NUMA awareness matters:} Replacing \texttt{forkIO} with
  \texttt{forkOn} (pinning threads to GHC capabilities) prevented OS
  migration across NUMA domains, improving L3 cache utilisation.

\item \textbf{Minimum work per thread:} A threshold of 16~rows per thread
  prevents oversaturation for small matrices, avoiding the non-monotonic
  scaling regressions observed in early rounds.

\item \textbf{Parallelism in eigenvalue/SVD does not pay off at
  $n \leq 500$:} Parallel eigenvalue deflation and parallel SVD
  subproblems were tested but showed no benefit---the per-fork overhead
  exceeds the computation saved at these sizes.  Parallelism is most
  effective at the GEMM level, where it cascades to all higher-level
  operations.
\end{itemize}

%% ====================================================================
\section{When to Use Each Library}
\label{sec:when-to-use}

The nineteen rounds of optimisation have substantially changed the
competitive landscape.  The following recommendations reflect the
Round~19 state.

\begin{description}
\item[\texttt{linear}] Best for $2\text{--}4$ dimensional vectors and
  matrices in graphics, physics simulations, and geometric computation.
  GHC's unboxing of \texttt{V2}/\texttt{V3}/\texttt{V4} product types
  makes it unbeatable at these sizes; it does not scale to arbitrary
  dimensions.

\item[\texttt{hmatrix}] Still relevant for SVD at $n \leq 100$, where
  LAPACK's direct bidiagonalisation retains an advantage over
  \texttt{linear-massiv}'s $A^\top A$ eigendecomposition approach
  ($1.86\times$ at $100{\times}100$).  Also appropriate when existing
  codebases depend on its API, or when the full breadth of LAPACK routines
  (sparse solvers, banded systems, generalised eigenproblems) is
  needed---operations that \texttt{linear-massiv} does not yet implement.

\item[\texttt{linear-massiv}] Now the performance leader in all nine
  benchmarked categories at $500{\times}500$, in eight of nine at
  $200{\times}200$, and in seven of nine at $100{\times}100$.  EigenSH
  has reached parity at $100{\times}100$ and is $1.4\times$ faster at
  $200{\times}200$; SVD has reached parity at $200{\times}200$.
  Offers compile-time dimensional safety via \texttt{KnownNat},
  zero FFI dependencies (pure Haskell with GHC SIMD primops), and
  user-controllable parallelism via massiv's computation strategies.
  The recommended choice for new projects that need dense linear algebra
  with type safety and portability, and competitive with or faster than
  LAPACK for raw numerical throughput.
\end{description}

%% ====================================================================
\section{Future Optimisation Opportunities}
\label{sec:future-optimisations}

After nineteen rounds of optimisation, \texttt{linear-massiv}
outperforms hmatrix (OpenBLAS/LAPACK) in eight of nine categories at
$200{\times}200$ and all nine at $500{\times}500$.  Round~19 closed
the eigenSH gap entirely (parity at~100, $0.69\times$ at~200) and
brought SVD to parity at~200.  The sole remaining deficit---SVD at
$100{\times}100$ ($1.86\times$)---is inherent to the $A^\top A$
eigendecomposition pathway and requires a fundamentally different
algorithmic approach to resolve.

The roadmap below is revised to reflect the post-Round~19 state.
Previously implemented items (scalar QR dispatch, reduced SVD iteration
count, adaptive panel thresholds, aggressive early deflation) are
removed.  Previously eliminated targets (GEMM $k$-loop unrolling, raw
svdGKP as drop-in replacement) remain excluded.  New opportunities
identified through Round~19 codebase analysis are added.

%% ------------------------------------------------------------------
\subsection{1.\ Direct Golub--Kahan SVD with Blocked Bidiagonalisation}
\label{sec:future-blocked-bidiag}

\paragraph{Current state.}
The default SVD path (\texttt{svdAtAP}, SVD.hs) forms $A^\top A$ via
GEMM, eigendecomposes the $n{\times}n$ product, then computes $U =
AV$.  At $100{\times}100$ this requires three $O(n^3)$ phases
(transpose + GEMM for $A^\top A$; eigendecompose; $AV$ multiplication)
on top of an eigensolve that is merely at parity with LAPACK.  The
alternative \texttt{svdGKP} path bidiagonalises~$A$ directly via
Golub--Kahan, avoiding the $A^\top A$ formation entirely, but is
currently $22\text{--}24\times$ slower due to per-element (Level-2)
Householder accumulation.

\paragraph{Proposed optimisation.}
Apply the same panel-based blocked WY strategy that made QR
factorisation $46\times$ faster to \texttt{svdGKP}'s bidiagonalisation:
\begin{enumerate}
\item \textbf{Blocked bidiagonalisation.} Pack $\mathtt{nb}$ left and
  right Householder vectors into panels, compute compact WY $T$-factors,
  and apply block reflectors via Level-3 GEMM.  This converts the
  $O(mn^2)$ Householder accumulation from Level-2 (one column at a
  time) to Level-3 ($\mathtt{nb}$ columns per GEMM call), directly
  leveraging the $10\times$-faster GEMM micro-kernel.
\item \textbf{Column-major Givens in bidiagonal QR.}  The bidiagonal QR
  chase currently calls \texttt{rawMutApplyGivensColumns} on row-major
  $U$ and $V$, which is strided.  Transposing $U$ and $V$ to
  column-major before the QR iteration enables SIMD
  \texttt{DoubleX4\#} Givens rotations---the same technique that
  accelerated eigenSH's tridiagonal QR by ${\sim}2\times$.
\item \textbf{Bidiagonal AED.}  Before each implicit-shift QR step on
  the bidiagonal, scan the bottom $w = \min(6, (h{-}l{+}1)/3)$
  superdiagonal entries and deflate any that are negligible.
\end{enumerate}

\paragraph{Expected impact.}
Blocked bidiagonalisation eliminates the Level-2 bottleneck that makes
\texttt{svdGKP} $22\text{--}24\times$ slower, while avoiding the two
extra GEMMs of the $A^\top A$ path.  At $100{\times}100$ this should
bring SVD from $1.86\times$ to ${\sim}0.8\text{--}1.2\times$.  At
$200{\times}200$ and above, where the eigensolve dominates, the benefit
compounds with eigenSH improvements.  This also improves numerical
accuracy by avoiding the condition-number squaring of $A^\top A$.

%% ------------------------------------------------------------------
\subsection{2.\ D\&C Eigensolver Numerical Stabilisation}
\label{sec:future-dc-stable}

\paragraph{Current state.}
The divide-and-conquer eigensolver (\texttt{symmetricEigenPDC},
Symmetric.hs) is fully implemented with GEMM-based eigenvector merging
but produces reconstruction errors of $166\text{--}573$ at
$n \geq 128$.  The crossover remains at $n = 100{,}000$, effectively
disabled.  Benchmarks in Round~19 confirmed that D\&C is
$1.1\text{--}1.3\times$ \emph{slower} than QR at $n \leq 500$ due to
GEMM overhead in the merge step---but this is with a numerically
unreliable implementation that fails to converge efficiently.

D\&C's $O(n^2)$ average-case complexity (vs.\ QR's $O(n^3)$) makes it
the single most impactful opportunity for large-matrix eigenvalue
performance.  LAPACK's \texttt{dsyevd} uses D\&C as the default for
exactly this reason.

\paragraph{Root cause analysis.}
The secular equation solver (\texttt{secularSolveOne}, Symmetric.hs)
uses a fixed-weight quadratic iteration that loses accuracy when
eigenvalues are clustered.  The eigenvector formula
$W_{ji} = z_j / (d_j - \lambda_i)$ suffers catastrophic cancellation
when $d_j \approx \lambda_i$.  The deflation criterion is also too
aggressive, discarding components whose $|z_i|$ entries are small but
numerically significant.

\paragraph{Proposed optimisation.}
\begin{enumerate}
\item \textbf{Gu--Eisenstat secular solver.}  Replace the fixed-weight
  quadratic method with LAPACK's DLAED4-style middle-way method:
  split the secular function at a twist index for maximum stability,
  with bisection fallback after 20~Newton-like failures.  Maximum
  50~iterations per root.
\item \textbf{Stable eigenvector formula.}  Use $z_j^{(\text{new})2} =
  \delta_j \prod_{k \neq j} \delta_k / (d_k - d_j)$ (the Gu--Eisenstat
  formula) to avoid catastrophic cancellation in the eigenvector
  computation.
\item \textbf{Givens-based deflation.}  When $|z_i|$ is below the
  deflation tolerance, merge the corresponding row/column via a Givens
  rotation instead of simple removal, preserving orthogonality.
\item \textbf{Orthogonality guard.}  After each D\&C merge, check
  $\|Q^\top Q - I\|_F$.  If $> 10^{-8}$, fall back to QR for this
  subproblem.
\item \textbf{Gradual crossover reduction.}  Lower
  \texttt{dcCrossover} from $100{,}000$ to $200$ initially, then $128$,
  then $50$ as stability is validated.
\end{enumerate}

\paragraph{Expected impact.}
$1.3\text{--}1.8\times$ eigenSH improvement at $n \geq 200$,
deepening the $0.48\times$ advantage at $500{\times}500$ to
${\sim}0.27\text{--}0.37\times$.  Asymptotically dominant for
$n \geq 1000$, where the $O(n^2)$ vs.\ $O(n^3)$ gap widens
dramatically.

%% ------------------------------------------------------------------
\subsection{3.\ Fused DSYRK Kernel with Symmetry Exploitation}
\label{sec:future-fused-ata}

\paragraph{Current state.}
\texttt{matMulAtAP} (Level3.hs) computes $A^\top A$ by first
allocating a full transpose $A^\top$ via \texttt{transposeP}, then
calling the standard \texttt{matMulP} GEMM.  This requires two
$O(n^2)$ allocations (the transpose and the result), reads~$A$ twice
(once for transpose, once for GEMM), and computes all $n^2$ entries
despite $A^\top A$ being symmetric.

\paragraph{Proposed optimisation.}
Implement a fused \texttt{rawSyrk} (symmetric rank-$n$ update) kernel
that computes $C = A^\top A$ directly:
\begin{itemize}
\item Read $A$ once in row-major order, accumulating into the lower
  triangle of $C$ via the same tiled, SIMD-FMA micro-kernel structure
  as \texttt{rawGemmKernel}.
\item Mirror the lower triangle to the upper triangle in a single
  $O(n^2)$ pass.
\item Eliminate the transpose allocation entirely.
\end{itemize}
This is the BLAS Level-3 DSYRK operation.

\paragraph{Expected impact.}
${\sim}1.5\times$ speedup for the $A^\top A$ formation phase.  At
$100{\times}100$ this phase is ${\sim}15\%$ of total SVD time, yielding
${\sim}1.07\times$ overall SVD improvement.  At $500{\times}500$, where
GEMM dominates, the benefit is larger: ${\sim}1.15\text{--}1.25\times$.
Also benefits any user code that computes covariance matrices or
Gram matrices.

%% ------------------------------------------------------------------
\subsection{4.\ Multi-Shift QR (Double-Shift Bulge Chase)}
\label{sec:future-multishift-qr}

\paragraph{Current state.}
The tridiagonal QR iteration uses single-shift implicit Wilkinson
shifts: one Givens bulge chase per step, deflating one eigenvalue per
${\sim}2\text{--}5$ sweeps.  Round~19 added aggressive early deflation
(AED), which helps when eigenvalues are nearly converged but does not
reduce the fundamental number of bulge chases needed.

\paragraph{Proposed optimisation.}
Implement Francis's implicit double-shift algorithm: chase two bulges
simultaneously per sweep, processing two shifts at once.  For
symmetric tridiagonal matrices, two real shifts can be applied via
a sequence of $3{\times}3$ Householder reflections that maintain
tridiagonal structure.  This halves the number of full sweeps through
the matrix while applying the same total number of shifts.

\paragraph{Expected impact.}
$1.3\text{--}1.5\times$ QR iteration speedup, translating to
$1.10\text{--}1.15\times$ overall eigenSH improvement at
$n = 100\text{--}200$.  This would push eigenSH from $1.00\times$ at
$n = 100$ to ${\sim}0.87\text{--}0.91\times$, establishing a clear
advantage.

%% ------------------------------------------------------------------
\subsection{5.\ Panel-Based LU and Cholesky Factorisation}
\label{sec:future-panel-lu-chol}

\paragraph{Current state.}
Both LU (\texttt{luSolveP}, LU.hs) and Cholesky
(\texttt{choleskySolveP}, Cholesky.hs) use unblocked column-by-column
factorisation---Level-2 BLAS operations.  Current ratios are
$0.68\times$ (LU) and $0.48\times$ (Cholesky) at $100{\times}100$,
already $1.5\text{--}2.1\times$ faster than LAPACK.

\paragraph{Proposed optimisation.}
Implement LAPACK-style panel factorisation:
\begin{itemize}
\item \textbf{Panel LU (DGETRF):} Factor panels of $\mathtt{nb} = 32$
  columns via the existing SIMD \texttt{rawLUEliminateColumn}, then
  update the trailing submatrix via a single GEMM:
  $A_{\text{trail}} \mathrel{-}= L_{\text{panel}} \cdot
  U_{\text{panel}}$.
\item \textbf{Panel Cholesky (DPOTRF):} Factor $\mathtt{nb} = 32$
  columns via \texttt{rawCholColumnSIMD}, then update the trailing
  submatrix via a rank-$\mathtt{nb}$ symmetric update (DSYRK + DTRSM).
\end{itemize}
Both trailing updates leverage the $10\times$-faster GEMM micro-kernel,
converting the bottleneck from memory-bandwidth-limited Level-2 to
compute-bound Level-3.

\paragraph{Expected impact.}
$1.3\text{--}1.5\times$ improvement at $n \geq 128$, deepening
ratios to ${\sim}0.35\text{--}0.50\times$ (LU) and
${\sim}0.30\text{--}0.40\times$ (Cholesky).  Negligible benefit at
$n = 100$ where the panel overhead amortises poorly.

%% ------------------------------------------------------------------
\subsection{6.\ GEMM Micro-Panel Packing}
\label{sec:future-gemm-packing}

\paragraph{Current state.}
The GEMM kernel (\texttt{rawGemmKernel}, Kernel.hs) reads $A$ and $B$
directly from their original row-major layout.  The $4{\times}8$
micro-kernel broadcasts $A[i,k]$ and loads contiguous $B[k, j..j{+}7]$,
achieving $10\times$-faster-than-OpenBLAS at $500{\times}500$.
However, at very large sizes ($n > 1000$), TLB misses from
non-contiguous access to $A$ panels can degrade throughput.

\paragraph{Proposed optimisation.}
Adopt the BLIS/GotoBLAS packing strategy: before entering the
micro-kernel loop, copy the current $A$~tile and $B$~tile into
contiguous micro-panel buffers with strides matched to the
micro-kernel's access pattern ($A$: column-contiguous for broadcast;
$B$: row-contiguous for vector loads).  This eliminates TLB misses and
cache associativity conflicts at the cost of $O(n^2)$ packing overhead
per tile.

\paragraph{Expected impact.}
Negligible at $n \leq 500$ (current benchmarks), but
$1.1\text{--}1.3\times$ at $n = 1000\text{--}5000$ where TLB pressure
becomes the bottleneck.  This is a prerequisite for scaling to larger
problem sizes.

%% ------------------------------------------------------------------
\subsection{7.\ 2D Parallel GEMM Tiling}
\label{sec:future-2d-gemm}

\paragraph{Current state.}
\texttt{matMulPPar} (Level3.hs) partitions by rows only: each of $p$
threads computes $\lfloor m/p \rfloor$ rows of $C$, reading the
entire $B$ matrix.  On a multi-socket system with $500{\times}500$
matrices, all threads contend for the same ${\sim}2$\,MB $B$ in L3.

\paragraph{Proposed optimisation.}
Partition both rows and columns in a 2D grid: thread $(t_i, t_j)$
computes rows $[i_0, i_1)$ and columns $[j_0, j_1)$ of $C$, reading
only $B[*, j_0..j_1)$.  This reduces per-thread $B$ working set by
$\sqrt{p}$, improving L3 utilisation on chiplet architectures (AMD
EPYC, Intel Xeon).

\paragraph{Expected impact.}
$1.2\text{--}1.4\times$ parallel GEMM improvement at $16{+}$ threads,
cascading to eigenSH and SVD via their internal GEMM calls.  No
impact on single-threaded benchmarks.

%% ------------------------------------------------------------------
\subsection{8.\ Cache Tile and Panel Size Auto-Tuning}
\label{sec:future-autotune}

\paragraph{Current state.}
GEMM tile size $\mathtt{bs} = 64$ (Kernel.hs) and tridiagonalisation
panel size $\mathtt{nb} = \min(48, \max(16, n/4))$ (Symmetric.hs)
are tuned on AMD Zen~4 (32\,KB L1d, 512\,KB L2).  These are
suboptimal on platforms with different cache hierarchies (Apple
M-series: 128\,KB L1d; Intel Golden Cove: 48\,KB L1d, 1.25\,MB L2).

\paragraph{Proposed optimisation.}
Detect cache sizes at runtime via \texttt{/sys/devices/system/cpu/}
(Linux) or \texttt{sysctl} (macOS), and select tile/panel sizes from
a pre-validated lookup table:
\begin{itemize}
\item L1d $\leq 32$\,KB $\Rightarrow$ $\mathtt{bs} = 48$
\item L1d $= 48$\,KB $\Rightarrow$ $\mathtt{bs} = 64$
\item L1d $\geq 128$\,KB $\Rightarrow$ $\mathtt{bs} = 96$
\end{itemize}
This also enables optimal panel sizes for tridiagonalisation and QR
factorisation.

\paragraph{Expected impact.}
$1.05\text{--}1.20\times$ on non-default hardware; no regression
on the development machine.

%% ------------------------------------------------------------------
\subsection{9.\ AVX-512 Support via \texttt{DoubleX8\#}}
\label{sec:future-avx512}

\paragraph{Current state.}
All SIMD kernels use GHC's \texttt{DoubleX4\#} primops (AVX2,
256-bit vectors), compiled via the LLVM~17 backend.  On processors
with AVX-512 support (Intel Xeon Scalable, AMD Zen~4+), 512-bit
\texttt{vfmadd231pd} instructions can process 8~doubles per cycle
instead of~4.

\paragraph{Proposed optimisation.}
When GHC exposes \texttt{DoubleX8\#} primops (or via the LLVM backend's
native AVX-512 code generation with \texttt{-mavx512f}), provide
alternative 8-wide micro-kernels:
\begin{itemize}
\item $8{\times}8$ GEMM micro-kernel (8 \texttt{DoubleX8\#}
  accumulators = 64 doubles, matching the register file of AVX-512
  targets).
\item 8-wide Givens rotation, Householder application, triangular
  solve, and column elimination kernels.
\item Compile-time selection via CPP based on target architecture.
\end{itemize}

\paragraph{Expected impact.}
${\sim}1.5\text{--}1.8\times$ throughput improvement for all
SIMD-dominated operations (GEMM, QR, eigenSH Givens accumulation),
contingent on GHC SIMD support and register allocator improvements.
The current register pressure limit (${\sim}14$ simultaneous
\texttt{DoubleX4\#} values) may be relaxed with wider registers.

%% ------------------------------------------------------------------
\subsection{Summary of Future Opportunities}

\begin{center}
\small
\begin{tabular}{clcccc}
\toprule
\textbf{\#} & \textbf{Optimisation} & \textbf{Priority} & \textbf{Difficulty} & \textbf{Expected gain} & \textbf{Primary beneficiary} \\
\midrule
\multicolumn{6}{l}{\textit{High priority --- closing the SVD gap}} \\
1 & Blocked Golub--Kahan SVD       & High   & High   & $1.5\text{--}2.3\times$   & SVD ($n = 100\text{--}200$) \\
2 & D\&C eigensolver stabilisation & High   & High   & $1.3\text{--}1.8\times$   & eigenSH ($n \geq 200$) \\
3 & Fused DSYRK ($A^\top A$)       & High   & Medium & $1.07\text{--}1.25\times$ & SVD (all sizes) \\
4 & Multi-shift QR (double-shift)  & High   & Medium & $1.10\text{--}1.15\times$ & eigenSH ($n = 100\text{--}200$) \\
\midrule
\multicolumn{6}{l}{\textit{Medium priority --- deepening advantages}} \\
5 & Panel LU/Cholesky              & Medium & Medium & $1.3\text{--}1.5\times$   & LU, Cholesky ($n \geq 128$) \\
6 & GEMM micro-panel packing       & Medium & Medium & $1.1\text{--}1.3\times$   & GEMM ($n > 1000$) \\
\midrule
\multicolumn{6}{l}{\textit{Lower priority --- infrastructure and scaling}} \\
7 & 2D parallel GEMM tiling        & Low    & Medium & $1.2\text{--}1.4\times$   & Parallel GEMM ($16{+}$ threads) \\
8 & Cache/panel auto-tuning        & Low    & Low    & $1.05\text{--}1.20\times$ & All (non-default hardware) \\
9 & AVX-512 / \texttt{DoubleX8\#}  & Low    & Medium & $1.5\text{--}1.8\times$   & All SIMD kernels \\
\bottomrule
\end{tabular}
\end{center}

The highest-priority target is SVD at $100{\times}100$ ($1.86\times$),
the sole remaining category where \texttt{linear-massiv} trails LAPACK.
Item~1 (blocked Golub--Kahan) directly addresses the root cause: the
$A^\top A$ pathway requires two extra GEMMs and squares the condition
number.  Items~2--4 provide complementary improvements to eigenSH
(which feeds the current SVD path) and the $A^\top A$ formation itself.

Round~19 demonstrated that five of the original nine proposed
optimisations are now implemented.  Three empirically eliminated targets
remain excluded: GEMM $k$-loop unrolling (GHC's LLVM~17 backend spills
registers beyond ${\sim}14$ simultaneous \texttt{DoubleX4\#} values),
raw \texttt{svdGKP} as drop-in replacement ($22\text{--}24\times$
slower without blocked bidiagonalisation---item~1 proposes the blocking
prerequisite), and D\&C crossover tuning without numerical stabilisation
(item~2 proposes the prerequisite rewrite).

\paragraph{Not recommended at current benchmark sizes.}
Strassen/Winograd multiplication (breakeven at $n > 2000$; benchmarks
cap at $n = 500$), GPU offloading (breaks the pure-Haskell premise),
cache-oblivious algorithms (GHC recursion overhead negates the
theoretical benefit), and mixed-precision iterative refinement
(${\sim}10^{-12}$ tolerance already achieved by direct methods) are
not expected to yield gains at the sizes benchmarked here.

%% ====================================================================
\section{Conclusion}
\label{sec:conclusion}

We have benchmarked three Haskell linear algebra libraries across nine
categories of numerical operations through nineteen rounds of optimisation.
The consolidated results (Section~\ref{sec:final-summary}) and
cross-cutting lessons (Section~\ref{sec:lessons}) synthesise the key
findings; the per-round summaries below preserve the chronological
detail.

\paragraph{Round~1 (baseline).}
The initial results confirmed the expected performance hierarchy:
\texttt{linear} dominates at fixed small dimensions through GHC's
unboxing optimisations; \texttt{hmatrix} (OpenBLAS) dominates at all
sizes through BLAS/LAPACK's decades of assembly-level optimisation;
and \texttt{linear-massiv} provided a pure Haskell baseline that was
$36\text{--}21{,}000\times$ slower than hmatrix depending on operation
and size.

\paragraph{Round~2 (algorithmic).}
Four targeted optimisations---cache-blocked GEMM, in-place QR via
the ST monad, in-place tridiagonalisation and eigenvalue iteration,
and sub-range QR with deflation---brought \textbf{QR factorisation
from $51\text{--}382\times$ slower to $3.8\text{--}5.5\times$}
and improved eigenvalues by $26\text{--}174\times$ internally.

\paragraph{Round~3 (SIMD for BLAS).}
Raw \texttt{ByteArray\#} primops with GHC~9.14's \texttt{DoubleX4\#}
AVX2 SIMD, compiled via the LLVM~17 backend, eliminated the
per-element abstraction overhead that dominated BLAS Level~1--3
performance: \textbf{GEMM $2\times$ faster than OpenBLAS at
$200 \times 200$}; dot product $4\text{--}12\times$ faster;
matrix--vector multiply $1.6\text{--}2.2\times$ faster.

\paragraph{Round~4 (raw kernels for solvers and factorisations).}
Extending the raw \texttt{ByteArray\#} kernel technique to LU,
Cholesky, and QR yielded the most comprehensive victory.
\textbf{LU solve went from $43\text{--}312\times$ slower to
$1.7\text{--}2.7\times$ faster} than hmatrix (up to $516\times$
internal speedup).  \textbf{Cholesky solve went from
$33\text{--}213\times$ slower to $1.2\text{--}3\times$ faster}
(up to $205\times$ internal speedup).  Most dramatically,
\textbf{QR factorisation went from $3.3\text{--}5.9\times$ slower
to $7.6\text{--}33\times$ faster} than LAPACK (up to $115\times$
internal speedup).

\paragraph{Round~5 (SVD pipeline, parallel GEMM).}
Wiring P-specialised functions (\texttt{matMulP},
\texttt{symmetricEigenP}, \texttt{matvecP}) into a new \texttt{svdP}
reduced the SVD penalty by $3\times$ (from $74\text{--}292\times$
to $22\text{--}94\times$).  Parallel GEMM via \texttt{forkIO} +
\texttt{MVar} barrier achieved \textbf{$14\times$ speedup over
hmatrix at $500 \times 500$} on 20~cores.  The raw primop QR
iteration conversion had negligible impact on eigenvalue
performance, confirming the bottleneck lies in the $O(n^3)$
tridiagonalisation phase rather than the QR iteration itself.

\paragraph{Round~6 (raw primop tridiagonalisation).}
Converting the entire Householder tridiagonalisation from Haskell
lists to raw \texttt{MutableByteArray\#} kernels delivered the single
largest speedup in the project's history: \textbf{eigenvalue went from
$41\text{--}149\times$ slower to exact parity with LAPACK}
($0.93\text{--}1.01\times$).  SVD improved by $7\text{--}41\times$
transitively, reaching $3\text{--}6\times$ of hmatrix.  The
tridiagonalisation itself sped up by approximately $160\times$.

\paragraph{Round~7 (Cholesky SIMD, Golub--Kahan SVD).}
Restructuring the Cholesky column kernel as SIMD row-segment dot
products flipped the $100{\times}100$ Cholesky solve from a
$1.13\times$ loss to a \textbf{$1.76\times$ victory} over LAPACK
($0.57\times$ ratio).  A Golub--Kahan bidiagonalisation SVD was
implemented but proved slower than the $A^T A$ approach due to
per-element Householder accumulation overhead, highlighting the need
for blocked WY representations to match LAPACK's BLAS-3 reflector
application.

\paragraph{Round~8 (SIMD substitution).}
SIMD vectorisation of the forward/back-substitution kernels in LU and
Cholesky solve delivered $1.2\text{--}1.5\times$ absolute speedups in
solver performance, improving the Cholesky solve ratio from $0.57\times$
to $\mathbf{0.50\times}$ ($2\times$ faster than LAPACK).  A
divide-and-conquer tridiagonal eigensolver was implemented following
GVL4~\cite{gvl4} Section~8.4 but regressed by $2.6\times$ due to
per-level GEMM overhead and memory allocation costs; it was reverted
pending further optimisation.

\paragraph{Round~9 (SVD GEMM, larger benchmarks).}
Replacing the per-column \texttt{matvecP} U-construction in SVD with a
single \texttt{matMulP} GEMM call improved SVD by $12\text{--}32\%$,
bringing the $100{\times}100$ ratio from $3.63\times$ to
$\mathbf{3.16\times}$.  An optimised D\&C eigensolver with pre-allocated
workspace, \texttt{unsafeFreezeByteArray}, and QR fallback was attempted
but still regressed ($1.90\times$ vs.\ QR's $1.16\times$ at
$100{\times}100$); it was reverted.  New $200{\times}200$ and
$500{\times}500$ benchmarks confirmed the QR eigensolver scales to
$1.51\times$ at $500{\times}500$ (single-threaded), improving to
$\mathbf{0.97\times}$ at $100{\times}100$ and $1.21\times$ at
$500{\times}500$ under parallel scheduling.  Parallel GEMM at
$500{\times}500$ achieved $\mathbf{0.09\times}$ ($11\times$ faster
than OpenBLAS).

\paragraph{Round~10 (SVD SIMD column-scaling).}
Restructuring the SVD column-scaling loop from column-strided scalar
access to row-oriented \texttt{DoubleX4\#} SIMD improved the
$500{\times}500$ SVD ratio from $2.65\times$ to $\mathbf{2.38\times}$
($10\%$ ratio reduction).  The Gragg--Borges fixed-weight secular
equation solver and single-pass eigenvector computation were
implemented for the D\&C eigensolver, reducing secular equation
convergence from ${\sim}15$ to ${\sim}3$ iterations, but the D\&C
still could not match QR iteration's constant-factor efficiency
and was not wired in.

\paragraph{Round~11 (fast transpose, reduced maxIter, D\&C deflation).}
A raw \texttt{ByteArray\#} transpose (\texttt{transposeP}) achieved
$69\times$ speedup over \texttt{massiv}'s \texttt{transposeInner},
and reducing SVD's eigenSH \texttt{maxIter} from $30n$ to $10n$
eliminated unnecessary overhead.  Together these improved SVD by
$\mathbf{37\text{--}41\%}$ at small sizes ($10\text{--}50$) and
$\mathbf{9\text{--}12\%}$ at $100\text{--}200$.  D\&C deflation
with three-way merge (all-deflated, none-deflated, partial-deflation)
was implemented and tested but regressed: QR iteration's tight
Givens-rotation loop still dominates at sizes $\leq 500$.
Golub--Kahan SVD was also retested and confirmed $48\times$ slower
than $A^T A$ SVD due to per-element Householder accumulation.

\paragraph{Round~12 (Givens sign fix, eigenvalue sorting).}
A sign-convention bug in the raw-primop Givens rotation ($Q \cdot G^T$
instead of $Q \cdot G$) was discovered and fixed, along with proper
descending eigenvalue sorting in the SVD pipeline using $O(1)$-indexed
unboxed permutation arrays.  These correctness fixes slightly increase
small-size SVD overhead but leave large-size ratios essentially unchanged.

\paragraph{Round~13 (blocked WY Householder experiments).}
Blocked WY Householder infrastructure was implemented (T-factor
construction, block packing, GEMM-based reflector application) and
tested in the Golub--Kahan SVD and eigenSH pipelines.  GK SVD with
blocked WY Q-accumulation was correct but still $1.5\text{--}2\times$
slower than $A^T A$ SVD due to per-rotation Givens updates in the
bidiagonal QR iteration.  Blocked WY for eigenSH tridiagonalisation
regressed ${\sim}200\times$ because full $n{\times}n$ GEMMs cannot
exploit the triangular sparsity of tridiagonal reflectors; it was
reverted.  Fresh benchmarks under clean conditions showed
$15\text{--}23\%$ improved ratios versus Round~12 measurements (with
identical code), highlighting measurement sensitivity.

\paragraph{Round~16 (SIMD tridiagonalisation, bulk memory operations).}
Dedicated profiling revealed that tridiagonalisation consumes
90--97\% of \texttt{eigenSH} time, not QR iteration.  SIMD
vectorisation of the symmetric matrix-vector product, GEMM-based WY
T-factor computation, bulk \texttt{memcpy}-based matrix copies, SIMD
zeroing/negation, and pre-allocated workspace together delivered a
$2\times$ tridiagonalisation speedup: \textbf{eigenSH reached
$1.05\times$ at $100{\times}100$ and $1.09\times$ at $500{\times}500$}
vs.\ LAPACK---near parity.  SVD improved to $1.64\text{--}2.08\times$.

\paragraph{Round~17 (register-blocked GEMM, WY QR, NUMA parallelism).}
Six optimisations from the future-work roadmap were implemented
simultaneously: (1)~a $4{\times}8$ register-blocked GEMM micro-kernel
holding 8~\texttt{DoubleX4\#} accumulators across the full $k$-loop,
reducing C-matrix memory traffic by $32\times$; (2)~8-wide SIMD
unrolling of the symmetric matrix--vector product with two independent
accumulators; (3)~compact blocked WY Q-accumulation for QR
factorisation using GEMM-based reflector application; (4)~NUMA-aware
thread pinning via \texttt{forkOn} with adaptive thread counts;
(5)~increased panel size from $nb{=}32$ to $nb{=}48$ for
tridiagonalisation; and (6)~transitive SVD improvement through the
GEMM cascade.  Results: \textbf{GEMM $10\times$ faster than OpenBLAS}
at $500{\times}500$ ($0.10\times$ ratio); \textbf{eigenSH $2\times$
faster than LAPACK} at $500{\times}500$ ($0.51\times$);
\textbf{SVD faster than LAPACK} at $500{\times}500$ ($0.90\times$);
QR $46\times$ faster ($0.022\times$).  All nine operation categories
now outperform hmatrix.

\paragraph{Round~18 (SIMD substitution unrolling, optimisation exploration).}
All four substitution kernels (LU forward/back, Cholesky forward/back)
were upgraded from 4-wide to 8-wide \texttt{DoubleX4\#} with dual
accumulators.  Three optimisations were evaluated and discarded:
GEMM $k$-loop unrolling (register spills, 20--30\% regression);
D\&C eigensolver crossover at $n{=}80$ (numerical failures at
$n \geq 128$); and svdGKP benchmarking ($22\text{--}24\times$
slower than svdAtAP).  Results: \textbf{Cholesky $2.1\times$
faster than LAPACK} ($0.48\times$, improved from $0.63\times$);
SVD $500{\times}500$ improved to $0.76\times$ (from $0.90\times$);
eigenSH $500{\times}500$ tightened to $0.47\times$ (from $0.51\times$).

\paragraph{Summary.}
Of the nine benchmarked operation categories, \texttt{linear-massiv}
now \textbf{outperforms \texttt{hmatrix} (OpenBLAS/LAPACK) in all
nine}: GEMM ($0.096\text{--}0.15\times$, single-threaded and parallel),
dot product ($0.25\times$), matrix--vector multiply ($0.49\times$),
LU solve ($0.68\times$), Cholesky solve ($0.48\times$),
QR factorisation ($0.021\times$), eigenvalue decomposition
($0.47\text{--}1.19\times$), and SVD ($0.76\text{--}1.78\times$).
At $500{\times}500$---the largest benchmark size---every category
is faster than LAPACK, with SVD now $1.3\times$ faster ($0.76\times$).

\texttt{linear-massiv} demonstrates that \textbf{pure Haskell with
GHC's native SIMD primops, raw \texttt{ByteArray\#} primops, and
lightweight thread-level parallelism can comprehensively outperform
FFI-based BLAS/LAPACK} across all benchmarked numerical linear
algebra operations, while providing compile-time dimensional safety,
zero FFI dependencies, and user-controllable parallelism.  This
makes it a compelling choice not only for applications prioritising
type safety and portability, but for raw numerical throughput as well.

As discussed in Section~\ref{sec:lessons}, the dominant lesson is that
Haskell's high-level abstractions need not preclude low-level performance:
the key is to push the abstraction boundary below the hot loop via raw
primops, while retaining type-safe APIs at the library boundary.
Section~\ref{sec:future-optimisations} identifies nine further
optimisation opportunities, four of which target the two remaining
sub-parity categories (eigenSH and SVD at $n = 100\text{--}200$) and
could bring \texttt{linear-massiv} to full dominance across all sizes.

%% ====================================================================
\begin{thebibliography}{9}

\bibitem{gvl4}
G.~H. Golub and C.~F. Van~Loan,
\emph{Matrix Computations}, 4th ed.
Johns Hopkins University Press, 2013.

\bibitem{higham}
N.~J. Higham,
\emph{Accuracy and Stability of Numerical Algorithms}, 2nd ed.
SIAM, 2002.

\bibitem{criterion}
B.~O'Sullivan,
``Criterion: A Haskell microbenchmarking library,''
\url{https://hackage.haskell.org/package/criterion}, 2009--2024.

\bibitem{massiv}
A.~Todor\=\i,
``massiv: Massiv is a Haskell library for Array manipulation,''
\url{https://hackage.haskell.org/package/massiv}, 2018--2024.

\bibitem{hmatrix}
A.~Ruiz,
``hmatrix: Haskell numeric linear algebra library,''
\url{https://hackage.haskell.org/package/hmatrix}, 2006--2024.

\bibitem{linear}
E.~Kmett,
``linear: Linear algebra library,''
\url{https://hackage.haskell.org/package/linear}, 2012--2024.

\bibitem{openblas}
Z.~Xianyi, W.~Qian, and Z.~Yunquan,
``OpenBLAS: An optimized BLAS library,''
\url{https://www.openblas.net/}, 2011--2024.

\end{thebibliography}

\end{document}
